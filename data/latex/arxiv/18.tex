\label{sec-intro}
In this paper we propose a new
dense initialization for quasi-Newton methods 
to solve problems of the form
\begin{equation*}
	\underset{ \vec{x} \in \Re^n }{\text{ minimize }} f(\vec{x}),
\end{equation*}
where $ f:\Re^n\rightarrow\Re $ is %a general nonconvex
at least a continuously differentiable function, which is not necessarily convex.
The dense
initialization matrix is designed to be updated each time a new
quasi-Newton pair is computed (i.e., as often as once an iteration);
however, in order to retain the efficiency of limited-memory
quasi-Newton methods, the dense initialization matrix and the
generated sequence of quasi-Newton matrices are not explicitly formed.
This proposed initialization makes use of a partial eigendecomposition 
of these matrices for separating  %based separation of 
$\Re^n$ into two orthogonal subspaces -- one for which
there is approximate curvature information and the other for which
there is no reliable curvature information. 
%\rfm{Should we just delete this sentence: } A different scaling
%parameter may then be used for each subspace.  
This initialization has
broad applications for general quasi-Newton trust-region and line
search methods.  In fact, this work can be applied to any quasi-Newton
method that uses an update with a compact representation,
which includes any member of the Broyden class of updates. For this
paper, we explore its use in one specific algorithm; in particular
we consider a limited-memory Broyden-Fletcher-Goldfarb-Shanno ({\small
L-BFGS}) trust-region method where each subproblem is defined using a
shape-changing norm~\cite{BurdakovLMTR16}.  The reason for this choice
is that the dense initialization is naturally well-suited for solving
{\small L-BFGS} trust-region subproblems defined by this norm.
Numerical results on the {\small CUTE}st test set suggest that the dense initialization outperforms
other \LBFGS{} methods.

\medskip 

The {\small BFGS} update is the most widely-used quasi-Newton update for large-scale optimization;
it is defined by the recursion formula 
\begin{equation}
	\label{eq:recursion}
  \vec{B}_{k+1} = \vec{B}_{k} -
\frac{1 }{\vec{s}^T_{k}\vec{B}_{k}\vec{s}_{k}}\vec{B}_{k}\vec{s}_{k}
\vec{s}_{k}^T\vec{B}_{k}+
\frac{1}{\vec{s}^T_{k}\vec{y}_{k}} \vec{y}_{k}\vec{y}^T_{k},
\end{equation}
where
\begin{equation}\label{eqn-sy}
	\vec{s}_{k} \defined \vec{x}_{k+1} - \vec{x}_{k} \quad \text{ and } \quad \vec{y}_{k} \defined \nabla f(\vec{x}_{k+1}) - \nabla f(\vec{x}_{k}),
\end{equation}
and $ \vec{B}_0 \in \Re^{n \times n} $ is a suitably-chosen
initial matrix.
%\je{which in general depends on $k$.  For the purpose
%Tof notational simplicity, we will drop the dependence of the initial matrix on T$k$.}
%\je{(Note that $B_0$ may change as $k$ is updated, and thus,
%is sometimes written as $B_0^{(k)}$; however, for the purposes of
%notational simplicitly we will drop the superscript indicating the
%dependence on $k$ while still assuming that $B_0$ may change each iteration.)}
This rank-two update to $B_{k}$ preserves positive definiteness when $
\vec{s}^T_{k}\vec{y}_{k} > 0$.

{\small L-BFGS} is a limited-memory variant of {\small BFGS} that only
stores a predetermined number, $m$, of the most recently-computed pairs $\{s_i,y_i\}$
where $m\ll n$. (Typically, $m\in[3,7]$ (see, e.g.,~\cite{ByrNS94}).)  Together with an intial matrix $B_0$ that depends on $k$, 
these pairs are used to compute $B_k$.  For notational simplicity, we drop the
dependence of the initial matrix on $k$ and simply denote it as $B_0$.
 This limitation on the number of stored pairs
allows for a practical implementation of the {\small BFGS} method
% makes {\small L-BFGS} a practical method
for large-scale optimization.


There are several desirable properties for picking the initial matrix
$B_0$.  First, in order for the sequence $\{B_k\}$ generated by
(\ref{eq:recursion}) to be symmetric and positive definite, it is necessary
that $ \vec{B}_0 $ is symmetric and positive definite. Second, it is
desirable for $B_0$ to be easily invertible so that solving linear systems with any matrix
in the sequence is computable using the so-called ``two-loop
recursion''~\cite{ByrNS94} or other recursive formulas for $B_k^{-1}$ (for
an overview of other available methods see~\cite{ErwayMarcia17LAA}).  For
these reasons, $\vec{B}_0$ is often chosen to be a scalar multiple of the
identity matrix, i.e., 
\begin{equation}\label{eqn-diagInit}
	\vec{B}_0 =  \gamma_k \vec{I}, \quad
\text{with}\quad \gamma_k > 0. 
\end{equation}
For {\small BFGS} matrices, the conventional choice for the initialization parameter $ \gamma_k $ is 
\begin{equation}\label{eqn-B0-usual} \gamma_k = \frac{\vec{y}^T_{k}
  \vec{y}_{k} }{\vec{s}^T_{k} \vec{y}_{k}},
\end{equation} which can be viewed as a spectral
estimate for $\nabla^2 f( \vec{x}_k )$~\cite{NocW99}.
(This choice was originally proposed in~\cite{ShannoPh78} using a derivation
based on
optimal conditioning.)
It is worth noting that this choice of $ \gamma_k $ can also be derived as the minimizer
of the scalar minimization problem 
\begin{equation}\label{eqn-mingamma}
 \gamma_k = \underset{ \gamma }{ \text{
   argmin } } \left\| \vec{B}^{-1}_0 \vec{y}_k - \vec{s}_k \right\|^2_2,
\end{equation}
where $ \vec{B}^{-1}_0 = \gamma^{-1}\vec{I} $.
For numerical studies on this choice of initialization,
see, e.g., the references listed within~\cite{BurWX96}.

\bigskip

In this paper, we consider a specific dense initialization in lieu of
the usual diagonal initialization.  The aforementioned separation of
$\Re^n$ into two orthogonal subspaces allows for different initialization parameters
to be used to estimate the curvature of the underlying %Hessian 
function in
these subspaces. In one space 
(the space spanned by the most recent updates $\{s_i, y_i\}$ with $k-m \le i \le k-1$), 
estimates of the curvature of the
underlying function are available, and thus, one initialization parameter can be set
using this information.  However, in its orthogonal complement,
curvature information is not available.  
Therefore, if the component of the trial step in the orthogonal subspace is (relatively) too large,
the predictive quality of the whole trial step  is expected to deteriorate. As a result, the
trust-region radius might be reduced, despite the fact that the predictive quality of the component in the aforementioned small subspace may be sufficiently good.
Separating the whole space  into these two subspaces allows users to treat each subspace
differently.
%no curvature information is available, and thus, a separate choice can be made
%for the curvature.  This allows users to treat the two subspaces
%differently.
An alternative view of this initialization is that it
makes use of \emph{two} spectral estimates of $\nabla^2 f(x_k)$.
Finally, the proposed initialization also allows for efficiently
solving and computing products with the resulting quasi-Newton
matrices.


\medskip

The paper is organized in five sections.  In Section 2, we review
properties of {\small L-BFGS} matrices arising from their special recursive
structure as well as overview the shape-changing trust-region method to be
used in this paper.  In Section 3, we present the proposed trust-region
method that uses a shape-changing norm together with a dense initialization
matrix.  While this dense initialization is presented in one specific
context, it can be used in combination with any quasi-Newton update that
admits a so-called \emph{compact representation}.  Numerical experiments
comparing this method with other combinations of initializations and
\LBFGS{} methods are reported in Section 4, and concluding remarks  are
found in Section 5.



