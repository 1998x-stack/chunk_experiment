%!TEX root = main.tex
\section{Discussion}
\label{sec:related}

\begin{table}%[!htbp]
\centering
\scalebox{0.95}{
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{\kNN based Model}
& vanilla \kNN & 31.2      \\
& \kNN + SGL & \textbf{32.2} \\
\hline
\hline
\multirow{2}{*}{SVM based Model}
& vanilla SVM & 33.7   \\
& SVM + SGL & \textbf{44.5}  \\
\hline
\hline
\multirow{1}{*}{CNNs based Model}
& vanilla CNNs & 51.2  \\
\hline
\end{tabular}
}
\caption{Experiments for two baseline model, \kNN and SVM, for the Insurance dataset.}
\label{tb:sgl}
\vspace{-1mm}
\end{table}

The idea of reforming signal to a sparse representation is first introduced in the domain of compressed sensing \cite{Candes+:2008} which achieves great success in signal compression, visualization and classification task. Especially when dictionary is well trained, the performance usually improves significantly, as shown in \cite{Wang:2010,Yang:2009} for image classification tasks. 
In Table~\ref{tb:sgl}, we test the influence of Sparse Group Lasso (SGL) \cite{Simon13asparse-group} with two baseline methods, $k$-Nearest Neighbor (\kNN) and SVM on the Insurance dataset. We use TF-IDF as feature representation for each question and answer sentence. We first select all the answer sentences from top 20 largest category and then find 10 centroids for each of these categories by k-Means. Then we have a dictionary with 200 centroids with 20 groups. We notice there is a great improvement of performance after we preprocess the original sentence representations with SGL before we use SVM. We further test the performance of CNNs on the same dataset, and CNNs outperforms SVM and \kNN even with SGL because of the well trained sentence representation through CNNs. However, for vanilla CNNs, it is not straightforward to embed SGL into the network and still get good representation for sentences since SGL will break the training error in backpropagation. 


However, GSA is fully neural network based framework. Our proposed GSA has similar functionalities to SGL \cite{Yuan06modelselection,Simon13asparse-group}, as it is shown in Fig.~\ref{fig:vis} and Fig.~\ref{fig:act}, but in different approach. Compared with sparse coding approaches which have intense optimizations on both dictionary and coding, GSA's optimization is based on simple backpropagation. GSA also can be easily placed into any neural network for joint training. 
Another advantage of GSA over sparse coding is that the projection function $\Phi$ in GSA can be linear or non-linear, while sparse coding always learns linear codings. 
