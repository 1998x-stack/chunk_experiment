%!TEX root = main.tex
%\vspace{-0.2cm}
\section{Group Sparse CNNs}
%\vspace{-0.2cm}
\label{sec:GSCNN}


%\begin{wrapfigure}[19]{l}[\dimexpr\columnwidth+\columnsep\relax]{12cm}
%\begin{wrapfigure}{l}{0.7\textwidth}%[ht!]
\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{figs/model2.pdf}
\caption{Group Sparse CNN. We add an extra dictionary learning layer between sentence representation $\vecz$ and the final classification layer. 
%Sentence representation after convolutional layer is denoted as $\vecz$, and 
$\vecW$ is the projection matrix (functions as a dictionary) 
that converts $\vecz$ to the group sparse representation $\vech$ (Eq.~\ref{eq:loss_sgl}).
%Hidden group sparse representation for question sentence is denoted as $\vech$. 
Different colors in the projection matrix represent different groups. 
We show $\vecW^\intercal$ instead of $\vecW$ for presentation purposes. 
Darker colors in $\vech$ mean larger values and white means zero.}
\label{fig:model}
%\vspace{-0.1cm}
\end{figure*}
%\end{wrapfigure}






% CNNs were first proposed by \cite{LeCun95comparisonof} in computer vision. 
% For a given image, CNNs apply convolution kernels on a series of continuous areas on images. 
% This concept was first adapted to NLP by \cite{collobert+:2011}. 
% Recently, many CNNs-based techniques 
% achieve great successes in sentence modeling and classification
% \cite{kim:2014,blunsom:2014,ma+:2015}. 
% For simplicity, we use the sequential CNNs \citep{kim:2014} as our baseline.

CNNs were first proposed by \namecite{LeCun95comparisonof} in computer vision and
adapted to NLP by \cite{collobert+:2011}.
Recently, many CNN-based techniques have
achieved great successes in sentence modeling and classification
\cite{kim:2014,blunsom:2014}.
% For simplicity, we use the sequential CNNs \citep{kim:2014} as our baseline.




Following sequential CNNs, one dimensional convolutions operate the convolution kernel in sequential order $\vecx_{i,j} =\vecx_i \oplus   \vecx_{i+1}\oplus \cdots \oplus  \vecx_{i+j}$, where~$\vecx_i \in \vecR^e$ represents the $e$ dimensional word representation for the $i$-th word in the sentence, and~$\oplus$ is the concatenation operator. Therefore $\vecx_{i,j}$ refers to concatenated word vector from the $i$-th word to the $(i+j)$-th word in sentence.


A convolution operates a filter $\mathbf{w} \in  \vecR^{n \times e}$ to a window of $n$ words $\vecx_{i,i+n}$ with bias term $b'$ by $a_i = \sigma (\vecw \cdot  \vecx_{i,i+n}+b' )$ 
with non-linear activation function $\sigma$ to produce a new feature.
The filter $\vecw$ is applied to each word in the sentence, generating the feature map $\veca= [a_1, a_2, \cdots, a_L]$ where $L$ is the sentence length.
We then use $\hat{a} = \max \{ \veca \}$ to represent the entire feature map after max-pooling. 



% The convolution described in Eq.~\ref{eq:con_def} can be regarded as feature detection: more similar patterns will return higher activation. In sequential CNNs, max-over-time pooling \cite{collobert+:2011,kim:2014} operates over the feature map to get the maximum activation $\hat{a} = \max \{ \veca \}$ representing the entire feature map. The idea is to detect the strongest activation over time. This pooling strategy also naturally deals with sentence length variations.

In order to capture different aspects of patterns, CNNs usually randomly initialize a set of filters with different sizes and values. 
Each filter will generate a feature as described above. 
To take all the features generated by $N$ different filters into count, we use $\vecz = [\hat{a_1}, \cdots, \hat{a_N}]$ as the final representation.
In conventional CNNs, this $\vecz$ will be directly fed into classifiers after the sentence representation is obtained, e.g.~fully connected neural networks \cite{kim:2014}. There is no easy way for CNNs to explore the possible hidden representations with underlaying structures.


In order to exploit these structures, % obtain the hidden representations for each sentence representation, 
we propose Group Sparse Convolutional Neural Networks (GSCNNs) by placing one extra layer between the convolutional and the classification layers. 
This extra layer mimics the functionality of GSA from Section~\ref{sec:GSA}.
Shown in Fig.~\ref{fig:model},
%The convolutional layer follows the traditional convolutional process. % described above.
after the conventional convolutional layer, we get the feature map $\vecz$ for each sentence. 
%The feature maps $\vecz$ is treated as the feature representation for each sentence. 
In stead of directly feeding it into a fully connected neural network for classification, 
we enforce the group sparse constraint on $\vecz$ in a way similar to the group sparse constraints on hidden layer in GSA from Sec.~\ref{sec:GSA}. %like the group sparse constraint we have on $h$ in Eq.~\ref{eq:loss_sgl}. 
Then, we use the sparse hidden representation $\vech$ in Eq.~\ref{eq:loss_sgl} as the new sentence representation,
which is then fed %. The last step is feeding the hidden representation $\vech$ 
into a fully connected neural network for classification. The parameters $\vecW$ in Eq.~\ref{eq:loss_sgl} will also be fine tunned during the last step. 


% In order to improve the robustness of the hidden representation and prevent it from simply learning the identity, we follow the idea of decisioning autoencoders \cite{VincentPLarochelleH2008} to add random noise (10\% in our experiments) into $\vecz$. The training process of our model is similar to the training process in stack autoencoders \cite{NIPS2006_3048}. 

% In order to prevent the co-adaptation of the hidden unites, we employ random dropout on penultimate layer \cite{hinton:2014}. We set the drop out rate as $0.5$ and learning rate as $0.95$ by default. 
% In our experiments, training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule \cite{zeiler:2012}. All the settings of the CNNs are the same as the settings in \cite{kim:2014}.
Different ways of initializing the projection matrix in Eq.~\ref{eq:loss_sgl} can be summarized below: % as the followings:

\begin{itemize}

\item \textbf{Random Initialization}: When there is no answer corpus available, we first randomly initialize $N$ vectors 
%(usually $N \gg s$) 
to represent the group information from the answer set. 
Then we cluster these $N$ vectors into $G$ categories with $g$ centroids for each category. 
These centroids from different categories will be the initialized bases for projection matrix $\vecW$ which will be learned during training.
%\vspace{-0.1cm}
\item \textbf{Initialization from Questions}: Instead of using random initialized vectors, we can also use question sentences for initializing 
the projection matrix when the answer set is not available. We need to pre-train the sentences with CNNs to get the sentence representation. 
We then select $G$ largest categories in terms of number of question sentences. Then we get $g$ centroids from each category by $k$-means. 
We concatenate these $G\times g$ vectors to form the projection matrix. %We need to pre-train the sentence with CNNs to get the sentence representation.  
%\vspace{-0.1cm}
\item \textbf{Initialization from Answers}: This is the most ideal case. We follow the same procedure as above, with the only difference being using the answer sentences in place of question sentences to pre-train the CNNs. % to get answer sentence representation.
%\vspace{-0.1cm}
\end{itemize}
