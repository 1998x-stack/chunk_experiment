\section{Prior Work}
\label{sec:prior}

There have been many studies of neural language models that can be dynamically adapted based on context.  Methods have been referred to as context-dependent models \cite{mikolov2012context}, context-aware models \cite{TangContextAware},  conditioned models \cite{Ficler2017ControllingLS}, and controllable text generation \cite{Hu2017ControllableTG}. These models have been used in scoring word sequences (such as for speech recognition or machine translation), for text classification, and for generation. In some work, context corresponds to the previous word history. Here, we instead consider known factors such as user, location and domain metadata, though the framework could be used with history-based context.

The studies that most directly relate to our work are neural models that correspond to special cases of the more general FactorCell model, including those that leverage what we call the SoftmaxBias model \cite{dieng2016topicrnn,TangContextAware,Yogatama2017GenerativeAD,Ficler2017ControllingLS} and others that use the ConcatCell approach \cite{mikolov2012context,wen2013recurrent,chen2015recurrent,ghosh2016contextual}. One study \cite{Ji2015DocumentCL} compares the two approaches, which they refer to as ccDCLM and coDCLM. They find that both approaches give similar perplexities,
but their ConcatCell style model does better at an auxiliary sentence ordering task. This is consistent with our finding that adapting at the recurrent layer can benefit certain tasks while having only a minor impact on perplexity. They do not test any models that adapt both the recurrent and output layers.
\newcite{Hoang2016IncorporatingSI} also consider adapting at the hidden layer vs.\ at the softmax layer, but their architecture does not fit cleanly into the framework of the SoftmaxBias model because they use an extra perceptron layer; thus, it is difficult to compare the experimental findings with ours.

The FactorCell model is distinguished by having an additive (factored) context-dependent transformation of the recurrent layer weight matrix. A related additive context-dependent transformation has been proposed for log-bilinear sequence models \cite{Eisenstein+11,Hutchinson+15}, but these are less powerful than the RNN. A somewhat different use of low-rank factorization has previously been used to reduce the parameter count in an LSTM LM \cite{Kuchaiev2017FactorizationTF}, finding that the reduced number of parameters leads to faster training.

There is a long history of adapting n-gram language models. (See \newcite{demori1999language} or \newcite{bellegarda2004statistical} for a survey.) One recent example is \newcite{chelba2015sparse} where a 34\% relative improvement in perplexity was obtained when using geographic features for adaptation. We hypothesize that the impressive improvement in perplexity is possible because the language in their dataset of Google mobile search queries is particularly sensitive to location. Compared to n-gram based LMs, our model has two advantages in the way that it handles context. First, as we showed in our GeoTwitter experiments, we can adapt to geography using GPS coordinates as input without using predefined geographic regions as in Chelba and Shazeer. Second, our model supports the joint effect of multiple contextual variables. Neural models have an advantage over discrete models as the number of context variables increases.

Much of the work on context-adaptive neural language models has focused on incorporating document or topic information \cite{mikolov2012context,Ji2015DocumentCL,ghosh2016contextual,dieng2016topicrnn}, 
where context is defined in terms of word or n-gram statistics. Our work differs from these studies in that the context is defined by a variety of sources, including discrete and/or continuous metadata, which is mapped to a context vector in end-to-end training.
Context-sensitive language models for text generation tend to involve other forms of context similar to the objective of our work, including speaker characteristics \mbox{\cite{LuanRole,li2016persona}}, dialog act \cite{WenCU2015}, sentiment and other factors \mbox{\cite{TangContextAware,Hu2017ControllableTG}}, and style \cite{Ficler2017ControllingLS}.
As noted earlier, some of this work has used discriminative text classification to evaluate generation. In preliminary experiments with the Yelp data set, we found that the generative classifier accuracy of our model is highly correlated with discriminative classfier accuracy (\mbox{$r \approx~0.95$}). Thus, by this measure, we anticipate that the model would be useful for generation applications. Anecdotally, we find that the model gives more coherent generation results for DBPedia data, but further validation with human ratings is necessary to confirm the benefits on more sources.





