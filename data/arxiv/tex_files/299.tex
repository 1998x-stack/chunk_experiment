\section{Experiment}
\label{sec:exprt}
\input{tab_cocoonline.tex}

\vspace{-5pt}
\subsection{Datasets}
We use two large scale datasets to test our contrastive learning method.
The first dataset is MSCOCO \cite{lin2014microsoft},
which contains $122,585$ images for training and validation.
Each image in MSCOCO has $5$ human annotated captions.
Following splits in \cite{lu2016knowing},
we reserved $2,000$ images for validation.
A more challenging dataset, InstaPIC-1.1M \cite{park2017attend}, 
is used as the second dataset,
which contains $648,761$ images for training, and $5,000$ images for testing.
The images and their ground-truth captions are acquired from Instagram,
where people post images with related descriptions.
Each image in InstaPIC-1.1M is paired with $1$ caption.
This dataset is challenging, as its captions are natural posts with varying formats.
In practice, we reserved $2,000$ images from the training set for validation.

On both datasets, non-alphabet characters except emojis are removed, 
and alphabet characters are converted to lowercases.
Words and emojis that appeared less than $5$ times are replaced with \emph{UNK}.
And all captions are truncated to have at most $18$ words and emojis.
As a result, we obtained a vocabulary of size $9,567$ on MSCOCO, and a vocabulary of size $22,886$ on InstaPIC-1.1M.

\vspace{-5pt}
\subsection{Settings}

To study the generalization ability of proposed CL method,
we tested it on two different image captioning models,
namely \textbf{Neuraltalk2} \cite{karpathy2015deep} and \textbf{AdaptiveAttention} \cite{lu2016knowing}. 
Both models are based on \emph{encoder-and-decoder} \cite{vinyals2015show},
where no attention mechanism is used in the former,
and an adaptive attention component is used in the latter.

For both models, we have pretrained them by MLE,
and use the pretrain checkpoints as initializations. 
In all experiments except for the experiment on model choices, 
we choose the same model and use the same initialization 
for target model and reference model. 
In all our experiments, 
we fixed the learning rate to be $1e$-$6$ for all components,
and used Adam optimizer. 
Seven evaluation metrics have been selected to compare the performances of different models,
including Bleu-1,2,3,4 \cite{papineni2002bleu}, Meteor \cite{lavie2014meteor}, Rouge \cite{lin2004rouge} and Cider \cite{vedantam2015cider}.
All experiments for ablation studies are conducted on the validation set of MSCOCO.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{imgs/compare.pdf}
\caption{\small This figure illustrates several images with captions generated by different models,
where \emph{AA} represents AdaptiveAttention \cite{lu2016knowing} learned by MLE, 
and \emph{AA + CL} represents the same model learned by CL.
Compared to \emph{AA}, \emph{AA + CL} generated more distinctive captions for these images.}
\label{fig:compare}
\end{figure}

\subsection{Results}
\input{tab_instagram.tex}
\vspace{-5pt}
\paragraph{Overall Results} We compared our best model (\emph{AdaptiveAttention} \cite{lu2016knowing} learned by CL) with state-of-the-art models on two datasets.
On MSCOCO, we submitted the results to the online COCO testing server.
%\footnote{\url{https://competitions.codalab.org/competitions/3221}}
The results along with other published results are listed in Table \ref{tab:cocoonline}.
Compared to MLE-learned \emph{AdaptiveAttention}, 
CL improves the performace of it by significant margins across all metrics.
While most of state-of-the-art results are achieved by ensembling multiple models,
our improved \emph{AdaptiveAttention} gains competitive results as a \emph{single} model.
Specifically, on Cider, CL improves \emph{AdaptiveAttention} from $1.003$ to $1.029$,
which is the best single-model result on C40 among all published ones.
In terms of Cider,
if we use MLE,
we need to combine $5$ models to get $4.5\%$ boost on C40 for \emph{AdaptiveAttention}.
Using CL, we improve the performance by $2.5\%$ with just a single model.
On InstaPIC-1.1M, CL improves the performance of \emph{AdaptiveAttention} by $14\%$ in terms of Cider,
which is the state-of-the-art.
Some qualitative results are shown in Figure \ref{fig:compare}.
It's worth noting that the proposed learning method can be used with stronger base models to obtain better results
without any modification.
\input{tab_methods.tex}

\vspace{-5pt}
\paragraph{Compare Learning Methods}
Using \emph{AdaptiveAttention} learned by MLE as base model and initialization, 
we compared our CL with similar learning methods,
including \textbf{CL(P)} and \textbf{CL(N)} that respectively contains only the positive constraint and the negative constraint in CL.
We also compared with
\textbf{IL} \cite{vedantam2017context}, and \textbf{GAN} \cite{dai2017towards}.
The results on MSCOCO are listed in Table \ref{tab:methods},
where (1) among IL, CL and GAN, CL improves performance of the base model,
while both IL and GAN decrease the results. 
This indicates the trade-off between learning distinctiveness and maintaining overall performance 
is not well settled in IL and GAN.
(2) comparing models learned by CL(P), CL(N) and CL,
we found using the positive constraint or the negative constraint alone is not sufficient,
as only one source of guidance is provided.
While CL(P) gives the base model lower improvement than full CL, 
CL(N) downgrades the base model, 
indicating overfits on distinctiveness.
Combining CL(P) and CL(N), 
CL is able to encourage distinctiveness while also emphasizing on overall performance,
resulting in largest improvements on all metrics.

\input{tab_modelchoice.tex}
\input{tab_periodic.tex}

\vspace{-5pt}
\paragraph{Compare Model Choices} To study the generalization ability of CL,
\emph{AdaptiveAttention} and \emph{Neuraltalk2} are respectively chosen 
as both the target and the reference in CL.
In addition, \emph{AdaptiveAttention} learned by MLE,
as a better model, is chosen to be the reference,
for \emph{Neuraltalk2}.
The results are listed in Table \ref{tab:modelchoice},
where compared to models learned by MLE,
both \emph{AdaptiveAttention} and \emph{Neuraltalk2} are improved after learning using CL.
For example, on Cider, \emph{AdaptiveAttention} improves from $1.042$ to $1.142$,
and \emph{Neuraltalk2} improves from $0.882$ to $0.905$.
Moreover, by using a stronger model, \emph{AdaptiveAttention}, as the reference,
\emph{Neuraltalk2} improves further from $0.905$ to $0.956$,
which indicates stronger references empirically provide tighter bounds on both the positive constraint and the negative constraint.

\vspace{-5pt}
\paragraph{Reference Replacement} As discussed in sec \ref{sec:discuss},
one can periodically replace the reference with latest best target model,
to further improve the performance.
In our study, using \emph{AdaptiveAttention} learned by MLE as a start,
each run we fix the reference model util the target saturates its performance on the validation set,
then we replace the reference with latest best target model 
and rerun the learning.
As listed in Table \ref{tab:periodic},
in second run, the relative improvements of the target model is incremental,
compared to its improvement in the first run.
Therefore, when learning a model using CL,
with a sufficiently strong reference,
the improvement is usually saturated in the first run,
and there is no need, in terms of overall performance,
to replace the reference multiple times.


