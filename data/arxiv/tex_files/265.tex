\section{Data}
\label{sec:data}

\begin{table*}[h]
\centering
\begin{tabular}{crrrrrc}
\textbf{Name} & \textbf{Train} & \textbf{Dev} & \textbf{Test} & \textbf{Vocab} & \textbf{Docs.} & \textbf{Context} \\ \hline
AGNews & 4.6M & 0.2M & 0.3M & 54,492 & 115K & 4 Newspaper sections \\
DBPedia & 28.7M & 0.3M & 3.6M & 84,341 & 555K & 14 Entity categories \\
TripAdvisor & 127.2M & 2.6M & 2.6M & 88,347 & 843K & 3.5K Hotels/5 Sentiment \\
Yelp & 91.5M & 0.7M & 7.1M & 57,794 & 645K & 5 Sentiment \\
EuroTwitter$^*$ & 5.3M &  0.8M & 1.0M & 194 & 80K & 9 Languages \\
GeoTwitter$^*$ & 51.7M & 2.2M & 2.2M & 203 & 604K & Latitude \& Longitude
\end{tabular}
\caption{Dataset statistics: Dataset size in words (* or characters) of Train, Dev and Test sets, vocabulary size, number of training documents, and context variables.}
\label{table:data}
\end{table*}

Our experiments make use of six datasets: four targeting word-level sequences, and two targeting character sequences. The character studies are motivated by the growing interest in character-level models in both speech recognition and machine translation \cite{hannun2014deep,chung2016character}. By using multiple datasets with different types of context, we hope to learn more about what makes a dataset amenable to adaptation. The datasets range in size from over 100 million words of training data to 5 million characters of training data for the smallest one.
When using a word-based vocabulary, we preprocess the data by lowercasing, tokenizing and removing most punctuation. We also truncate sentences to be shorter than a maximum length of 60 words for AGNews and DBPedia and 150 to 200 tokens for the remaining datasets. 
Summary information is provided in Table~\ref{table:data}, including the training, development, and test data sizes in terms of number of tokens, vocabulary size, number of training documents (i.e. context samples), and the context variables ($f_{1:n}$). The largest dataset, TripAdvisor, has over 800 thousand hotel review documents, which adds up to over 125 million words of training data. 

The first three datasets (AGNews, DBPedia, and Yelp) have previously been used for text classification \cite{zhang2015character}.
These consist of newspaper headlines, encyclopedia entries, and restaurant and business reviews, respectively. The context variables associated with these correspond to the newspaper section (world, sports, business, sci \& tech) for each headline, the page category on DBPedia (out of 14 options such as actor, athlete, building, etc.), and the star rating on Yelp (from one to five).  For AgNews, DBPedia, and Yelp we use the same test data as in previous work. Our fourth dataset, from TripAdvisor, was previously used for language modeling and consists of two relevant context variables: an identifier for the hotel and a sentiment score from one to five stars \cite{TangContextAware}. Some of the reviews are written in French or German but most are in English. There are 4,333 different hotels but we group all the ones that do not occur at least 50 times in the training data into a single entity, leaving us with around 3,500. These four datasets use word-based vocabularies. 

We also experiment on two Twitter datasets: EuroTwitter and GeoTwitter. EuroTwitter consists of 80 thousand Tweets labeled with one of nine languages: (English, Spanish, Galician, Catalan, Basque, Portuguese, French, German, and Italian). The corpus was created by combining portions of multiple published datasets for language identification including Twitter70 \cite{jaech2016hierarchical}, TweetLID \cite{zubiaga2014overview}, and the monolingual portion of Tweets from a code-switching detection workshop \cite{molina2016overview}. The GeoTwitter data contains Tweets with latitude and longitude information from England, Spain, and the United States.\footnote{Data was accessed from http://followthehashtag.com.} The latitude and longitude coordinates are given as numerical inputs. This is different from the other five datasets that all use categorical context variables.
