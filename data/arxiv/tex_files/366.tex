\section{Context-Aware RNN}
\label{sec:model}

Our model uses adaptation in both the recurrent layer and in the bias vector of the output layer. In this section we describe how we represent context as an embedding 
and methods for adapting the recurrent layer and the softmax layer, showing that our proposed model is a generalization of prior methods. 
The novelty of our model is that instead of using context as an additional input to the model, it uses the context information to transform the weights of the recurrent layer. This is accomplished using a low-rank decomposition in order to control the extent of parameter sharing between contexts,
which is important for handling high-dimensional, sparse contexts.

\subsection{Context representation}

We assume the availability of contextual information (metadata or other side information) that is represented as a set of
context variables $f_{1:n}=f_1, f_2, \dots f_n$, from which we produce a $k$-dimensional representation in the form of an embedding, $c \in \mathbb{R}^k$. Each of the context variables, $f_i$, represents some type of information or metadata about the sequence and can be either categorical or numerical. The embeddings can either be learned off-line using a topic model \cite{mikolov2012context} or end-to-end as part of the adapted LM \cite{TangContextAware}.  Here, we use end-to-end learning, where the context embedding is the output of a feed-forward network with a ReLU activation function.
The resulting embedding, $c$, is used for adapting both the recurrent layer and the output layer of the RNN. 

\subsection{Adapting the recurrent layer}

The basic operation of the recurrent layer is to use a matrix $\mathbf{W}$ to transform the concatenation of a word embedding, $w_t \in \mathbb{R}^e$, with the hidden state from the previous time step, $h_{t-1} \in \mathbb{R}^d$, and produce a new hidden state, $h_t$, as given by Equation \ref{eq:rnn}:  
\begin{align}
\label{eq:rnn}
\begin{split}
h_t &= \sigma(\mathbf{W}_1 w_t + \mathbf{W}_2 h_{t-1} + b) \\
    &= \sigma(\mathbf{W}[w_t, h_{t-1}] + b).
\end{split}
\end{align}
The size of $\mathbf{W}$ is $d\times (e+d)$. For simplicity, our equations assume a simple RNN. Appendix \ref{sec:lstm_equations} shows how the equations can be adjusted to work with an LSTM. 

The standard approach to recurrent layer adaptation is to include (via concatenation) the context embedding as an additional input to the recurrent layer \cite{mikolov2012context}. When the context embedding is constant across the whole sequence, it is easy to show that this concatenation is equivalent to using a context-dependent bias at the recurrent layer:  
\begin{align}
\label{eq:ConcatCell}
\begin{split}
  h_t &= \sigma( \hat{\mathbf{W}}[w_t, h_{t-1}, c] + b) \\
      &= \sigma( \mathbf{W}[w_t, h_{t-1}] + \mathbf{V}c + b) \\
      &= \sigma( \mathbf{W}[w_t, h_{t-1}] + b'),
\end{split}
\end{align}
where 
$\hat{\mathbf{W}} = [ \mathbf{W} \  \mathbf{V} ]$ 
and $b' = \mathbf{V}c + b$ is the context-dependent bias, formed by adding a linear projection of the context embedding. We refer to this adaptation approach as the ConcatCell model. 

Our proposed model extends the ConcatCell by using a context-dependent weight matrix
$\mbox{$\mathbf{W}' = \mathbf{W} + \mathbf{A}$}$, in place of the generic weight matrix $\mathbf{W}$. (We refer to $\mathbf{W}$ as generic because it is shared across all context settings.) 
The adaptation matrix, $\mathbf{A}$, is generated by taking the product of the context embedding vector against a set of left and right basis tensors to produce a rank $r$ matrix. 
The left and right adaptation basis tensors are given as  $\mathbf{Z}_L \in \mathbb{R}^{k \times (e+d) \times r}$ and $\mathbf{Z}_R \in \mathbb{R}^{r \times d \times k}$. The two bases tensors together can be thought of as holding $k$ different rank $r$ matrices, $A_j=\mathbf{Z}_{L,j}\mathbf{Z}_{R,j}$, each the size of $\mathbf{W}$. By taking the product between $c$ and the corresponding tensor modes of $\mathbf{Z}_L$ and $\mathbf{Z}_R$ (using $\times_i$ to denote the mode-$i$ tensor product, i.e., the product with the $i$-th dimension of the tensor), the context determines the weighted combination of the $k$ matrices:
\begin{equation}
\label{eq:w_prime}
  A=(c \times_1 \mathbf{Z}_L)(\mathbf{Z}_R \times_3 c^\intercal) .  
\end{equation}
The number of degrees of freedom of $A$ is controlled by the dimension $k$ of the context vector and the rank $r$ of the $k$ weight matrices. The rank is treated as a hyperparameter and controls the extent to which the model relies on the generic weight matrix $\mathbf{W}$ versus behaves in a more context-specific manner.

We call this model the FactorCell because the weight matrix has been adapted by adding a factored component. The ConcatCell model is a special case of the FactorCell where $\mathbf{Z}_L$ and $\mathbf{Z}_R$ are set to zero. In summary, the proposed model is given by:
\begin{align}
\label{eq:FactorCell}
\begin{split}
  h_t &= \sigma( \mathbf{W}'[w_t, h_{t-1}] + b') \\
\mathbf{W}'  &=  \mathbf{W} + (c \times_1 \mathbf{Z}_L)(\mathbf{Z}_R \times_3 c)\\
 b'  &=  \mathbf{V}c + b.
\end{split}
\end{align}


If the context is known in advance, $\mathbf{W}'$ can be precomputed, in which case applying the RNN at test time requires no more computation than using an unadapted RNN of the same size. This means that for a fixed sized recurrent layer, the FactorCell model can have many more parameters than the ConcatCell model but hardly any increase in computational cost.

\subsection{Adapting the Softmax Bias}
\label{sec:softmaxbias}

The last layer of the model predicts the probability of the next symbol in the sequence using the output from the recurrent layer using the softmax function to create a normalized probability distribution. The output probabilities are given by 
\begin{equation}
y_t = \mathrm{softmax}(\mathbf{E}\mathbf{L}h_t + b_{out}),
\end{equation}
where $\mathbf{E}\in\mathbb{R}^{|V|\times e}$ is the matrix of word embeddings, $\mathbf{L}\in\mathbb{R}^{e\times d}$ is a linear projection to match the dimension of the recurrent layer (when $e\ne d$), and $b_{out} \in \mathbb{R}^{|V|}$ is the softmax bias vector. 
We tie the word embeddings in the input layer with the ones in the output layer \cite{press2016using,inan2016tying}. 

If $s_j$ is the indicator row vector for the $j$th word in the vocabulary then $p(w_t|w_{1:t-1}) = s_t y_t$ and $\log p(w_{1:T}) = \sum_t \log s_{w_t} y_t$.

Adapting the softmax bias alters the unigram distribution. There are two ways to accomplish this. When the values that the context can take are categorical with low cardinality then context-dependent softmax bias vectors can be learned directly. This is equivalent to replacing $c$ with a one-hot vector. Otherwise, a projection of the context embedding, $\mathbf{Q}c$ where $\mathbf{Q} \in \mathbb{R}^{|V| \times k}$, can be used to adapt the bias vector as in 
\begin{equation}
\label{eqn:adaptSoftmax}
y_t = \mathrm{softmax}(\mathbf{E}\mathbf{L}h_t + \mathbf{Q}c + b_{out}).
\end{equation}
The projection can be thought of as a low-rank approximation to using the one-hot context vector. Both strategies are explored,
depending on the nature of the original context space.

As noted in Section~\ref{sec:prior}, adaptation of the softmax bias has been used in other studies. As we show in the experimental work, it is useful for representing phenomena where unigram statistics are important.


