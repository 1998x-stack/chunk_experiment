<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1710.03006] Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features</title><meta property="og:description" content="In recent years, (retro-)digitizing paper-based files became a major undertaking for private and public archives as well as an important task in electronic mailroom applications.
As a first step, the workflow involves ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1710.03006">
<link rel="canonical" target="_blank" href="https://ar5iv.labs.arxiv.org/html/1710.03006">

<!--Generated on Sun Mar  3 03:31:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">In recent years, (retro-)digitizing paper-based files became a major undertaking for private and public archives as well as an important task in electronic mailroom applications.
As a first step, the workflow involves scanning and Optical Character Recognition (OCR) of documents.
Preservation of document contexts of single page scans is a major requirement in this context.
To facilitate workflows involving very large amounts of paper scans, page stream segmentation (PSS) is the task to automatically separate a stream of scanned images into multi-page documents.
In a digitization project together with a German federal archive, we developed a novel approach based on convolutional neural networks (CNN) combining image and text features to achieve optimal document separation results. Evaluation shows that our PSS architecture achieves an accuracy up to 93¬†% which can be regarded as a new state-of-the-art for this task.

<br class="ltx_break">
<br class="ltx_break">
<span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords:‚Äâ</span>page stream segmentation, convolutional neural nets, document image classification, document management, text classification</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\newcites</span>
<p id="p1.2" class="ltx_p">languageresourceLanguage Resources





</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features</span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id1.p1.2.1.1" class="ltx_tr">
<td id="id1.p1.2.1.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Gregor Wiedemann, Gerhard Heyer</span></td>
</tr>
<tr id="id1.p1.2.2.2" class="ltx_tr">
<td id="id1.p1.2.2.2.1" class="ltx_td ltx_align_center">Department of Computer Science</td>
</tr>
<tr id="id1.p1.2.3.3" class="ltx_tr">
<td id="id1.p1.2.3.3.1" class="ltx_td ltx_align_center">Leipzig University, Germany</td>
</tr>
<tr id="id1.p1.2.4.4" class="ltx_tr">
<td id="id1.p1.2.4.4.1" class="ltx_td ltx_align_center">gregor.wiedemann@uni-leipzig.de, heyer@informatik.uni-leipzig.de</td>
</tr>
</tbody>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">For digitization of incoming mails in business contexts as well as for retro-digitizing archives, batch scanning of documents can be a major simplification of the processing workflow. In this scenario, scanned images of multi-page documents arrive at a document management system as an ordered stream of single pages lacking information on document boundaries.
Page stream segmentation (PSS) then is the task of dividing the continuous document stream into sequences of pages that represent single physical documents.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The task is also referred to as Document Flow Segmentation or Document Separation.</span></span></span></p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Applying a fully automated approach of document page segmentation can be favorable over manually separating and scanning documents, especially in contexts of very large data sets which need to be separated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Gallo et al., 2016</a>]</cite>.
In a joint research project together with a German research archive, we supported the task of retro-digitization of a paper archive consisting of circa one million pages put on file between 1922 and 2010 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Isemann et al., 2014</a>]</cite>.
The collection contains documents of varying content, types and lengths around the topic of ultimate disposal of nuclear waste, mostly administrative letter correspondence and research reports, but also stock lists, meeting minutes and email printouts.
The 1M pages were archived in roughly 20.000 binders which were batch-scanned due to limited manual capacities for separating individual documents. The long time range of archived material affects document quality, proliferation of layout standards, different fonts and the use of hand-written texts. All these circumstances pose severe challenges to OCR as well as to page stream segmentation (PSS).</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this article, we introduce our approach to PSS comparing (linear) support vector machines (SVM) and convolutional neural networks (CNN). For the first time for this task, we combine textual and visual features into one network to achieve most-accurate results. The upcoming section <a href="#S2" title="2. Related work ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.</span></a> elaborates on related work. In section <a href="#S3" title="3. Datasets ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.</span></a> we describe our dataset together with one reference dataset for this task. In section <a href="#S4" title="4. Binary classification for PSS ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.</span></a> we introduce our neural network based architecture for PSS.
As a baseline, we introduce an SVM-based model solely operating on text features. Then, we introduce CNN for PSS on text and image data separately as well as in a combined architecture. Section <a href="#S5" title="5. Evaluation ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.</span></a> presents a quantitative and a qualitative evaluation of the approach on the two datasets.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Page stream segmentation is related to a series of other tasks concerned with digital document management workflows. Table¬†<a href="#S2.T1" title="Table 1 ‚Ä£ 2. Related work ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes important characteristics of recent works in this field. A common task related to PSS is document image classification (DIC) in which typically visual features (pixels) are utilized to classify scanned document representations into categories such as ‚Äúinvoice‚Äù, ‚Äúletter‚Äù, ‚Äúcertificate‚Äù etc. Category systems can become quite large and complex. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Gordo et al., 2013</a>]</cite> summarize different approaches in a survey article on PSS and DIC.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Gallo et al., 2016</a>]</cite>, PSS is performed on top of the results from a DIC process. Pages from the stream are segmented each time the DIC system detects a change of class labels between consecutive page images.
This approach can only be successful in case there are alternating types of documents in the sequential stream. Often, this cannot be guaranteed, especially in case of small document category systems.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">Alternative approaches seek to identify document boundaries explicitly. Such approaches are proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Daher and Bela√Ød, 2014</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Agin et al., 2015</a>]</cite> where each individual image of the sequence is classified as either continuity of the same document (SD) or beginning of a new document (ND). For this binary classification, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Daher and Bela√Ød, 2014</a>]</cite> rely on textual features extracted from OCR-results and classify pages with SVM and multi-layer perceptrons (MLP). <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Agin et al., 2015</a>]</cite> employ bag of visual words (BoVW) and font information obtained from OCR as features, and test performance with three binary classifiers (SVM, Random Forest, and MLP).</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">The recent state-of-the-art for DIC is achieved by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Gallo et al., 2016</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Harley et al., 2015</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Noce et al., 2016</a>]</cite> who employ Deep Learning with Convolutional Neural Networks to identify document classes. While the former two employ only visual features, the latter study uses both, visual and text features for DIC. For this, class-specific key terms are extracted from the OCR-ed training documents and highlighted with correspondingly colored boxes in the document images. Then, a CNN is applied to learn document classes from these images augmented with textual information highlighting.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p">Although with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Gallo et al., 2016</a>]</cite> there is already one study employing neural network technology not only for DIC but also for PSS, their approach was not applicable to our project for two reasons. First, as mentioned earlier, they perform PSS only indirectly based on changing class labels of consecutive pages. Since we only have 17 document categories and a majority of them belong to one category (‚Äùletter‚Äù), we need to perform direct separation of the page stream by classifying each page into either SD or ND. Second, quality and layout of our data is extremely heterogeneous due to the long time period of document creation. We expect a lowered performance by solely relying on visual features for separation. Therefore, taking the previous work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Gallo et al., 2016</a>]</cite> as a starting point, we propose our approach for direct PSS as a binary classification task combining textual features and visual features using deep neural networks. We compare this architecture against a baseline comprising an SVM classifier solely relying on textual features.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Recent work on page stream segmentation</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Authors</span></td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">PSS</span></td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">DIC</span></td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Visual Features</span></td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Text Features</span></td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Architecture</span></td>
<td id="S2.T1.1.1.1.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Daher; Belaid (2014)</td>
<td id="S2.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">X</td>
<td id="S2.T1.1.2.2.3" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.2.2.4" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">X</td>
<td id="S2.T1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_t">SVM, MLP</td>
<td id="S2.T1.1.2.2.7" class="ltx_td ltx_align_left ltx_border_t">F = 0.8 - 0.9</td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.3.3.1" class="ltx_td ltx_align_left">Agin et al. (2015)</td>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.3.3.3" class="ltx_td"></td>
<td id="S2.T1.1.3.3.4" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.3.3.5" class="ltx_td ltx_align_center">(X, fonts)</td>
<td id="S2.T1.1.3.3.6" class="ltx_td ltx_align_left">SVM, RF, MLP</td>
<td id="S2.T1.1.3.3.7" class="ltx_td ltx_align_left">F = 0.89</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.4.4.1" class="ltx_td ltx_align_left">Harley et al. (2015)</td>
<td id="S2.T1.1.4.4.2" class="ltx_td"></td>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.4.4.4" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.4.4.5" class="ltx_td"></td>
<td id="S2.T1.1.4.4.6" class="ltx_td ltx_align_left">CNN</td>
<td id="S2.T1.1.4.4.7" class="ltx_td ltx_align_left">A = 0.76 - 0.90</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.5.5.1" class="ltx_td ltx_align_left">Noce et al. (2016)</td>
<td id="S2.T1.1.5.5.2" class="ltx_td"></td>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.5.5.4" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.5.5.5" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.5.5.6" class="ltx_td ltx_align_left">CNN</td>
<td id="S2.T1.1.5.5.7" class="ltx_td ltx_align_left">A = 0.8 - 0.9</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.6.6.1" class="ltx_td ltx_align_left">Gallo et al. (2016)</td>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_center">(X, indirect)</td>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.6.6.4" class="ltx_td ltx_align_center">X</td>
<td id="S2.T1.1.6.6.5" class="ltx_td"></td>
<td id="S2.T1.1.6.6.6" class="ltx_td ltx_align_left">CNN+MLP</td>
<td id="S2.T1.1.6.6.7" class="ltx_td ltx_align_left">A = 0.88</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_t">Our approach</td>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">X</td>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">X</td>
<td id="S2.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">X</td>
<td id="S2.T1.1.7.7.6" class="ltx_td ltx_align_left ltx_border_t">SVM</td>
<td id="S2.T1.1.7.7.7" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.8.8.1" class="ltx_td ltx_border_b"></td>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_border_b"></td>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_border_b"></td>
<td id="S2.T1.1.8.8.4" class="ltx_td ltx_border_b"></td>
<td id="S2.T1.1.8.8.5" class="ltx_td ltx_border_b"></td>
<td id="S2.T1.1.8.8.6" class="ltx_td ltx_align_left ltx_border_b">CNN+MLP</td>
<td id="S2.T1.1.8.8.7" class="ltx_td ltx_align_left ltx_border_b">see Section <a href="#S5" title="5. Evaluation ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.</span></a>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Datasets</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We evaluate our approach on two datasets, one sample from the German archive data of our project context, and one public resource of annotated document scans from U.S. tobacco companies.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>German archive data</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The German dataset consists of a variety of document classes from a very long time frame. Most of the documents were archived between the mid-1960s and 2010. Due to this, OCR-quality, document lengths, layout standards as well as used fonts differ widely.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">After batch scanning, about 40¬†% of all binders from the German research archive have been manually separated into documents and annotated with document categories. The manually separated documents can serve as a ground truth for our experiments on model selection and feature engineering for automatic page stream segmentation.
For these experiments, we randomly selected 100 binders from the set of all manually separated binders. The binders represent 100 ordered streams of scanned pages, in total consisting of 22,741 pages. 80 of the selected binders containing 17,376 pages were taken as a training set, 20 binders with 5095 pages were taken as test set.
Scanned pages were resampled to a size of 224 <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><times id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\times</annotation></semantics></math> 224 pixels and color-converted to black and white with the OTSU binarization method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Otsu, 1979</a>]</cite>.
The upper lines in Fig. <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.1. German archive data ‚Ä£ 3. Datasets ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.F2" title="Figure 2 ‚Ä£ 3.1. German archive data ‚Ä£ 3. Datasets ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> show examples of first pages, resp. subsequent pages from documents.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">From original document scans, text information was extracted by optical character recognition (OCR). In the following, this dataset is referred to as <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">Archive22k</em>.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<p id="S3.F1.8" class="ltx_p ltx_align_center"><span id="S3.F1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/FP_1.png" id="S3.F1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F1.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/FP_2.png" id="S3.F1.2.2.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F1.3.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/FP_3.png" id="S3.F1.3.3.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F1.4.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/FP_4.png" id="S3.F1.4.4.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F1.5.5" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/FirstPage_1.png" id="S3.F1.5.5.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F1.6.6" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/FirstPage_2.png" id="S3.F1.6.6.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F1.7.7" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/FirstPage_3.png" id="S3.F1.7.7.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F1.8.8" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/FirstPage_4.png" id="S3.F1.8.8.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples for first pages (class <span id="S3.F1.10.1" class="ltx_text ltx_font_italic">new document</span>); from Archive22k (above) and Tobacco800 (below).</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.8" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/NP_1.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span> 
<span id="S3.F2.2.2" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/NP_4.png" id="S3.F2.2.2.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F2.3.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/NP_3.png" id="S3.F2.3.3.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F2.4.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/NP_2.png" id="S3.F2.4.4.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F2.5.5" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/NextPage_1.png" id="S3.F2.5.5.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F2.6.6" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/NextPage_2.png" id="S3.F2.6.6.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F2.7.7" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/NextPage_3.png" id="S3.F2.7.7.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span>
<span id="S3.F2.8.8" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/1710.03006/assets/NextPage_4.png" id="S3.F2.8.8.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples for subsequent pages (class <span id="S3.F2.10.1" class="ltx_text ltx_font_italic">same document</span>); from Archive22k (above) and Tobacco800 (below).</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Tobacco800</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">As a second evaluation set, we run our classification process on the <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Tobacco800</em> document image database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Lewis et al., 2006</a>]</cite>. The dataset allows comparing the performance of our approach to other recent studies.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">The Tobacco800 dataset is a small annotated subset of the Truth Tobacco Industry Documents, a collection of more than 14 million documents originating from seven major U.S. tobacco industry organizations dealing with their research, manufacturing, and marketing during the last decades. The documents had to be publicly released due to lawsuits in the United States.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">The annotated subset for our experiments is composed of 1,290 document images sampled from the original corpus. Similar to the German dataset, it contains multi-page documents of different types (e.g. letters, invoices, hand-written documents) and thus is well suited for evaluation of our task. Samples from the Tobacco dataset were also used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Harley et al., 2015</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Noce et al., 2016</a>]</cite>. Again, we extract text information for each page via OCR from the original page scans, OTSU-binarize them to a black/white color palette and resize them to a 224 <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mo id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><times id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\times</annotation></semantics></math> 224 pixel resolution.
The lower lines in Fig. <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.1. German archive data ‚Ä£ 3. Datasets ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.F2" title="Figure 2 ‚Ä£ 3.1. German archive data ‚Ä£ 3. Datasets ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> show examples of first pages, resp. subsequent pages from Tobacco800 documents.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p">As the example pages show, both collections share similarities in their visual appearance. First pages compared to subsequent ones may contain distinct header elements. But in general, the human observer has difficulties to identify clear layout patterns discriminating between both classes, especially for the Archive22k documents. Therefore, visual features alone may not be sufficient for accurate PSS.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p">Regarding their textual content, the two datasets share certain similarities but also differ with respect to language, size, and creatorship.
Both have in common that they cover long time periods and are thematically located within a rather narrow domain (nuclear waste disposal, tobacco industry).
Nonetheless, they largely differ
with respect to characteristics of content creators. On the one hand, there is a state-run research library archiving material from a wide variety of actors, while on the other hand there are internal documents from a rather small set of business actors with corporate design standards. Due to this, we expect different performance from textual and visual features for PSS on both datasets.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Binary classification for PSS</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Analogue to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Daher and Bela√Ød, 2014</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Agin et al., 2015</a>]</cite>, we approach PSS as a binary classification task on single pages from a data stream. Pages are classified into either continuity of the <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">same document</em> (SD) or beginning of a <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">new document</em> (ND). For classification, we compare two architectures: SVM with specifically engineered text features (<a href="#S4.SS1" title="4.1. Baseline: SVM on text features ‚Ä£ 4. Binary classification for PSS ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.</span></a>) and a combination of CNN and MLP with both, textual and visual features (<a href="#S4.SS2" title="4.2. Neural networks on text and image features ‚Ä£ 4. Binary classification for PSS ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.</span></a>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Baseline: SVM on text features</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">As a baseline, we use linear text classification together with specifically engineered features for PSS. For this first step, we rely on SVM with a linear kernel<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We use the Liblinear library by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Fan et al., 2008</a>]</cite></span></span></span>. This learning algorithm has proven to be very efficient for binary classification problems with sparse and large feature spaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Joachims, 1998</a>]</cite> and is computationally much faster than neural network architectures.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We refrain from using image features in this architecture, because pixel features are not supposed to be linearly separable. First experiments confirmed that pixel features do not contribute discriminative information on top of text features to the linear SVM for our task. Of course, we could use a different SVM kernel for image classification. But, very likely we would lose the advantage of computational speed. Due to this, we stick to text features for our baseline method.</span></span></span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">We extract four types of features from the OCR-ed text data of the single pages.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Unigrams:</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">Page texts were tokenized and resulting tokens reduced to their word stem. We further replaced digits in tokens with a #-character and pruned types from the vocabulary which occurred less than 3 times (Tobacco800), resp. 10 times (Archive22k). Pruning was applied to maintain manageable vocabulary sizes and reduce noise from infrequent events in the data. Different thresholds for feature pruning were chosen with respect to different collection sizes. This step resulted in 6,849 (Tobacco800), resp. 18,917 (Archive22k) features encoding raw frequency counts of all word types on each page.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Topic composition:</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px2.p1.5" class="ltx_p">In a second step, we obtained features of topical composition for each page from an unsupervised machine learning process. For this, we rely on Latent Dirichlet Allocation (LDA), also referred to as topic modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Blei et al., 2003</a>]</cite>.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Actually, there is a large variety of unsupervised topic models as well as many other methods to reduce sparse, high-dimensional text data to a dense, lower-dimensional space (e.g. latent semantic analysis). For our baseline system, we stick to LDA as the seminal and most widely-used topic model.</span></span></span> Topic proportions based on multinomial posterior probability distributions <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">\theta</annotation></semantics></math> from a topic model can be used as a dense feature vector comprising latent semantics of the modeled documents. In addition to highly sparse n-gram features, they can provide useful information to any text classifier.
Following a method proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Phan et al., 2011</a>]</cite>, we presented single page texts as pseudo-documents to the process and compute a model with <math id="S4.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="K=50" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">K</mi><mo id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1"><eq id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1"></eq><ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2">ùêæ</ci><cn type="integer" id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">K=50</annotation></semantics></math> (Tobacco800), resp. <math id="S4.SS1.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="K=100" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.3.m3.1a"><mrow id="S4.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">K</mi><mo id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1"><eq id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1"></eq><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2">ùêæ</ci><cn type="integer" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.3.m3.1c">K=100</annotation></semantics></math> (Archive22k) topics. Different topic resolutions were chosen again with respect to different collection sizes. For each page <math id="S4.SS1.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.4.m4.1a"><mi id="S4.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS1.SSS0.Px2.p1.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.4.m4.1b"><ci id="S4.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.4.m4.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.4.m4.1c">p</annotation></semantics></math>, we then use the resulting topic-page distribution <math id="S4.SS1.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\theta_{p}" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.5.m5.1a"><msub id="S4.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p1.5.m5.1.1.2" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml">Œ∏</mi><mi id="S4.SS1.SSS0.Px2.p1.5.m5.1.1.3" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.5.m5.1b"><apply id="S4.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1.2">ùúÉ</ci><ci id="S4.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.5.m5.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.5.m5.1c">\theta_{p}</annotation></semantics></math> as feature vector supplementary to the previously extracted vector of unigram counts.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Topic difference:</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px3.p1.3" class="ltx_p">We expect multi-page documents to comprise a rather coherent topic structure. Thus, for each page <math id="S4.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS1.SSS0.Px3.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.1.m1.1c">p</annotation></semantics></math>, we determine the difference between its topic composition <math id="S4.SS1.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="\theta_{p}" display="inline"><semantics id="S4.SS1.SSS0.Px3.p1.2.m2.1a"><msub id="S4.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml">Œ∏</mi><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2">ùúÉ</ci><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.2.m2.1c">\theta_{p}</annotation></semantics></math> and its predecessor <math id="S4.SS1.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="\theta_{p-1}" display="inline"><semantics id="S4.SS1.SSS0.Px3.p1.3.m3.1a"><msub id="S4.SS1.SSS0.Px3.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.2" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml">Œ∏</mi><mrow id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mi id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">p</mi><mo id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.1" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.1.cmml">‚àí</mo><mn id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.3.m3.1b"><apply id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.2">ùúÉ</ci><apply id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3"><minus id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.1"></minus><ci id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.2">ùëù</ci><cn type="integer" id="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px3.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.3.m3.1c">\theta_{p-1}</annotation></semantics></math> as a third feature type for PSS. We utilize two measures, Hellinger distance and Cosine distance, to create two additional features. While the former is a common metric to compare two probability distributions, the latter also has been adopted successfully to compare topic model results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Niekler and J√§hnichen, 2012</a>]</cite>. Distance values near zero indicate a high similarity of topic composition compared to the predecessor page. Values near one indicate a significant change of topic composition which could indicate the beginning of a new document.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Predecessor pages:</h4>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">As a last feature type, we add a copy of features extracted in the previous three steps belonging to the predecessor page as new features to each current page. This can be achieved easily by appending their values together with a new unique feature identifier. For this, we simply concatenate existing feature identifiers with a prefix, e.g. ‚ÄòPREV#‚Äô. This is necessary to allow for the distinction between feature values for the current page and copied values from the predecessor page. By this, any classifier not only can rely on the information about characteristics of the current page for its decision but also may learn from information contained on the previous page. For instance, the presence of a salutation phrase such as ‚ÄúWith kind regards‚Äù on a predecessor page highly increases the probability for the beginning of a new document on the current page.</p>
</div>
<div id="S4.SS1.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px4.p2.1" class="ltx_p">The performance of <em id="S4.SS1.SSS0.Px4.p2.1.1" class="ltx_emph ltx_font_italic">SVM classification</em> to determine for each page whether it is the beginning of a new document or the continuation of the current document is tested in different steps. In each step, one of the four just introduced feature types is added to the feature set. Step-wise addition of the feature types to the linear SVM allows controlling whether each type effectively provides valuable information for the process.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Neural networks on text and image features</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">For our new PSS approach (cp. Fig. <a href="#S4.F3" title="Figure 3 ‚Ä£ 4.2. Neural networks on text and image features ‚Ä£ 4. Binary classification for PSS ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for a schematic representation of the architecture), we first create two separate convolutional neural networks (CNN) for binary classification of pages into either SD or ND, one based on text data and another based on image scans. In a third step, we combine the learned parameters from the two final hidden layers of both CNN to an input vector of features for a multi-layer perceptron. This MLP delivers a third and final classification result based on both feature types.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>CNN + MLP architecture for PSS</figcaption><img src="/html/1710.03006/assets/x1.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="260" height="368" alt="Refer to caption">
</figure>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CNN for text data:</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Kim, 2014</a>]</cite> proposed a simple but effective CNN-architecture for text classification which achieved high performance for sentiment analysis tasks on standard data sets. He uses 1-dimensional convolution over word sequences encoded as embedding vectors.
We adopt a slightly simpler version of this network architecture by relying on only one kernel size instead of combining convolution layers with three different kernel sizes. Our network starts with an embedding layer with 300 dimensions, followed by a convolution layer with 350 filters and a kernel size of 3. On the resulting convolution filters, global max pooling is applied, followed by a dense layer of 256 neurons with ‚ÄúReLU‚Äù activation, a dropout layer (<math id="S4.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="dr=.5" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mrow id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1"><eq id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1"></eq><apply id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2"><times id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.1"></times><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.2">ùëë</ci><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.3">ùëü</ci></apply><cn type="float" id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3">.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.1c">dr=.5</annotation></semantics></math>) and a final prediction layer for the binary class (sigmoid activation). The embedding layer is randomly initialized.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>High performance for sentiment analysis in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Kim, 2014</a>]</cite> is achieved by initializing the embedding layer with pre-trained word2vec embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Mikolov et al., 2013</a>]</cite> obtained from very large empirical data sets. Since we operate with data from two different languages, do not classify for semantic categories such as sentiments and also have a situation of rather noisy OCR data, we refrained from using pre-trained word embeddings in our setup.</span></span></span> Learning for this network was performed using RMSProp optimization with learning rate 0.0002 and mini-batches of size 32.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CNN for image data:</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px2.p1.2" class="ltx_p">Following the works in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Noce et al., 2016</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Gallo et al., 2016</a>]</cite>, we use a very deep CNN architecture to classify scanned pages based on their binarized and resized representation as 224<math id="S4.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS0.Px2.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.1.m1.1b"><times id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.1.m1.1c">\times</annotation></semantics></math>224 pixels. We employ a network of 16 weight layers with very small convolution filters (<math id="S4.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS2.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1"><times id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.2">3</cn><cn type="integer" id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.2.m2.1c">3\times 3</annotation></semantics></math>) and max pooling as introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Simonyan and Zisserman, 2014</a>]</cite>. The network is initialized with pretrained weights based on the ‚Äòimagenet‚Äô dataset (VGG-16). Actually, ‚Äòimagenet‚Äô provides manually labeled photographs for object recognition tasks. But, earlier work has shown that CNN weights pre-trained on imagenet, although not specifically intended for the task of document image classification, can significantly improve DIC results for small datasets, too <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Harley et al., 2015</a>]</cite>. Hence, we expect them to be beneficial also for our PSS task.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px2.p2.2" class="ltx_p">To allow the network to adapt to our specific data and classification task, we applied a common technique of fine-tuning pretrained deep CNN. For this, we removed the final prediction layer and flattened the output of the last fully connected layer. Then, we fixate all weights of the original model layers. On top of this architecture, we added a new trainable, fully connected layer with 256 units and dropout regularization (<math id="S4.SS2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="dr=.5" display="inline"><semantics id="S4.SS2.SSS0.Px2.p2.1.m1.1a"><mrow id="S4.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mrow id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml"><mi id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.2" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.1" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.3" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.1" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1"><eq id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.1"></eq><apply id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2"><times id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.1"></times><ci id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.2">ùëë</ci><ci id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.3.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.3">ùëü</ci></apply><cn type="float" id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3">.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p2.1.m1.1c">dr=.5</annotation></semantics></math>), and a new final prediction layer (sigmoid activation) for our binary classification task. Learning for this network was performed using the Adam optimizer with a small learning rate (<math id="S4.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="lr=0.0001" display="inline"><semantics id="S4.SS2.SSS0.Px2.p2.2.m2.1a"><mrow id="S4.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"><mrow id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml"><mi id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.2" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.1" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.3" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.1" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.3" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p2.2.m2.1b"><apply id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1"><eq id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.1"></eq><apply id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2"><times id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.1.cmml" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.1"></times><ci id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.2.cmml" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.2">ùëô</ci><ci id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.3.cmml" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.2.3">ùëü</ci></apply><cn type="float" id="S4.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px2.p2.2.m2.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p2.2.m2.1c">lr=0.0001</annotation></semantics></math>) and mini-batches of size 32.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Combining text and visual features:</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">Each of the two previously introduced CNN are capable of classifying pages into either SD or ND on their own. But, since different information is utilized in each approach, we expect a performance gain from combining textual and visual information. For this, we modify the two previously introduced models in the same way. First, each model is trained on the training data individually. Then, we remove the final prediction layer from each model. In a next step, each example from the training and test data is fed into the networks again, to receive prediction values from the last fully connected layers of the two pruned networks. The output values from these last layers can be interpreted as new feature vectors for each data instance which encode dimensionality-reduced information from both, text and images.</p>
</div>
<div id="S4.SS2.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px3.p2.2" class="ltx_p">From the text-based CNN, we receive a feature vector of 256 dimensions for each page according to the last dense layer of the model. To this vector, we concatenate the <math id="S4.SS2.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS2.SSS0.Px3.p2.1.m1.1a"><mi id="S4.SS2.SSS0.Px3.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p2.1.m1.1b"><ci id="S4.SS2.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p2.1.m1.1c">K</annotation></semantics></math> inferred topic proportion features and the two topic distance features from our baseline approach. Since text features from predecessor pages proved to be very useful in SVM baseline classification, we also use features from neighbor pages in our final model.
For this, we concatenate the vector of the current page with the vectors from its two predecessor pages to one text feature vector of length <math id="S4.SS2.SSS0.Px3.p2.2.m2.1" class="ltx_Math" alttext="3\times(256+K+2)" display="inline"><semantics id="S4.SS2.SSS0.Px3.p2.2.m2.1a"><mrow id="S4.SS2.SSS0.Px3.p2.2.m2.1.1" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.cmml"><mn id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.3" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.2" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.2.cmml">√ó</mo><mrow id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.2" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.cmml"><mn id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.2" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.2.cmml">256</mn><mo id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.1" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.1.cmml">+</mo><mi id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.3" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.cmml">K</mi><mo id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.1a" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.1.cmml">+</mo><mn id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.4" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.4.cmml">2</mn></mrow><mo stretchy="false" id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.3" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p2.2.m2.1b"><apply id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1"><times id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.2"></times><cn type="integer" id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.3">3</cn><apply id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1"><plus id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.1"></plus><cn type="integer" id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.2">256</cn><ci id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.3">ùêæ</ci><cn type="integer" id="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.4.cmml" xref="S4.SS2.SSS0.Px3.p2.2.m2.1.1.1.1.1.4">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p2.2.m2.1c">3\times(256+K+2)</annotation></semantics></math>. In a last step, we concatenate the 256 image features from the image-based CNN to receive a final vector of 1,180 (Tobacco800), resp. 1,330 (Archive22k) dimensions.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Concatenating text features from a window size 3 has been decided experimentally. We also found that concatenating image features from predecessor pages did not improve the final performance.</span></span></span></p>
</div>
<div id="S4.SS2.SSS0.Px3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px3.p3.3" class="ltx_p">These final feature vectors now encode both, text and visual information from each page. They serve as input for a new MLP network consisting of 256 fully connected nodes with ‚ÄúReLU‚Äù activation and l2-regularization (<math id="S4.SS2.SSS0.Px3.p3.1.m1.1" class="ltx_Math" alttext="factor=0.01" display="inline"><semantics id="S4.SS2.SSS0.Px3.p3.1.m1.1a"><mrow id="S4.SS2.SSS0.Px3.p3.1.m1.1.1" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.cmml"><mrow id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.cmml"><mi id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.2" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.3" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1a" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.4" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1b" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.5" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1c" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.6" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1d" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.7" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.7.cmml">r</mi></mrow><mo id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.1" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.3" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p3.1.m1.1b"><apply id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1"><eq id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.1"></eq><apply id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2"><times id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.1"></times><ci id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.2.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.2">ùëì</ci><ci id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.3.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.3">ùëé</ci><ci id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.4.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.4">ùëê</ci><ci id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.5.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.5">ùë°</ci><ci id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.6.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.6">ùëú</ci><ci id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.7.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.2.7">ùëü</ci></apply><cn type="float" id="S4.SS2.SSS0.Px3.p3.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px3.p3.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p3.1.m1.1c">factor=0.01</annotation></semantics></math>), followed by dropout regularization (<math id="S4.SS2.SSS0.Px3.p3.2.m2.1" class="ltx_Math" alttext="dr=.5" display="inline"><semantics id="S4.SS2.SSS0.Px3.p3.2.m2.1a"><mrow id="S4.SS2.SSS0.Px3.p3.2.m2.1.1" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.cmml"><mrow id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.cmml"><mi id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.2" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.1" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.3" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.1" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.3" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.3.cmml">.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p3.2.m2.1b"><apply id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1"><eq id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.1"></eq><apply id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2"><times id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.1.cmml" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.1"></times><ci id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.2.cmml" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.2">ùëë</ci><ci id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.3.cmml" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.2.3">ùëü</ci></apply><cn type="float" id="S4.SS2.SSS0.Px3.p3.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px3.p3.2.m2.1.1.3">.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p3.2.m2.1c">dr=.5</annotation></semantics></math>) and a final, fully connected prediction layer with sigmoid activation. Learning is performed using the Adam optimizer (<math id="S4.SS2.SSS0.Px3.p3.3.m3.1" class="ltx_Math" alttext="lr=0.0005" display="inline"><semantics id="S4.SS2.SSS0.Px3.p3.3.m3.1a"><mrow id="S4.SS2.SSS0.Px3.p3.3.m3.1.1" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.cmml"><mrow id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.cmml"><mi id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.2" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.1" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.3" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.1" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.3" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.3.cmml">0.0005</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p3.3.m3.1b"><apply id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1"><eq id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.1"></eq><apply id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.cmml" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2"><times id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.1.cmml" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.1"></times><ci id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.2.cmml" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.2">ùëô</ci><ci id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.3.cmml" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.2.3">ùëü</ci></apply><cn type="float" id="S4.SS2.SSS0.Px3.p3.3.m3.1.1.3.cmml" xref="S4.SS2.SSS0.Px3.p3.3.m3.1.1.3">0.0005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p3.3.m3.1c">lr=0.0005</annotation></semantics></math>) and a batch size of 16.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Quantitative evaluation:</span> Table¬†<a href="#S5.T2" title="Table 2 ‚Ä£ 5. Evaluation ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> displays the results of all tested model architectures and features types for PSS on our two investigated data sets.
Performance is measured by the accuracy of identification of a new document beginning vs. continuity of the same document. Since the distribution of both classes is fairly uneven due to different document length (there are a lot more pages in the SD class), we additionally use kappa statistics to report a chance-corrected agreement between human and machine separations of page streams.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation of page stream segmentation</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Approach/dataset</span></th>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center" colspan="2"><span id="S5.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Archive22k</span></td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center" colspan="2"><span id="S5.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Tobacco800</span></td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<th id="S5.T2.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_right">Acc.</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_right">kappa</td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_align_right">Acc.</td>
<td id="S5.T2.1.2.2.5" class="ltx_td ltx_align_right">kappa</td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<th id="S5.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SVM unigrams</th>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t">0.840</td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">0.421</td>
<td id="S5.T2.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">0.829</td>
<td id="S5.T2.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">0.640</td>
</tr>
<tr id="S5.T2.1.4.4" class="ltx_tr">
<th id="S5.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ topic composition</th>
<td id="S5.T2.1.4.4.2" class="ltx_td ltx_align_right">0.839</td>
<td id="S5.T2.1.4.4.3" class="ltx_td ltx_align_right">0.419</td>
<td id="S5.T2.1.4.4.4" class="ltx_td ltx_align_right">0.829</td>
<td id="S5.T2.1.4.4.5" class="ltx_td ltx_align_right">0.640</td>
</tr>
<tr id="S5.T2.1.5.5" class="ltx_tr">
<th id="S5.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ topic difference</th>
<td id="S5.T2.1.5.5.2" class="ltx_td ltx_align_right">0.847</td>
<td id="S5.T2.1.5.5.3" class="ltx_td ltx_align_right">0.446</td>
<td id="S5.T2.1.5.5.4" class="ltx_td ltx_align_right">0.837</td>
<td id="S5.T2.1.5.5.5" class="ltx_td ltx_align_right">0.657</td>
</tr>
<tr id="S5.T2.1.6.6" class="ltx_tr">
<th id="S5.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ predecessor page</th>
<td id="S5.T2.1.6.6.2" class="ltx_td ltx_align_right">0.855</td>
<td id="S5.T2.1.6.6.3" class="ltx_td ltx_align_right">0.446</td>
<td id="S5.T2.1.6.6.4" class="ltx_td ltx_align_right">0.822</td>
<td id="S5.T2.1.6.6.5" class="ltx_td ltx_align_right">0.624</td>
</tr>
<tr id="S5.T2.1.7.7" class="ltx_tr">
<th id="S5.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CNN Text</th>
<td id="S5.T2.1.7.7.2" class="ltx_td ltx_align_right ltx_border_t">0.904</td>
<td id="S5.T2.1.7.7.3" class="ltx_td ltx_align_right ltx_border_t">0.594</td>
<td id="S5.T2.1.7.7.4" class="ltx_td ltx_align_right ltx_border_t">0.760</td>
<td id="S5.T2.1.7.7.5" class="ltx_td ltx_align_right ltx_border_t">0.493</td>
</tr>
<tr id="S5.T2.1.8.8" class="ltx_tr">
<th id="S5.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CNN Image</th>
<td id="S5.T2.1.8.8.2" class="ltx_td ltx_align_right">0.884</td>
<td id="S5.T2.1.8.8.3" class="ltx_td ltx_align_right">0.515</td>
<td id="S5.T2.1.8.8.4" class="ltx_td ltx_align_right">0.837</td>
<td id="S5.T2.1.8.8.5" class="ltx_td ltx_align_right">0.654</td>
</tr>
<tr id="S5.T2.1.9.9" class="ltx_tr">
<th id="S5.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MLP Image + Text</th>
<td id="S5.T2.1.9.9.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.1.9.9.2.1" class="ltx_text ltx_font_bold">0.929</span></td>
<td id="S5.T2.1.9.9.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.1.9.9.3.1" class="ltx_text ltx_font_bold">0.691</span></td>
<td id="S5.T2.1.9.9.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.1.9.9.4.1" class="ltx_text ltx_font_bold">0.911</span></td>
<td id="S5.T2.1.9.9.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.1.9.9.5.1" class="ltx_text ltx_font_bold">0.816</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">The text features specifically engineered for PSS based on LDA topic composition and difference between consecutive pages improve the SVM results for text-based classification. Adding features from the predecessor page improves results for one dataset (Archive22k), but not for the other (Tobacco800).</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">For the German dataset, we can observe that document boundaries can be identified more accurately with the CNN architectures than with linear SVM classification. For the English dataset, SVM constantly beats convolutional neural net classification on text features, but not on image features. One potential reason might be the rather small size of the dataset which does not contain enough examples for the complex CNN architecture to learn from.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p">For both datasets, accuracy and kappa statistics improve significantly when image and text feature types are combined in one MLP architecture. The classifier achieves circa 93¬†% accuracy on the German dataset and more than 91¬†% on the English data. Compared to the results reported by the studies in section¬†<a href="#S2" title="2. Related work ‚Ä£ Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.</span></a>, this can be regarded as a new state-of-the-art for page stream segmentation.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Qualitative evaluation:</span> Although first pages and subsequent pages of documents can be distinguished with high accuracy, our improved PSS approach still makes a considerable number of errors. There are two types of errors for the binary classification of pages: False positives (FP) and false negatives (FN). According to the manually separated pages in the gold standard, FP are subsequent pages (class SD) that are recognized by the classifier as first page (class ND). FN are defined the other way around.
For SVM classification in the German dataset, FP account for about three quarters of all errors in the test set. FN make up about one quarter of all errors. This mismatch means that automatic PSS potentially splits the page stream into more documents than there are actually in the gold standard.</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p">For the final MLP architecture, we observe not only an increased accuracy but also a more balanced ratio between FP and FN. Apparently, the architecture is able to avoid more FP errors that FN errors resulting in less (incorrect) document splits.
On closer inspection, the remaining FPs often prove to contain characteristics of valid first pages, e.g. the beginning of a sub-document attached to one main document. This means although an automatic split is counted as an error in the quantitative evaluation, it nevertheless can represent a meaningful, content-related split for our application of retro-digitizing a large paper archive.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We presented a new approach for page stream segmentation based on binary classification of pages. Our approach combines two convolutional neural networks to create features from image and text data which are used as input for a third MLP network. Our approach achieves very high accuracy for the task to identify the beginning of a new document in a flow of scanned document pages. An accuracy above 91¬†% for the Tobacco800 dataset which has been used in previous studies on this task, and accuracy of 93¬†% on our own dataset can be regarded as a new state-of-the-art for this task.
The approach allowed us to drastically reduce costs for separating batch-scanned pages into document units in our project of retro-digitizing a research archive of around one million pages.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">This work has been realized at the Leipzig University in the joint research project ‚ÄúKnowledge Management of Legacy Documents in Science, Administration and Industry‚Äù together with the Helmholtz Research Centre for Environmental Health in Munich and the CID GmbH, Freigericht. The authors thank colleagues at Helmholtz and CID for their valuable support.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Bibliographical References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agin et al., 2015</span>
<span class="ltx_bibblock">
Agin, O., Ulas, C., Ahat, M., and Bekar, C.

</span>
<span class="ltx_bibblock">(2015).

</span>
<span class="ltx_bibblock">An approach to the segmentation of multi-page document flow using
binary classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.1.1" class="ltx_text ltx_font_italic">6th International Conference on Graphic and Image Processing
(ICGIP 2014)</span>.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blei et al., 2003</span>
<span class="ltx_bibblock">
Blei, D.¬†M., Ng, A.¬†Y., and Jordan, M.¬†I.

</span>
<span class="ltx_bibblock">(2003).

</span>
<span class="ltx_bibblock">Latent dirichlet allocation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 3:993‚Äì1022.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Daher and Bela√Ød, 2014</span>
<span class="ltx_bibblock">
Daher, H. and Bela√Ød, A.

</span>
<span class="ltx_bibblock">(2014).

</span>
<span class="ltx_bibblock">Document flow segmentation for business applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">Document Recognition and Retrieval XXI</span>, pages 9201‚Äì9215,
San Francisco, France.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al., 2008</span>
<span class="ltx_bibblock">
Fan, R.-e., Chang, K.-w., Hsieh, C.-j., Wang, X.-r., and Lin, C.-j.

</span>
<span class="ltx_bibblock">(2008).

</span>
<span class="ltx_bibblock">Liblinear: A library for large linear classification.

</span>
<span class="ltx_bibblock"><span id="bib.bibx4.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 9:1871‚Äì1874.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gallo et al., 2016</span>
<span class="ltx_bibblock">
Gallo, I., Noce, L., Zamberletti, A., and Calefati, A.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">Deep neural networks for page stream segmentation and classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.1.1" class="ltx_text ltx_font_italic">International Conference on Digital Image Computing:
Techniques and Applications (DICTA)</span>, pages 1‚Äì7.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordo et al., 2013</span>
<span class="ltx_bibblock">
Gordo, A., Rusi√±ol, M., Karatzas, D., and Bagdanov, A.¬†D.

</span>
<span class="ltx_bibblock">(2013).

</span>
<span class="ltx_bibblock">Document classification and page stream segmentation for digital
mailroom applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">12th International Conference on Document Analysis and
Recognition</span>, pages 621‚Äì625.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harley et al., 2015</span>
<span class="ltx_bibblock">
Harley, A., Ufkes, A., and Derpanis, K.

</span>
<span class="ltx_bibblock">(2015).

</span>
<span class="ltx_bibblock">Evaluation of deep convolutional nets for document image
classification and retrieval.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">13th International Conference on Document Analysis and
Recognition (ICDAR)</span>, pages 991‚Äì995.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Isemann et al., 2014</span>
<span class="ltx_bibblock">
Isemann, D., Niekler, A., Pre√üler, B., Viereck, F., and Heyer, G.

</span>
<span class="ltx_bibblock">(2014).

</span>
<span class="ltx_bibblock">OCR of legacy documents as a building block in industrial disaster
prevention.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">DIMPLE Workshop on DIsaster Management and Principled
Large-scale information Extraction for and post emergency logistics, LREC
2014</span>.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joachims, 1998</span>
<span class="ltx_bibblock">
Joachims, T.

</span>
<span class="ltx_bibblock">(1998).

</span>
<span class="ltx_bibblock">Text categorization with support vector machines: learning with many
relevant features.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim, 2014</span>
<span class="ltx_bibblock">
Kim, Y.

</span>
<span class="ltx_bibblock">(2014).

</span>
<span class="ltx_bibblock">Convolutional neural networks for sentence classification.

</span>
<span class="ltx_bibblock"><span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">EMNLP 2014</span>.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al., 2006</span>
<span class="ltx_bibblock">
Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., and Heard, J.

</span>
<span class="ltx_bibblock">(2006).

</span>
<span class="ltx_bibblock">Building a test collection for complex document information
processing.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">Proc. 29th Annual Int. ACM SIGIR Conference</span>, pages 665‚Äì666.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al., 2013</span>
<span class="ltx_bibblock">
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J.

</span>
<span class="ltx_bibblock">(2013).

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niekler and J√§hnichen, 2012</span>
<span class="ltx_bibblock">
Niekler, A. and J√§hnichen, P.

</span>
<span class="ltx_bibblock">(2012).

</span>
<span class="ltx_bibblock">Matching results of latent dirichlet allocation for text.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.1.1" class="ltx_text ltx_font_italic">Proceedings of the 11th International Conference on Cognitive
Modeling (ICCM 2012)</span>, pages 317‚Äì322. Universit√§tsverlag der TU
Berlin.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noce et al., 2016</span>
<span class="ltx_bibblock">
Noce, L., Gallo, I., Zamberletti, A., and Calefati, A.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">Embedded textual content for document image classification with
convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 ACM Symposium on Document Engineering
(DocEng)</span>, pages 165‚Äì173, New York. ACM.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Otsu, 1979</span>
<span class="ltx_bibblock">
Otsu, N.

</span>
<span class="ltx_bibblock">(1979).

</span>
<span class="ltx_bibblock">A threshold selection method from gray-level histograms.

</span>
<span class="ltx_bibblock"><span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Systems, Man, and Cybernetics</span>,
9(1):62‚Äì66, jan.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phan et al., 2011</span>
<span class="ltx_bibblock">
Phan, X.-H., Nguyen, C.-T., Le, D.-T., Nguyen, L.-M., Horiguchi, S., and Ha,
Q.-T.

</span>
<span class="ltx_bibblock">(2011).

</span>
<span class="ltx_bibblock">A hidden topic-based framework toward building applications with
short web documents.

</span>
<span class="ltx_bibblock"><span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>,
23(7):961‚Äì976.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman, 2014</span>
<span class="ltx_bibblock">
Simonyan, K. and Zisserman, A.

</span>
<span class="ltx_bibblock">(2014).

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1409.1556.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1710.03005" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/land_of_honey_and_milk" rel="nofollow" aria-hidden="true" tabindex="-1"></a>
    <a href="/log/1710.03006" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1710.03006">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1710.03006" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1710.03007" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar  3 03:31:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
