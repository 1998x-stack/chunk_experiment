<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1710.02560] The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments</title><meta property="og:description" content="This paper introduces the contents and the possible usage of the DIRHA-ENGLISH multi-microphone corpus, recently realized under the EC DIRHA project. The reference scenario is a domestic environment equipped with a lar…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1710.02560">
<link rel="canonical" target="_blank" href="https://ar5iv.labs.arxiv.org/html/1710.02560">

<!--Generated on Sat Mar 16 04:18:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This paper introduces the contents and the possible usage of the DIRHA-ENGLISH multi-microphone corpus, recently realized under the EC DIRHA project. The reference scenario is a domestic environment equipped with a large number of microphones and microphone arrays distributed in space.</p>
<p id="id2.id2" class="ltx_p">The corpus is composed of both real and simulated material, and it includes 12 US and 12 UK English native speakers.
Each speaker uttered different sets of phonetically-rich sentences, newspaper articles, conversational speech, keywords, and commands.
From this material, a large set of 1-minute sequences was generated, which also includes typical domestic background noise as well as inter/intra-room reverberation effects.
Dev and test sets were derived, which represent a very precious material for different studies on multi-microphone speech processing and distant-speech recognition.
Various tasks and corresponding Kaldi recipes have already been developed.</p>
<p id="id3.id3" class="ltx_p">The paper reports a first set of baseline results obtained using different techniques, including Deep Neural Networks (DNN), aligned with the state-of-the-art at international level.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
distant speech recognition, microphone arrays, corpora, Kaldi, DNN</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">During the last decade, much research has been devoted to improve Automatic Speech Recognition (ASR) performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. As a result, ASR has recently been applied in several fields, such as web-search, car control, automated voice answering, radiological reporting and it is currently used by millions of users worldwide.
Nevertheless, most state-of-the-art systems are still based on close-talking solutions, forcing the user to speak very close to a microphone-equipped device.
Although this approach usually leads to better performance, it is easy to predict that, in the future, users will prefer to relax the constraint of handling or wearing any device to access speech recognition services. There are indeed various real-life situations where a distant-talking (far-field)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In the following, the same concept is referred to as “distant-speech”.</span></span></span> interaction is more natural, convenient and attractive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
In particular, amongst all the possible applications, an emerging field is speech-based domestic control, where users might prefer to freely interact with their home appliances without wearing or even handling any microphone-equipped device.
This scenario was addressed under the EC DIRHA (Distant-speech Interaction for Robust Home Applications) project<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The research presented here has been partially funded by the
European Unionâs 7th Framework Programme (FP7/2007-2013) under grant agreement no. 288121 DIRHA (for more details, please see http:// dirha.fbk.eu).</span></span></span>, which had the ultimate goal of developing voice-enabled automated home services based on Distant-Speech Recognition (DSR) in different languages.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the growing interest towards DSR, current technologies still exhibit a significant lack of robustness and flexibility, since the adverse acoustic conditions originated by non-stationary noises and acoustic reverberation make speech recognition significantly more challenging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Although considerable progresses were made at multi-microphone front-end processing level in order to feed ASR with an enhanced speech input <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the performance loss observed from close-talking to distant-speech remains quite critical, even when the most advanced DNN-based backend frameworks are adopted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To further progress, a crucial step regards the selection of data suitable to train and test the various speech processing, enhancement, and recognition algorithms.
Collecting and transcribing sufficiently large data sets to cover any possible application scenario is a prohibitive, time-consuming and expensive task. In a domestic context, in particular, due to the large variabilities that can be introduced when deploying such systems in different houses, this issue becomes even more challenging than in any other traditional ASR application.
In this context, an ideal system should be flexible enough in terms of microphone distribution in space, and in terms of other possible profiling actions. Moreover, it must be able to provide a satisfactory behaviour immediately after its installation, and to improve performance thanks to its capability to learn from the environment and from the users.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In order to develop such solutions, the availability of high-quality and realistic, multi-microphone corpora represents one of the fundamental steps towards reducing the performance gap between close-talking and distant-speech interaction.
Along this direction,
strong efforts have been spent recently by the international scientific community, through the development of corpora and challenges, such as REVERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, CHIME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and ASpIRE.
Nevertheless, we feel that other complementary corpora and tasks are necessary to the research community in order to further boost technological advances in this field, for instance providing a large number of “observations” of the same acoustic scene.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The DIRHA-ENGLISH corpus complements the set of corpora previously collected under the DIRHA project in other four languages (i.e., Austrian German, Greek, Italian, Portuguese) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. It gives the chance of working on English, the most commonly used language inside the ASR research community, with a very large number of microphone channels, a multi-room setting, and the use of microphone arrays having different characteristics. Half of the material is based on simulations, and half is based on real recordings, which allows one to assess recognition performance in real-world conditions.
It is also worth mentioning that some portions of the corpus will be made publicly available, with free access, in the short term (as done with other data produced by the DIRHA consortium).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The purpose of this paper is to introduce the DIRHA-ENGLISH corpus as well as to provide some baseline results on phonetically-rich sentences, which were obtained using the Kaldi framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
The resulting TIMIT-like phone recognition task can be seen as complementary to the WSJ-like and conversational speech tasks also available for a next distribution.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The remainder of the paper is organized as follows. Section <a href="#S2" title="2 The DIRHA project ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides a brief description of the DIRHA project, while Section <a href="#S3" title="3 The DIRHA-ENGLISH corpus ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> focuses the contents and characteristics of the DIRHA-ENGLISH corpus. Section <a href="#S4" title="4 Experiments and Results ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives a description of the experimental tasks so far defined, and of the corresponding baseline results. Section <a href="#S5" title="5 Conclusions and future work ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> draws some conclusions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The DIRHA project</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The EC DIRHA project, which started in January 2012 and lasted three years, had the goal of addressing acoustic scene analysis and distant-speech interaction in a home environment. In the following, some information are reported about project goals, tasks, and corpora.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Goals and tasks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The application scenario targeted by the project is characterized by a quite flexible voice interactive system to talk with in any room, and from any position in space. Exploiting a microphone network distributed in the different rooms of the apartment, the DIRHA system reacts properly when a command is given by the user. The system is always-listening, waiting for a specific keyword to “capture” in order to begin a dialogue. The dialogue that is triggered in this way, gives the end-user a possible access to devices and services, e.g., open/close doors and windows, switch on/off lights, control the temperature, play music, etc.
Barge-in (to interact while music/speech prompts are played), speaker verification, concurrent dialogue management (to support simultaneous dialogues with different users) are some advanced features characterizing the system.
Finally, a very important aspect to mention is the need to limit the rate of false alarms, due to possible misinterpretation of normal conversations or of other sounds captured by the microphones, which do not carry any relevant message to the system.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Starting from these targeted functionalities, several experimental tasks were defined concerning the combination between front-end processing algorithms (e.g., of speaker localization, acoustic echo cancellation, speech enhancement, etc.) and an ASR backend, in each language.
Most of these tasks were referred to voice interaction in the ITEA apartment<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We would like to thank ITEA S.p.A (Istituto Trentino per l’Edilizia Abitativa) for making available the apartment used for this research.</span></span></span>, situated in Trento (Italy), which was the main site for acoustic and speech data collection.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>DIRHA corpora</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The DIRHA corpora were designed in order to provide multi-microphone data sets that can be used to investigate a wide range of tasks as those mentioned above.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Some data sets were based on simulations realized applying a contamination method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> that combines clean-speech signals, estimated Impulse Responses (IRs), and real multichannel background noise sequences, as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
Other corpora were recorded under real-world conditions.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Besides the DIRHA-ENGLISH corpus described in the next section, other corpora developed in the project are:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">The DIRHA Sim corpus described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> (30 speakers x 4 languages), which consists of 1-minute multi-channel sequences including different acoustic events and speech utterances;
</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">A Wizard-of-OZ data set proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to evaluate the performance of speech activity detection and speaker localization components;</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">The DIRHA AEC corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, which includes data specifically created for studies on Acoustic Echo Cancellation (AEC), to suppress known interferences diffused in the environment (e.g., played music);</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">The DIRHA-GRID corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, a multi-microphone multi-room simulated data set that derives from contaminating the GRID corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> of short commands in the English language.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The DIRHA-ENGLISH corpus</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As done for the other four languages, also the DIRHA-ENGLISH corpus consists of a real and a simulated data set, the latter one deriving from contamination of a clean speech data set described next.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Clean speech material</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The clean speech data set was realized in a recording studio of FBK,
using professional equipment (e.g., a Neumann TLM 103 microphone) to obtain high-quality 96 kHz - 24 bit material.
</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">12 UK and 12 US speakers were recorded (6 males and 6 females, for each language).
For each of them, the corresponding recorded material includes:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">15 read commands;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">15 spontaneous commands;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">13 keywords;</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">48 phonetically-rich sentences (from the Harvard corpus);</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">66 or 67 sentences from WSJ-5k;</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p">66 or 67 sentences from WSJ-20k;</p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i7.p1" class="ltx_para">
<p id="S3.I1.i7.p1.1" class="ltx_p">about 10 minutes of conversational speech (e.g., the subject was asked to talk about a movie recently seen).</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The total time is about 11 hours. All the utterances were manually annotated by an expert.
For the phonetically-rich sentences, an automatic phone segmentation procedure was applied as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. An expert then checked manually the resulting phone transcriptions and time-aligned boundaries to confirm their reliability.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">For both US and UK English, 6 speakers were assigned to the development set, while the other 6 speakers were assigned to the test set. These assignments were done in order to distribute WSJ sentences as in the original task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. The data set contents are compliant with TIMIT specifications (e.g., file format).</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1710.02560/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="264" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>An outline of the microphone set-up adopted for the DIRHA-ENGLISH corpus. Blue small dots represent digital MEMS microphones, red ones refers to the channels considered for the following experimental activity, while black ones represent the other available microphones. The right pictures show the ceiling array and the two linear harmonic arrays installed in the living-room.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The microphone network</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The ITEA apartment is the reference home environment that was available during the project for data collection as well as for the development of
prototypes and showcases. The flat comprises five rooms which are equipped with a network of several microphones.
Most of them are high-quality omnidirectional microphones (Shure MX391/O), connected to multichannel clocked pre-amp and A/D boards (RME Octamic II), which allowed a perfectly synchronous sampling at 48 kHz, with 24 bit resolution.
The bathroom and two other rooms were equipped with a limited number of microphone pairs and triplets (i.e., overall 12 microphones), while the living-room and the kitchen comprise the largest concentration of sensors and devices. As shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Clean speech material ‣ 3 The DIRHA-ENGLISH corpus ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the living-room includes three microphone pairs, a microphone triplet, two 6-microphone ceiling arrays (one consisting of MEMS digital microphones), two harmonic arrays (consisting of 15 electret microphones and 15 MEMS digital microphones, respectively).
More details about this facility can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Concerning this microphone network, a strong effort was devoted to characterize the environment at acoustic level, through different campaigns of IR estimation, leading to more than 10.000 IRs that describe sound propagation from different positions in space (and different possible orientations of the sound source) to any of the available microphones.
The method adopted to estimate an IR consists in diffusing
a known Exponential Sine Sweep (ESS) signal in the target environment, and recording it by the available microphones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
The accuracy of the resulting IR estimation has a remarkable impact on the speech recognition performance, as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
For more details on the creation of the ITEA IR database, please refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Note that the microphone network considered for the DIRHA-ENGLISH data set (shown in Fig.<a href="#S3.F1" title="Figure 1 ‣ 3.1 Clean speech material ‣ 3 The DIRHA-ENGLISH corpus ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) is limited to the living-room and to the kitchen of the ITEA apartment, but also considers harmonic arrays and MEMS microphones which were unavailable in the other DIRHA corpora.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Simulated data set</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The DIRHA-ENGLISH simulated data sets derive from the clean speech described in Section <a href="#S3.SS1" title="3.1 Clean speech material ‣ 3 The DIRHA-ENGLISH corpus ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, and from the application of the contamination method discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The resulting corpus consists of a large number of 1-minute sequences, each including a variable number of sentences uttered in the living-room with different noisy conditions.
Four types of sequences have been created, corresponding to the respective following tasks: 1) Phonetically-rich sentences<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>While noisy conditions are quite challenging for the WSJ and conversational parts of the DIRHA-English corpus, the phonetically-rich sequences are characterized by more favorable conditions in order to make this material more suitable for studies on reverberation effects only.</span></span></span>; 2) WSJ 5-k utterances; 3) WSJ 20-k utterances; 4) Conversational speech (also including keywords and commands).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">For each sequence, 62 microphone channels are available, as outlined in Section <a href="#S3.SS2" title="3.2 The microphone network ‣ 3 The DIRHA-ENGLISH corpus ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Real data set</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">For what concerns real recordings, each subject was positioned in the living-room and read the material from a tablet, standing still or sitting on a chair, in a given position. After each set, she/he was asked to move to a different position and take a different orientation.
For each speaker, the recorded material corresponds to the same list of contents reported in Section <a href="#S3.SS1" title="3.1 Clean speech material ‣ 3 The DIRHA-ENGLISH corpus ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> for the clean speech data set.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Note also that all the channels recorded through MEMS digital microphones were time-aligned with the others during a post-processing step (since using the same clock for all the platforms was not feasible due to different settings and sampling frequency in the case of MEMS devices) .</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Once collected the whole real material, 1-minute sequences were derived from it in order to ensure a coherence in terms of sequence between simulation and real data sets.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section describes the proposed task and the related baseline experiments concerning the US phonetically-rich portion of the DIRHA-English corpus.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>ASR framework</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Training and testing corpora</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">In this work, the training phase is accomplished with the train part of the TIMIT corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. For the DSR experiments, the original TIMIT dataset is reverberated using three impulse responses measured in the living-room. Moreover, some multi-channel noisy background sequences are added to the reverberated signals, in order to better match real-world conditions. Both the impulse responses and the noisy sequences are different from those used to generate the DIRHA-ENGLISH simulated data set.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">The test phase is conducted using the real and the simulated phonetically-rich sentences of the DIRHA-English data set.
In both cases, for each 1-minute sequence an oracle voice activity detector (VAD) is applied in the next experiments<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Alternative tasks (not presented here) have been defined as well with the same material, in order to investigate VAD and ASR components together.</span></span></span>, in order to avoid any possible bias due to inconsistent sentence boundaries.
A down-sampling of the speech sequences from 48 to 16 kHz is finally performed.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Feature extraction</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">A standard feature extraction based on MFCCs is applied to the speech sentences. In particular, the signal is blocked into frames of 25 ms with 10 ms overlapping and, for each frame, 13 MFCCs are extracted. The resulting features, together with their first and second order derivatives, are then arranged into a single observation vector of 39 components.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Acoustic model training</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">In the following experiments, three different acoustic models of increasing complexity are considered. The procedure adopted for training such models is the same as that used for the original s5 TIMIT Kaldi recipe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The first baseline (<span id="S4.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">mono</span>), refers to a simple system characterized by 48 context-independent phones of the English language, each modeled by a three state left-to-right HMM (overall using 1000 gaussians).
The second baseline (<span id="S4.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_italic">tri</span>) is based on a context-dependent phone modeling and on speaker adaptive training (SAT). Overall, 2.5k tied states with 15k gaussians are employed. Finally, the DNN baseline (<span id="S4.SS1.SSS3.p1.1.3" class="ltx_text ltx_font_italic">DNN</span>), trained with the Karel’s recipe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, is composed of 6 hidden layers of 1024 neurons, with a context window of 11 consecutive frames (5 before and 5 after the current frame) and an initial learning rate of 0.008.
</p>
</div>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Proposed task and evaluation</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p id="S4.SS1.SSS4.p1.1" class="ltx_p">The original Kaldi recipe is based on a bigram language model estimated from the phone transcriptions available in the training set. Conversely, we propose the adoption of a pure phone-loop (i.e., zero-gram based) task, in order to avoid any non-linear influence and artifacts possibly originated by a language model. Our past experience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> indeed suggests that, even though the use of language models is certainly helpful in increasing the recognition performance, the adoption of a simple phone-loop task is more suitable for experiments purely focusing on the acoustic information.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/1710.02560/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="212" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.3.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>The phrase “<span id="S4.F2.4.2" class="ltx_text ltx_font_italic">a chicken leg</span>” uttered in close and distant-talking scenarios, respectively. The closures (in red) are dimmed by the reverberation tail in the distant speech.</figcaption>
</figure>
<div id="S4.SS1.SSS4.p2" class="ltx_para">
<p id="S4.SS1.SSS4.p2.1" class="ltx_p">Another difference with the original Kaldi recipe regards the evaluation of silences and closures. In the evaluation phase, the standard Kaldi recipe (based on <span id="S4.SS1.SSS4.p2.1.1" class="ltx_text ltx_font_italic">Sclite</span>) maps the original 48 English phones into a reduced set of 39 units, as originally done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. In particular, the six closures (<span id="S4.SS1.SSS4.p2.1.2" class="ltx_text ltx_font_italic">bcl</span>, <span id="S4.SS1.SSS4.p2.1.3" class="ltx_text ltx_font_italic">dcl</span>, <span id="S4.SS1.SSS4.p2.1.4" class="ltx_text ltx_font_italic">gcl</span>, <span id="S4.SS1.SSS4.p2.1.5" class="ltx_text ltx_font_italic">kcl</span>, <span id="S4.SS1.SSS4.p2.1.6" class="ltx_text ltx_font_italic">pcl</span>, <span id="S4.SS1.SSS4.p2.1.7" class="ltx_text ltx_font_italic">tcl</span>) are mapped as “optional silences” and possible deletions of such units are not scored as errors. These phones would be likely considered as correct, since deletions of short closures occur very frequently.
We believe that the latter aspect might introduce a bias in the evaluation metrics, especially for DSR tasks, where the reverberation tail makes the recognition of the closures nearly infeasible, as highlighted in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1.4 Proposed task and evaluation ‣ 4.1 ASR framework ‣ 4 Experiments and Results ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For this reason, we propose to simply filter out all the silences and closures from both the reference and the hypothesized phone sequences. This leads to a performance reduction, since all the favorable optional silences added in the original recipe are avoided. However, a more coherent estimation of the recognition rates concerning phones as occlusive and vowels is reached.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baseline results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">This section provides some baseline results<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Part of the experiments are conducted with a Tesla K40 donated by the NVIDIA Corporation.</span></span></span>, which might be useful in the future to other researchers for reference purposes.
In the following sections, results based on close-talking and distant-speech input are presented.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Close-talking performance</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.2.1 Close-talking performance ‣ 4.2 Baseline results ‣ 4 Experiments and Results ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports the performance obtained by decoding the clean sentences recorded in the FBK recording studio with either a phone bigram language model or a simple phone-loop. Results are provided using both the standard Kaldi s5 and the proposed recipe, in order to highlight all the discrepancies in performance that can be observed in these different experimental settings.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Recipe</span></th>
<td id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">LM Type</span></td>
<td id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Mono</span></td>
<td id="S4.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.1.1.4.1" class="ltx_text" style="font-size:90%;">Tri</span></td>
<td id="S4.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.1.1.5.1" class="ltx_text" style="font-size:90%;">DNN</span></td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Standard Kaldi s5</span></th>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Bigram LM</span></td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.3.1" class="ltx_text" style="font-size:90%;">36.4</span></td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.4.1" class="ltx_text" style="font-size:90%;">23.2</span></td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.5.1" class="ltx_text" style="font-size:90%;">20.1</span></td>
</tr>
<tr id="S4.T1.2.3.3" class="ltx_tr">
<th id="S4.T1.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.3.3.1.1" class="ltx_text" style="font-size:90%;">Standard Kaldi s5</span></th>
<td id="S4.T1.2.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.3.3.2.1" class="ltx_text" style="font-size:90%;">Phone-loop</span></td>
<td id="S4.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.3.3.3.1" class="ltx_text" style="font-size:90%;">39.4</span></td>
<td id="S4.T1.2.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.3.3.4.1" class="ltx_text" style="font-size:90%;">26.3</span></td>
<td id="S4.T1.2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.3.3.5.1" class="ltx_text" style="font-size:90%;">22.4</span></td>
</tr>
<tr id="S4.T1.2.4.4" class="ltx_tr">
<th id="S4.T1.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Proposed Evaluation</span></th>
<td id="S4.T1.2.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.4.2.1" class="ltx_text" style="font-size:90%;">Bigram LM</span></td>
<td id="S4.T1.2.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.4.3.1" class="ltx_text" style="font-size:90%;">42.7</span></td>
<td id="S4.T1.2.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.4.4.1" class="ltx_text" style="font-size:90%;">28.6</span></td>
<td id="S4.T1.2.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.4.5.1" class="ltx_text" style="font-size:90%;">24.6</span></td>
</tr>
<tr id="S4.T1.2.5.5" class="ltx_tr">
<th id="S4.T1.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.5.5.1.1" class="ltx_text" style="font-size:90%;">Proposed Evaluation</span></th>
<td id="S4.T1.2.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.5.5.2.1" class="ltx_text" style="font-size:90%;">Phone-loop</span></td>
<td id="S4.T1.2.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.5.5.3.1" class="ltx_text" style="font-size:90%;">46.7</span></td>
<td id="S4.T1.2.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.5.5.4.1" class="ltx_text" style="font-size:90%;">32.5</span></td>
<td id="S4.T1.2.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">27.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.5.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Phone Error Rate (PER%) performance obtained applying different Kaldi recipes to the phonetically-rich sentence (dev-set) acquired in the FBK recording studio.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">As expected, these results highlight that the system performance is significantly improved when passing from a simple monophone-based model to a more competitive DNN baseline.
Moreover, as outlined in Sec.<a href="#S4.SS1.SSS4" title="4.1.4 Proposed task and evaluation ‣ 4.1 ASR framework ‣ 4 Experiments and Results ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.4</span></a>, applying the original Kaldi evaluation provides a mismatch of about 20% in relative error reduction, which does not correspond to any real system improvement. Next experiments will be based on the pure phone-loop grammar scored with the proposed evaluation method.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Single distant-microphone performance</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">In this section, the results obtained with a single distant microphone are discussed. Table <a href="#S4.T2" title="Table 2 ‣ 4.2.2 Single distant-microphone performance ‣ 4.2 Baseline results ‣ 4 Experiments and Results ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the performance achieved with some of the microphones highlighted in Fig.<a href="#S3.F1" title="Figure 1 ‣ 3.1 Clean speech material ‣ 3 The DIRHA-ENGLISH corpus ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<td id="S4.T2.2.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="S4.T2.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Simulated Data</span></td>
<td id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="S4.T2.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Real Data</span></td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S4.T2.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Mic. ID</span></td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Mono</span></td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Tri</span></td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.4.1" class="ltx_text" style="font-size:90%;">DNN</span></td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.5.1" class="ltx_text" style="font-size:90%;">Mono</span></td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.6.1" class="ltx_text" style="font-size:90%;">Tri</span></td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.7.1" class="ltx_text" style="font-size:90%;">DNN</span></td>
</tr>
<tr id="S4.T2.2.3.3" class="ltx_tr">
<td id="S4.T2.2.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.1.1" class="ltx_text" style="font-size:90%;">LA6</span></td>
<td id="S4.T2.2.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.2.1" class="ltx_text" style="font-size:90%;">68.8</span></td>
<td id="S4.T2.2.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.3.1" class="ltx_text" style="font-size:90%;">57.7</span></td>
<td id="S4.T2.2.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">51.6</span></td>
<td id="S4.T2.2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.5.1" class="ltx_text" style="font-size:90%;">70.5</span></td>
<td id="S4.T2.2.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.6.1" class="ltx_text" style="font-size:90%;">60.9</span></td>
<td id="S4.T2.2.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">55.1</span></td>
</tr>
<tr id="S4.T2.2.4.4" class="ltx_tr">
<td id="S4.T2.2.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.4.4.1.1" class="ltx_text" style="font-size:90%;">L1C</span></td>
<td id="S4.T2.2.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.4.2.1" class="ltx_text" style="font-size:90%;">67.4</span></td>
<td id="S4.T2.2.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.4.3.1" class="ltx_text" style="font-size:90%;">58.5</span></td>
<td id="S4.T2.2.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.4.4.1" class="ltx_text" style="font-size:90%;">52.4</span></td>
<td id="S4.T2.2.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.4.5.1" class="ltx_text" style="font-size:90%;">70.3</span></td>
<td id="S4.T2.2.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.4.6.1" class="ltx_text" style="font-size:90%;">61.7</span></td>
<td id="S4.T2.2.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.4.7.1" class="ltx_text" style="font-size:90%;">55.6</span></td>
</tr>
<tr id="S4.T2.2.5.5" class="ltx_tr">
<td id="S4.T2.2.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.5.5.1.1" class="ltx_text" style="font-size:90%;">LD07</span></td>
<td id="S4.T2.2.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.5.5.2.1" class="ltx_text" style="font-size:90%;">67.5</span></td>
<td id="S4.T2.2.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.5.5.3.1" class="ltx_text" style="font-size:90%;">58.1</span></td>
<td id="S4.T2.2.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.5.5.4.1" class="ltx_text" style="font-size:90%;">53.2</span></td>
<td id="S4.T2.2.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.5.5.5.1" class="ltx_text" style="font-size:90%;">71.5</span></td>
<td id="S4.T2.2.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.5.5.6.1" class="ltx_text" style="font-size:90%;">62.6</span></td>
<td id="S4.T2.2.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.5.5.7.1" class="ltx_text" style="font-size:90%;">57.3</span></td>
</tr>
<tr id="S4.T2.2.6.6" class="ltx_tr">
<td id="S4.T2.2.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.1.1" class="ltx_text" style="font-size:90%;">KA6</span></td>
<td id="S4.T2.2.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.2.1" class="ltx_text" style="font-size:90%;">76.7</span></td>
<td id="S4.T2.2.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.3.1" class="ltx_text" style="font-size:90%;">67.3</span></td>
<td id="S4.T2.2.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.4.1" class="ltx_text" style="font-size:90%;">64.0</span></td>
<td id="S4.T2.2.6.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.5.1" class="ltx_text" style="font-size:90%;">80.5</span></td>
<td id="S4.T2.2.6.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.6.1" class="ltx_text" style="font-size:90%;">73.6</span></td>
<td id="S4.T2.2.6.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.7.1" class="ltx_text" style="font-size:90%;">70.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.5.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>PER(%) performance obtained with single distant microphones for both the simulated and real dataset of phonetically-rich sentences.</figcaption>
</figure>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">The results clearly highlights that in the case of distant-speech input the ASR performance is dramatically reduced, if compared to a close-talking case.
As already observed with close-talking results, the use of a DNN significantly outperforms the other acoustic modeling approaches. This is consistent for all the considered channels, with both simulated and real data sets. Actually, the performance on real data is slightly worse than that achieved on simulated data, due to a lower SNR characterizing the real recording sessions.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">It is also worth noting that almost all the channels provide a similar performance and a comparable trend over the considered acoustic models. Only the kitchen microphone (KA6) corresponds to a more challenging situation, since all the utterances of both real and simulated data sets were pronounced in the living-room.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Delay-and-sum beamforming performance</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">This section reports the results obtained with a standard delay-and-sum beamforming <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> applied to both the ceiling and the harmonic arrays of the living-room.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<td id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="S4.T3.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Simulated Data</span></td>
<td id="S4.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="S4.T3.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Real Data</span></td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Array ID</span></th>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Mono</span></td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Tri</span></td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.2.4.1" class="ltx_text" style="font-size:90%;">DNN</span></td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.2.5.1" class="ltx_text" style="font-size:90%;">Mono</span></td>
<td id="S4.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.2.6.1" class="ltx_text" style="font-size:90%;">Tri</span></td>
<td id="S4.T3.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.2.7.1" class="ltx_text" style="font-size:90%;">DNN</span></td>
</tr>
<tr id="S4.T3.2.3.3" class="ltx_tr">
<th id="S4.T3.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.2.3.3.1.1" class="ltx_text" style="font-size:90%;">Ceiling arr.</span></th>
<td id="S4.T3.2.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.3.2.1" class="ltx_text" style="font-size:90%;">66.2</span></td>
<td id="S4.T3.2.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.3.3.1" class="ltx_text" style="font-size:90%;">55.9</span></td>
<td id="S4.T3.2.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">50.4</span></td>
<td id="S4.T3.2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.3.5.1" class="ltx_text" style="font-size:90%;">65.9</span></td>
<td id="S4.T3.2.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.3.6.1" class="ltx_text" style="font-size:90%;">55.9</span></td>
<td id="S4.T3.2.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.3.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">50.6</span></td>
</tr>
<tr id="S4.T3.2.4.4" class="ltx_tr">
<th id="S4.T3.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Harmonic arr.</span></th>
<td id="S4.T3.2.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.4.4.2.1" class="ltx_text" style="font-size:90%;">66.2</span></td>
<td id="S4.T3.2.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.4.4.3.1" class="ltx_text" style="font-size:90%;">56.0</span></td>
<td id="S4.T3.2.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.4.4.4.1" class="ltx_text" style="font-size:90%;">51.8</span></td>
<td id="S4.T3.2.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.4.4.5.1" class="ltx_text" style="font-size:90%;">66.2</span></td>
<td id="S4.T3.2.4.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.4.4.6.1" class="ltx_text" style="font-size:90%;">56.2</span></td>
<td id="S4.T3.2.4.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.4.4.7.1" class="ltx_text" style="font-size:90%;">51.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>PER(%) performance obtained with a delay-and-sum beamforming applied to both the ceiling and the linear harmonic array.</figcaption>
</figure>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.2" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.2.3 Delay-and-sum beamforming performance ‣ 4.2 Baseline results ‣ 4 Experiments and Results ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that beamforming is helpful in improving the system performance. For instance, in the case of real data one passes from a PER of 55.1<math id="S4.SS2.SSS3.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS3.p2.1.m1.1a"><mo id="S4.SS2.SSS3.p2.1.m1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.1.m1.1c">\%</annotation></semantics></math>, with the single microphone, to a PER of 50.6<math id="S4.SS2.SSS3.p2.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><mo id="S4.SS2.SSS3.p2.2.m2.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.2.m2.1c">\%</annotation></semantics></math>, when delay-and-sum beamforming is applied to the ceiling array signals.</p>
</div>
<div id="S4.SS2.SSS3.p3" class="ltx_para">
<p id="S4.SS2.SSS3.p3.1" class="ltx_p">Even though the ceiling array is composed of six microphones only, it ensures a slightly better performance when compared with a less compact 13 element harmonic array. This result might be due both to a better position of the ceiling array, which often ensures the presence of a direct path stronger than reflections, and to adoption of higher quality microphones.
The performance improvement introduced by delay-and-sum beamforming is higher with real data, confirming that
spatial filtering techniques are particularly helpful when the acoustic conditions are less stationary and predictable.</p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Microphone selection performance</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">The DIRHA-English corpus can also be used for microphone selection experiments. It would be thus of interest to provide some lower and upper bound performance for a microphone selection technique applied to this data set. Table <a href="#S4.T4" title="Table 4 ‣ 4.2.4 Microphone selection performance ‣ 4.2 Baseline results ‣ 4 Experiments and Results ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> compares the results achieved with random and with oracle selections of the microphone, for each phonetically-rich sentence. For this selection, we considered the six microphones of the living-room, which are depicted as red dots in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Clean speech material ‣ 3 The DIRHA-ENGLISH corpus ‣ The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.1" class="ltx_tr">
<td id="S4.T4.2.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S4.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="S4.T4.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Simulated Data</span></td>
<td id="S4.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="S4.T4.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Real Data</span></td>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<td id="S4.T4.2.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S4.T4.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Mic. Sel.</span></td>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Mono</span></td>
<td id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Tri</span></td>
<td id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.2.4.1" class="ltx_text" style="font-size:90%;">DNN</span></td>
<td id="S4.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.2.5.1" class="ltx_text" style="font-size:90%;">Mono</span></td>
<td id="S4.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.2.6.1" class="ltx_text" style="font-size:90%;">Tri</span></td>
<td id="S4.T4.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.2.7.1" class="ltx_text" style="font-size:90%;">DNN</span></td>
</tr>
<tr id="S4.T4.2.3.3" class="ltx_tr">
<td id="S4.T4.2.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T4.2.3.3.1.1" class="ltx_text" style="font-size:90%;">Random</span></td>
<td id="S4.T4.2.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.3.3.2.1" class="ltx_text" style="font-size:90%;">67.6</span></td>
<td id="S4.T4.2.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.3.3.3.1" class="ltx_text" style="font-size:90%;">57.7</span></td>
<td id="S4.T4.2.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.3.3.4.1" class="ltx_text" style="font-size:90%;">52.4</span></td>
<td id="S4.T4.2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.3.3.5.1" class="ltx_text" style="font-size:90%;">70.3</span></td>
<td id="S4.T4.2.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.3.3.6.1" class="ltx_text" style="font-size:90%;">61.0</span></td>
<td id="S4.T4.2.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.3.3.7.1" class="ltx_text" style="font-size:90%;">55.4</span></td>
</tr>
<tr id="S4.T4.2.4.4" class="ltx_tr">
<td id="S4.T4.2.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T4.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Oracle</span></td>
<td id="S4.T4.2.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.4.4.2.1" class="ltx_text" style="font-size:90%;">56.6</span></td>
<td id="S4.T4.2.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.4.4.3.1" class="ltx_text" style="font-size:90%;">47.1</span></td>
<td id="S4.T4.2.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">42.0</span></td>
<td id="S4.T4.2.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.4.4.5.1" class="ltx_text" style="font-size:90%;">60.3</span></td>
<td id="S4.T4.2.4.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.4.4.6.1" class="ltx_text" style="font-size:90%;">49.6</span></td>
<td id="S4.T4.2.4.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.4.4.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">44.0</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.5.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>PER(%) performance obtained with a random and an oracle microphone selection.</figcaption>
</figure>
<div id="S4.SS2.SSS4.p2" class="ltx_para">
<p id="S4.SS2.SSS4.p2.1" class="ltx_p">Results show that a proper microphone selection is crucial for improving the performance of a DSR system. The gap between the upper bound limit, based on an oracle channel selection and the lower bound limit, based on a random selection of the microphone, is particularly large. This confirms the importance of suitable microphone selection criteria.
A proper channel selection has a great potential even when compared with a microphone combination based on delay-and-sum beamforming. For instance, a PER of 44.0<math id="S4.SS2.SSS4.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS4.p2.1.m1.1a"><mo id="S4.SS2.SSS4.p2.1.m1.1.1" xref="S4.SS2.SSS4.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS4.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p2.1.m1.1c">\%</annotation></semantics></math> is obtained with an oracle channel selection against a PER of 50.6% achieved with the ceiling array.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and future work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper described the DIRHA-ENGLISH multi-microphone corpus and the first baseline results concerning the use of the phonetically-rich sentence data sets.
Overall, the experimental results show the expected trend of performance, quite well aligned to past works in this field.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In research studies on DSR, there are many advantages
in using phonetically-rich material with such a large number of microphone channels. For instance, there is the possibility of better focusing on the impact on performance of some front-end processing techniques for what concerns specific phone categories.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The corpus, also includes WSJ and conversational speech data
sets that can be object of public distribution<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>The distribution of WSJ data set is under discussion with LDC.</span></span></span>
and of a possible use in next challenges regarding DSR.
The latter data sets can be very helpful to investigate other key topics as, for instance, multi-microphone hypothesis combination based on confusion networks, multiple lattices, and rescoring.
Forthcoming works include the development of baselines and related recipes for MEMS microphones, for WSJ and conversational sequences, as well as for the UK English language.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Corpus release</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Some 1-minute sequences can be found at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://dirha.fbk.eu/DIRHA_English</span>.
The access to data that were used in this work, and to related documents, will be possible soon through a FBK server, with modalities that will be reported under http://dirha.fbk.eu.
In the future, other data sets will be made publicly available, together with corresponding documentation and recipes, and with instructions to allow comparison of systems and maximize scientific insights.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Dong Yu and Li Deng,

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Automatic Speech Recognition - A Deep Learning Approach</span>,

</span>
<span class="ltx_bibblock">Springer, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Wölfel and J. McDonough,

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Distant Speech Recognition</span>,

</span>
<span class="ltx_bibblock">Wiley, 2009.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
E. Hänsler and G. Schmidt,

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Speech and Audio Processing in Adverse Environments</span>,

</span>
<span class="ltx_bibblock">Springer, 2008.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Brandstein and D. Ward,

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Microphone arrays</span>,

</span>
<span class="ltx_bibblock">Springer, Berlin, 2000.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. Kellermann,

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Beamforming for Speech and Audio Signals</span>,

</span>
<span class="ltx_bibblock">in HandBook of Signal Processing in Acoustics, Springer, 2008.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Wolf and C. Nadeu,

</span>
<span class="ltx_bibblock">“Channel selection measures for multi-microphone speech
recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Speech Communication</span>, vol. 57, pp. 170–180, Feb. 2014.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Makino, T. Lee, and H. Sawada,

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Blind Speech Separation</span>,

</span>
<span class="ltx_bibblock">Springer, 2010.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. A. Naylor and N. D. Gaubitch,

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Speech Dereverberation.</span>,

</span>
<span class="ltx_bibblock">Springer, 2010.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P. Swietojanski, A. Ghoshal, and S. Renals,

</span>
<span class="ltx_bibblock">“Hybrid acoustic models for distant and multichannel large
vocabulary speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. of ASRU 2013</span>, 2013, pp. 285–290.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Liu, P. Zhang, and T. Hain,

</span>
<span class="ltx_bibblock">“Using neural network front-ends on far field multiple microphones
based speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proc. of ICASSP 2014</span>, 2014, pp. 5542–5546.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
F. Weninger, S. Watanabe, J. Le Roux, J.R. Hershey, Y. Tachioka, J. Geiger,
B. Schuller, and G. Rigoll,

</span>
<span class="ltx_bibblock">“The MERL/MELCO/TUM System for the REVERB Challenge Using Deep
Recurrent Neural Network Feature Enhancement,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE REVERB Workshop</span>, 2014.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Sakai M. Mimura and T. Kawahara,

</span>
<span class="ltx_bibblock">“Reverberant speech recognition combining deep neural networks and
deep autoencoders,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE REVERB Workshop</span>, 2014.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Schwarz, C. Huemmer, R. Maas, and W. Kellermann,

</span>
<span class="ltx_bibblock">“Spatial diffuseness features for dnn-based speech recognition in
noisy and reverberant environments,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proc. of ICASSP 2015</span>, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Ravanelli and M. Omologo,

</span>
<span class="ltx_bibblock">“Contaminated speech training methods for robust DNN-HMM distant
speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. of INTERSPEECH 2015</span>, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. Kinoshita, M. Delcroix, T. Yoshioka, T. Nakatani, E. Habets, R. Haeb-Umbach,
V. Leutnant, A. Sehr, W. Kellermann, R. Maas, S. Gannot, and B. Raj,

</span>
<span class="ltx_bibblock">“The reverb challenge: A common evaluation framework for
dereverberation and recognition of reverberant speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proc. of WASPAA 2013</span>, 2013, pp. 1–4.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Barker, E. Vincent, N. Ma, H. Christensen, and P. Green,

</span>
<span class="ltx_bibblock">“The PASCAL CHiME speech separation and recognition challenge,”

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Computer Speech and Language</span>, vol. 27, no. 3, pp. 621–633,
2013.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Barker, R. Marxer, E. Vincent, and S. Watanabe,

</span>
<span class="ltx_bibblock">“The third CHiME Speech Separation and Recognition Challenge:
Dataset, task and baselines,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proc. of ASRU 2015</span>, 2015.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. Cristoforetti, M. Ravanelli, M. Omologo, A. Sosi, A. Abad, M. Hagmueller,
and P. Maragos,

</span>
<span class="ltx_bibblock">“The DIRHA simulated corpus,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proc. of LREC 2014</span>, 2014, pp. 2629–2634.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and
K. Vesely,

</span>
<span class="ltx_bibblock">“The Kaldi Speech Recognition Toolkit,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proc. of ASRU 2011</span>, 2011.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Matassoni, M. Omologo, D. Giuliani, and P. Svaizer,

</span>
<span class="ltx_bibblock">“Hidden Markov model training with contaminated speech material for
distant-talking speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Computer Speech &amp; Language</span>, vol. 16, no. 2, pp. 205–223,
2002.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L. Couvreur, C. Couvreur, and C. Ris,

</span>
<span class="ltx_bibblock">“A corpus-based approach for robust ASR in reverberant
environments.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proc. of INTERSPEECH 2000</span>, 2000, pp. 397–400.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. Haderlein, E. Nöth, W. Herbordt, W. Kellermann, and H. Niemann,

</span>
<span class="ltx_bibblock">“Using Artificially Reverberated Training Data in Distant-Talking
ASR,”

</span>
<span class="ltx_bibblock">2005, vol. 3658 of <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pp.
226–233, Springer.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Brutti, M. Ravanelli, P. Svaizer, and M. Omologo,

</span>
<span class="ltx_bibblock">“A speech event detection/localization task for multiroom
environments,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proc. of HSCMA 2014</span>, 2014, pp. 157–161.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
E. Zwyssig, M. Ravanelli, P. Svaizer, and M. Omologo,

</span>
<span class="ltx_bibblock">“A multi-channel corpus for distant-speech interaction in presence
of known interferences,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proc. of ICASSP 2015</span>, 2015, pp. 4480–4485.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Matassoni, R. Astudillo, A. Katsamanis, and M. Ravanelli,

</span>
<span class="ltx_bibblock">“The DIRHA-GRID corpus: baseline and tools for multi-room distant
speech recognition using distributed microphones,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proc. of INTERSPEECH 2014</span>, 2014, pp. 1616–1617.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Cooke, J. Barker, S. Cunningham, and X. Shao,

</span>
<span class="ltx_bibblock">“An audio-visual corpus for speech perception and automatic speech
recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Journal of the Acoustical Society of America</span>, vol. 120, no. 5,
pp. 2421–2424, November 2006.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
F. Brugnara, D. Falavigna, and M. Omologo,

</span>
<span class="ltx_bibblock">“Automatic segmentation and labeling of speech based on hidden
markov models,”

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Speech Communication</span>, vol. 12, no. 4, pp. 357–370, 1993.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Douglas B. Paul and Janet M. Baker,

</span>
<span class="ltx_bibblock">“The design for the wall street journal-based csr corpus,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proc. of the Workshop on Speech and Natural Language</span>, 1992,
pp. 357–362.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Farina,

</span>
<span class="ltx_bibblock">“Simultaneous measurement of impulse response and distortion with a
swept-sine technique,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proc. of the 108th AES Convention</span>, 2000, pp. 18–22.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Ravanelli, A. Sosi, P. Svaizer, and M. Omologo,

</span>
<span class="ltx_bibblock">“Impulse response estimation for robust speech recognition in a
reverberant environment,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proc. of EUSIPCO 2012</span>, 2012, pp. 1668–1672.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and
N. L. Dahlgren,

</span>
<span class="ltx_bibblock">“DARPA TIMIT Acoustic Phonetic Continuous Speech Corpus CDROM,”
1993.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Ghoshal and D. Povey,

</span>
<span class="ltx_bibblock">“Sequence discriminative training of deep neural networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2013</span>, 2013.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M. Ravanelli and M. Omologo,

</span>
<span class="ltx_bibblock">“On the selection of the impulse responses for distant-speech
recognition based on contaminated speech training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proc. of INTERSPEECH 2014</span>, 2014, pp. 1028–1032.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K.-F. Lee and H.-W. Hon,

</span>
<span class="ltx_bibblock">“Speaker-independent phone recognition using hidden markov models,”

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Acoustics, Speech and Signal Processing</span>,
vol. 37, no. 11, pp. 1641–1648, Nov 1989.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1710.02559" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/land_of_honey_and_milk" rel="nofollow" aria-hidden="true" tabindex="-1"></a>
    <a href="/log/1710.02560" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1710.02560">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1710.02560" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1710.02561" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 04:18:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
