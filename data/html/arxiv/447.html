<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1710.02856] Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder</title><meta property="og:description" content="Soft biometric modalities have shown their utility in different applications including reducing the search space significantly. This leads to improved recognition performance, reduced computation time, and faster proce…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1710.02856">
<link rel="canonical" target="_blank" href="https://ar5iv.labs.arxiv.org/html/1710.02856">

<!--Generated on Sat Mar 16 04:09:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maneet Singh<sup id="id9.9.id1" class="ltx_sup">1</sup>, Shruti Nagpal<sup id="id10.10.id2" class="ltx_sup">1</sup>, Mayank Vatsa<sup id="id11.11.id3" class="ltx_sup"><span id="id11.11.id3.1" class="ltx_text ltx_font_italic">1,2</span></sup>, Richa Singh<sup id="id12.12.id4" class="ltx_sup"><span id="id12.12.id4.1" class="ltx_text ltx_font_italic">1,2</span></sup>, Afzel Noore<sup id="id13.13.id5" class="ltx_sup">2</sup>, and Angshul Majumdar<sup id="id14.14.id6" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="id15.15.id7" class="ltx_sup">1</sup>IIIT-Delhi, India, <sup id="id16.16.id8" class="ltx_sup">2</sup>West Virginia University
<br class="ltx_break">{<span id="id17.17.id9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">maneets, shrutin, mayank, rsingh, angshul}@iiitd.ac.in, afzel.noore@mail.wvu.edu
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id18.id1" class="ltx_p">Soft biometric modalities have shown their utility in different applications including reducing the search space significantly. This leads to improved recognition performance, reduced computation time, and faster processing of test samples. Some common soft biometric modalities are ethnicity, gender, age, hair color, iris color, presence of facial hair or moles, and markers. This research focuses on performing ethnicity and gender classification on iris images. We present a novel supervised autoencoder based approach, Deep Class-Encoder, which uses class labels to learn discriminative representation for the given sample by mapping the learned feature vector to its label. The proposed model is evaluated on two datasets each for ethnicity and gender classification. The results obtained using the proposed Deep Class-Encoder demonstrate its effectiveness in comparison to existing approaches and state-of-the-art methods.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Soft biometric traits are physical or behavioral characteristics that can be extracted from the human body which help differentiate individuals from one another. However, they often lack information that is permanent or sufficiently discriminative to uniquely identify an individual. These traits are valuable as they aid in recognition and enhance the performance of automated biometric systems by reducing the search space. This results in a significant decrease of testing time as well. Some of the most commonly used soft biometric modalities are gender, ethnicity, gait, age, height, hair color, eye color, and presence of moles or mustache on the face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents few sample soft biometric modalities that can be extracted from face and iris.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Iris texture has emerged as one of the most robust and widely used biometric modality for automated identity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Studies in literature have also established the presence of similar information in iris patterns to discriminate between ethnicities and gender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. That is, samples belonging to the same ethnicity, have some commonalities in the iris pattern, and the ones belonging to the same gender also share certain information.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1710.02856/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="192" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Sample soft biometric modalities that can be extracted from face and iris images. All images have been taken from the Internet. </figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In literature, gender from iris images was first predicted by Thomas <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> using an in-house dataset of left irises only. The iris images were segmented and normalized, onto which log-Gabor filter was applied to obtain the iris texture information. Features like mean, standard deviation, and local variation were computed for different neighborhoods of the texture pattern. Seven additional geometric features were also computed, followed by projection onto a random subspace ensemble of trees for classification. Lagree and Bowyer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> extended this work by computing texture features for different horizontal and vertical bands of the normalized iris image. A total of 630 features were computed for 14 regions in a given image. These features were used as input to the Sequential Minimum Optimization algorithm for classification. The same extracted features were also used to perform ethnicity classification of iris images. Tapia <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> used uniform LBP with concatenated histograms to perform gender classification on iris images on an in-house UND dataset. However, in this work, no mutual exclusivity of subjects was maintained between the training and testing partitions. Building upon this, in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, the disjoint train-test sets in terms of the subject were created, and gender classification was performed using the same binary texture pattern that is used for identity recognition. This work presented results on three datasets, the UND dataset used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, a novel ND-Gender From Iris (ND-GFI) dataset, and a subject disjoint validation set (UND_V).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Qiu <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> first proposed the idea of classifying iris images into ethnicity based categories. The authors combined images from three datasets and performed classification between Asian and Non-Asian by computing the Gabor energy ratio between two regions of the iris. Continuing this work, a method to represent iris images using iris textons was developed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The authors performed classification between Asian and Non-Asian on the CASIA BioSecure dataset. A filter bank of 40 Gabor filters was created and a 40-dimensional feature vector was computed. Each iris image was represented with a histogram of iris textons, which was used as a feature for SVMs to perform classification. The authors reported an accuracy of 91.02% to classify Asians v/s non-Asians. However, it is essential to note that these results were demonstrated on images collected from different datasets for different ethnicities, thus incorporating inter-database variations at the time of classification. Lagree and Bowyer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> proposed to identify the ethnicity using features obtained from several bands for 14 subregions of the iris. In 2012, Zarei and Mou <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> used artificial neural networks to predict the ethnicity. The authors computed a total of 882 features from various regions of the iris, which were combined to create a feature vector used as input to a neural network for classification.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This research presents ethnicity and gender classification from iris images, using the proposed <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">Deep Class-Encoder</span>. It is an autoencoder based supervised model which utilizes the robust feature extraction capabilities of deep learning and discriminative capabilities of supervised models. The research contributions of this work are two-fold:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">An autoencoder based supervised model, <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Deep Class-Encoder</span> is presented. The proposed model incorporates supervision by learning weights such that the hidden representation is mapped onto its class label. The proposed model is optimized using Alternating Direction Method of Multipliers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, instead of a gradient descent approach, thus reducing the training time substantially.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">For gender classification, results are reported on existing protocols of ND-Iris-0405 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and ND-Gender From Iris <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> datasets. Ethnicity classification is performed on ND-Iris-0405 and the proposed Multi-Ethnicity dataset. A new protocol is also defined on the Multi-Ethnicity dataset for ethnicity classification. The proposed model achieves state-of-the-art results for each task.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Proposed Deep Class-Encoder</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The success of unsupervised feature learning architectures has been well established over the past couple of years, especially with the advent of representation learning techniques such as Deep Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The performance of these architectures is not restricted to a particular domain, and spans across areas such as speech recognition, biometric recognition, computer vision, and natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Unsupervised architectures aim to harness the information provided from unlabeled data to learn representative features used for classification at a later stage.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.6" class="ltx_p">Autoencoders are deep learning architectures used to learn representations in an unsupervised manner. A traditional autoencoder consists of two sub-systems, encoder and decoder. For a given input <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\mathbf{X}</annotation></semantics></math>, the encoder is used to obtain the representation <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">𝐇</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝐇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\mathbf{H}</annotation></semantics></math> using the learned encoder weights <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{W_{e}}" display="inline"><semantics id="S2.p2.3.m3.1a"><msub id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">𝐖</mi><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">𝐞</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">𝐖</ci><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">𝐞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\mathbf{W_{e}}</annotation></semantics></math>. The decoder is used for reconstructing the input from the learned representation using the decoder weights, <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{W_{d}}" display="inline"><semantics id="S2.p2.4.m4.1a"><msub id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mi id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml">𝐖</mi><mi id="S2.p2.4.m4.1.1.3" xref="S2.p2.4.m4.1.1.3.cmml">𝐝</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1">subscript</csymbol><ci id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2">𝐖</ci><ci id="S2.p2.4.m4.1.1.3.cmml" xref="S2.p2.4.m4.1.1.3">𝐝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">\mathbf{W_{d}}</annotation></semantics></math>. The loss function of an autoencoder is optimized by minimizing the difference between the given input data, <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S2.p2.5.m5.1a"><mi id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><ci id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">\mathbf{X}</annotation></semantics></math>, and the reconstructed data, <math id="S2.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{\hat{X}}" display="inline"><semantics id="S2.p2.6.m6.1a"><mover accent="true" id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><mi id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml">𝐗</mi><mo id="S2.p2.6.m6.1.1.1" xref="S2.p2.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><ci id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1.1">^</ci><ci id="S2.p2.6.m6.1.1.2.cmml" xref="S2.p2.6.m6.1.1.2">𝐗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">\mathbf{\hat{X}}</annotation></semantics></math>. It is mathematically expressed as:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\min\left\|\mathbf{X}-\mathbf{\hat{X}}\right\|_{F}^{2}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">min</mi><mo id="S2.E1.m1.1.1a" xref="S2.E1.m1.1.1.cmml">⁡</mo><msubsup id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.2.cmml"><mo id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.2.cmml">𝐗</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S2.E1.m1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.3.2.cmml">𝐗</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mi id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml">F</mi><mn id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml">2</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><min id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"></min><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1">superscript</csymbol><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1">subscript</csymbol><apply id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"><minus id="S2.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1"></minus><ci id="S2.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2">𝐗</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.3"><ci id="S2.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.3.2">𝐗</ci></apply></apply></apply><ci id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3">𝐹</ci></apply><cn type="integer" id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\min\left\|\mathbf{X}-\mathbf{\hat{X}}\right\|_{F}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p2.12" class="ltx_p">where, the input consists of <math id="S2.p2.7.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p2.7.m1.1a"><mi id="S2.p2.7.m1.1.1" xref="S2.p2.7.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m1.1b"><ci id="S2.p2.7.m1.1.1.cmml" xref="S2.p2.7.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m1.1c">n</annotation></semantics></math> training samples, the hidden representation is computed as <math id="S2.p2.8.m2.1" class="ltx_Math" alttext="\mathbf{H}=\phi\mathbf{(W_{e}X)}" display="inline"><semantics id="S2.p2.8.m2.1a"><mrow id="S2.p2.8.m2.1.1" xref="S2.p2.8.m2.1.1.cmml"><mi id="S2.p2.8.m2.1.1.3" xref="S2.p2.8.m2.1.1.3.cmml">𝐇</mi><mo id="S2.p2.8.m2.1.1.2" xref="S2.p2.8.m2.1.1.2.cmml">=</mo><mrow id="S2.p2.8.m2.1.1.1" xref="S2.p2.8.m2.1.1.1.cmml"><mi id="S2.p2.8.m2.1.1.1.3" xref="S2.p2.8.m2.1.1.1.3.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S2.p2.8.m2.1.1.1.2" xref="S2.p2.8.m2.1.1.1.2.cmml">​</mo><mrow id="S2.p2.8.m2.1.1.1.1.1" xref="S2.p2.8.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p2.8.m2.1.1.1.1.1.2" xref="S2.p2.8.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.p2.8.m2.1.1.1.1.1.1" xref="S2.p2.8.m2.1.1.1.1.1.1.cmml"><msub id="S2.p2.8.m2.1.1.1.1.1.1.2" xref="S2.p2.8.m2.1.1.1.1.1.1.2.cmml"><mi id="S2.p2.8.m2.1.1.1.1.1.1.2.2" xref="S2.p2.8.m2.1.1.1.1.1.1.2.2.cmml">𝐖</mi><mi id="S2.p2.8.m2.1.1.1.1.1.1.2.3" xref="S2.p2.8.m2.1.1.1.1.1.1.2.3.cmml">𝐞</mi></msub><mo lspace="0em" rspace="0em" id="S2.p2.8.m2.1.1.1.1.1.1.1" xref="S2.p2.8.m2.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.p2.8.m2.1.1.1.1.1.1.3" xref="S2.p2.8.m2.1.1.1.1.1.1.3.cmml">𝐗</mi></mrow><mo stretchy="false" id="S2.p2.8.m2.1.1.1.1.1.3" xref="S2.p2.8.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.8.m2.1b"><apply id="S2.p2.8.m2.1.1.cmml" xref="S2.p2.8.m2.1.1"><eq id="S2.p2.8.m2.1.1.2.cmml" xref="S2.p2.8.m2.1.1.2"></eq><ci id="S2.p2.8.m2.1.1.3.cmml" xref="S2.p2.8.m2.1.1.3">𝐇</ci><apply id="S2.p2.8.m2.1.1.1.cmml" xref="S2.p2.8.m2.1.1.1"><times id="S2.p2.8.m2.1.1.1.2.cmml" xref="S2.p2.8.m2.1.1.1.2"></times><ci id="S2.p2.8.m2.1.1.1.3.cmml" xref="S2.p2.8.m2.1.1.1.3">italic-ϕ</ci><apply id="S2.p2.8.m2.1.1.1.1.1.1.cmml" xref="S2.p2.8.m2.1.1.1.1.1"><times id="S2.p2.8.m2.1.1.1.1.1.1.1.cmml" xref="S2.p2.8.m2.1.1.1.1.1.1.1"></times><apply id="S2.p2.8.m2.1.1.1.1.1.1.2.cmml" xref="S2.p2.8.m2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.p2.8.m2.1.1.1.1.1.1.2.1.cmml" xref="S2.p2.8.m2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.p2.8.m2.1.1.1.1.1.1.2.2.cmml" xref="S2.p2.8.m2.1.1.1.1.1.1.2.2">𝐖</ci><ci id="S2.p2.8.m2.1.1.1.1.1.1.2.3.cmml" xref="S2.p2.8.m2.1.1.1.1.1.1.2.3">𝐞</ci></apply><ci id="S2.p2.8.m2.1.1.1.1.1.1.3.cmml" xref="S2.p2.8.m2.1.1.1.1.1.1.3">𝐗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m2.1c">\mathbf{H}=\phi\mathbf{(W_{e}X)}</annotation></semantics></math>, and the reconstructed data <math id="S2.p2.9.m3.1" class="ltx_Math" alttext="\mathbf{\hat{X}}=\mathbf{W_{d}H}" display="inline"><semantics id="S2.p2.9.m3.1a"><mrow id="S2.p2.9.m3.1.1" xref="S2.p2.9.m3.1.1.cmml"><mover accent="true" id="S2.p2.9.m3.1.1.2" xref="S2.p2.9.m3.1.1.2.cmml"><mi id="S2.p2.9.m3.1.1.2.2" xref="S2.p2.9.m3.1.1.2.2.cmml">𝐗</mi><mo id="S2.p2.9.m3.1.1.2.1" xref="S2.p2.9.m3.1.1.2.1.cmml">^</mo></mover><mo id="S2.p2.9.m3.1.1.1" xref="S2.p2.9.m3.1.1.1.cmml">=</mo><mrow id="S2.p2.9.m3.1.1.3" xref="S2.p2.9.m3.1.1.3.cmml"><msub id="S2.p2.9.m3.1.1.3.2" xref="S2.p2.9.m3.1.1.3.2.cmml"><mi id="S2.p2.9.m3.1.1.3.2.2" xref="S2.p2.9.m3.1.1.3.2.2.cmml">𝐖</mi><mi id="S2.p2.9.m3.1.1.3.2.3" xref="S2.p2.9.m3.1.1.3.2.3.cmml">𝐝</mi></msub><mo lspace="0em" rspace="0em" id="S2.p2.9.m3.1.1.3.1" xref="S2.p2.9.m3.1.1.3.1.cmml">​</mo><mi id="S2.p2.9.m3.1.1.3.3" xref="S2.p2.9.m3.1.1.3.3.cmml">𝐇</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.9.m3.1b"><apply id="S2.p2.9.m3.1.1.cmml" xref="S2.p2.9.m3.1.1"><eq id="S2.p2.9.m3.1.1.1.cmml" xref="S2.p2.9.m3.1.1.1"></eq><apply id="S2.p2.9.m3.1.1.2.cmml" xref="S2.p2.9.m3.1.1.2"><ci id="S2.p2.9.m3.1.1.2.1.cmml" xref="S2.p2.9.m3.1.1.2.1">^</ci><ci id="S2.p2.9.m3.1.1.2.2.cmml" xref="S2.p2.9.m3.1.1.2.2">𝐗</ci></apply><apply id="S2.p2.9.m3.1.1.3.cmml" xref="S2.p2.9.m3.1.1.3"><times id="S2.p2.9.m3.1.1.3.1.cmml" xref="S2.p2.9.m3.1.1.3.1"></times><apply id="S2.p2.9.m3.1.1.3.2.cmml" xref="S2.p2.9.m3.1.1.3.2"><csymbol cd="ambiguous" id="S2.p2.9.m3.1.1.3.2.1.cmml" xref="S2.p2.9.m3.1.1.3.2">subscript</csymbol><ci id="S2.p2.9.m3.1.1.3.2.2.cmml" xref="S2.p2.9.m3.1.1.3.2.2">𝐖</ci><ci id="S2.p2.9.m3.1.1.3.2.3.cmml" xref="S2.p2.9.m3.1.1.3.2.3">𝐝</ci></apply><ci id="S2.p2.9.m3.1.1.3.3.cmml" xref="S2.p2.9.m3.1.1.3.3">𝐇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m3.1c">\mathbf{\hat{X}}=\mathbf{W_{d}H}</annotation></semantics></math>. <math id="S2.p2.10.m4.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S2.p2.10.m4.1a"><mi id="S2.p2.10.m4.1.1" xref="S2.p2.10.m4.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.10.m4.1b"><ci id="S2.p2.10.m4.1.1.cmml" xref="S2.p2.10.m4.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m4.1c">\phi</annotation></semantics></math> is an activation function which can be any non-linear function such as <math id="S2.p2.11.m5.1" class="ltx_Math" alttext="sigmoid" display="inline"><semantics id="S2.p2.11.m5.1a"><mrow id="S2.p2.11.m5.1.1" xref="S2.p2.11.m5.1.1.cmml"><mi id="S2.p2.11.m5.1.1.2" xref="S2.p2.11.m5.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m5.1.1.1" xref="S2.p2.11.m5.1.1.1.cmml">​</mo><mi id="S2.p2.11.m5.1.1.3" xref="S2.p2.11.m5.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m5.1.1.1a" xref="S2.p2.11.m5.1.1.1.cmml">​</mo><mi id="S2.p2.11.m5.1.1.4" xref="S2.p2.11.m5.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m5.1.1.1b" xref="S2.p2.11.m5.1.1.1.cmml">​</mo><mi id="S2.p2.11.m5.1.1.5" xref="S2.p2.11.m5.1.1.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m5.1.1.1c" xref="S2.p2.11.m5.1.1.1.cmml">​</mo><mi id="S2.p2.11.m5.1.1.6" xref="S2.p2.11.m5.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m5.1.1.1d" xref="S2.p2.11.m5.1.1.1.cmml">​</mo><mi id="S2.p2.11.m5.1.1.7" xref="S2.p2.11.m5.1.1.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m5.1.1.1e" xref="S2.p2.11.m5.1.1.1.cmml">​</mo><mi id="S2.p2.11.m5.1.1.8" xref="S2.p2.11.m5.1.1.8.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.11.m5.1b"><apply id="S2.p2.11.m5.1.1.cmml" xref="S2.p2.11.m5.1.1"><times id="S2.p2.11.m5.1.1.1.cmml" xref="S2.p2.11.m5.1.1.1"></times><ci id="S2.p2.11.m5.1.1.2.cmml" xref="S2.p2.11.m5.1.1.2">𝑠</ci><ci id="S2.p2.11.m5.1.1.3.cmml" xref="S2.p2.11.m5.1.1.3">𝑖</ci><ci id="S2.p2.11.m5.1.1.4.cmml" xref="S2.p2.11.m5.1.1.4">𝑔</ci><ci id="S2.p2.11.m5.1.1.5.cmml" xref="S2.p2.11.m5.1.1.5">𝑚</ci><ci id="S2.p2.11.m5.1.1.6.cmml" xref="S2.p2.11.m5.1.1.6">𝑜</ci><ci id="S2.p2.11.m5.1.1.7.cmml" xref="S2.p2.11.m5.1.1.7">𝑖</ci><ci id="S2.p2.11.m5.1.1.8.cmml" xref="S2.p2.11.m5.1.1.8">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.11.m5.1c">sigmoid</annotation></semantics></math>, or <math id="S2.p2.12.m6.1" class="ltx_Math" alttext="tanh" display="inline"><semantics id="S2.p2.12.m6.1a"><mrow id="S2.p2.12.m6.1.1" xref="S2.p2.12.m6.1.1.cmml"><mi id="S2.p2.12.m6.1.1.2" xref="S2.p2.12.m6.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p2.12.m6.1.1.1" xref="S2.p2.12.m6.1.1.1.cmml">​</mo><mi id="S2.p2.12.m6.1.1.3" xref="S2.p2.12.m6.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p2.12.m6.1.1.1a" xref="S2.p2.12.m6.1.1.1.cmml">​</mo><mi id="S2.p2.12.m6.1.1.4" xref="S2.p2.12.m6.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p2.12.m6.1.1.1b" xref="S2.p2.12.m6.1.1.1.cmml">​</mo><mi id="S2.p2.12.m6.1.1.5" xref="S2.p2.12.m6.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.12.m6.1b"><apply id="S2.p2.12.m6.1.1.cmml" xref="S2.p2.12.m6.1.1"><times id="S2.p2.12.m6.1.1.1.cmml" xref="S2.p2.12.m6.1.1.1"></times><ci id="S2.p2.12.m6.1.1.2.cmml" xref="S2.p2.12.m6.1.1.2">𝑡</ci><ci id="S2.p2.12.m6.1.1.3.cmml" xref="S2.p2.12.m6.1.1.3">𝑎</ci><ci id="S2.p2.12.m6.1.1.4.cmml" xref="S2.p2.12.m6.1.1.4">𝑛</ci><ci id="S2.p2.12.m6.1.1.5.cmml" xref="S2.p2.12.m6.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.12.m6.1c">tanh</annotation></semantics></math> or linear activation corresponding to unit activation. Upon expanding Equation <a href="#S2.E1" title="In 2 Proposed Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, with unit activation function, the loss function can be expressed as:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.4" class="ltx_Math" alttext="\mathbf{\operatorname*{arg\,min}_{\textit{$\mathbf{W_{d},W_{e}}$}}\left\|\mathbf{X-W_{d}W_{e}X}\right\|_{F}^{2}}\vspace{-5pt}" display="block"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><munder id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.2.cmml"><mrow id="S2.E2.m1.4.4.2.2" xref="S2.E2.m1.4.4.2.2.cmml"><mi id="S2.E2.m1.4.4.2.2.2" xref="S2.E2.m1.4.4.2.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S2.E2.m1.4.4.2.2.1" xref="S2.E2.m1.4.4.2.2.1.cmml">​</mo><mi id="S2.E2.m1.4.4.2.2.3" xref="S2.E2.m1.4.4.2.2.3.cmml">min</mi></mrow><mrow id="S2.E2.m1.3.3.3.3.3" xref="S2.E2.m1.3.3.3.4.cmml"><msub id="S2.E2.m1.2.2.2.2.2.1" xref="S2.E2.m1.2.2.2.2.2.1.cmml"><mi id="S2.E2.m1.2.2.2.2.2.1.2" xref="S2.E2.m1.2.2.2.2.2.1.2.cmml">𝐖</mi><mi mathsize="98%" id="S2.E2.m1.2.2.2.2.2.1.3" xref="S2.E2.m1.2.2.2.2.2.1.3.cmml">𝐝</mi></msub><mo id="S2.E2.m1.3.3.3.3.3.3" xref="S2.E2.m1.3.3.3.4.cmml">,</mo><msub id="S2.E2.m1.3.3.3.3.3.2" xref="S2.E2.m1.3.3.3.3.3.2.cmml"><mi id="S2.E2.m1.3.3.3.3.3.2.2" xref="S2.E2.m1.3.3.3.3.3.2.2.cmml">𝐖</mi><mi mathsize="98%" id="S2.E2.m1.3.3.3.3.3.2.3" xref="S2.E2.m1.3.3.3.3.3.2.3.cmml">𝐞</mi></msub></mrow></munder><mo id="S2.E2.m1.4.4a" xref="S2.E2.m1.4.4.cmml">⁡</mo><msubsup id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.2.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml">𝐗</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.3.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.3.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2.2.cmml">𝐖</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.3.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2.3.cmml">𝐝</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.3.1.cmml">​</mo><msub id="S2.E2.m1.4.4.1.1.1.1.1.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.3.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.3.3.2.cmml">𝐖</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.3.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.3.3.3.cmml">𝐞</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.3.1a" xref="S2.E2.m1.4.4.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.3.4" xref="S2.E2.m1.4.4.1.1.1.1.1.3.4.cmml">𝐗</mi></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.2.1.cmml">‖</mo></mrow><mi id="S2.E2.m1.4.4.1.1.3" xref="S2.E2.m1.4.4.1.1.3.cmml">𝐅</mi><mn id="S2.E2.m1.4.4.1.3" xref="S2.E2.m1.4.4.1.3.cmml">𝟐</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><apply id="S2.E2.m1.4.4.2.cmml" xref="S2.E2.m1.4.4.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.2.1.cmml" xref="S2.E2.m1.4.4.2">subscript</csymbol><apply id="S2.E2.m1.4.4.2.2.cmml" xref="S2.E2.m1.4.4.2.2"><times id="S2.E2.m1.4.4.2.2.1.cmml" xref="S2.E2.m1.4.4.2.2.1"></times><ci id="S2.E2.m1.4.4.2.2.2.cmml" xref="S2.E2.m1.4.4.2.2.2">arg</ci><ci id="S2.E2.m1.4.4.2.2.3.cmml" xref="S2.E2.m1.4.4.2.2.3">min</ci></apply><list id="S2.E2.m1.3.3.3.4.cmml" xref="S2.E2.m1.3.3.3.3.3"><apply id="S2.E2.m1.2.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.2.1.1.cmml" xref="S2.E2.m1.2.2.2.2.2.1">subscript</csymbol><ci id="S2.E2.m1.2.2.2.2.2.1.2.cmml" xref="S2.E2.m1.2.2.2.2.2.1.2">𝐖</ci><ci id="S2.E2.m1.2.2.2.2.2.1.3.cmml" xref="S2.E2.m1.2.2.2.2.2.1.3">𝐝</ci></apply><apply id="S2.E2.m1.3.3.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.3.3.2.1.cmml" xref="S2.E2.m1.3.3.3.3.3.2">subscript</csymbol><ci id="S2.E2.m1.3.3.3.3.3.2.2.cmml" xref="S2.E2.m1.3.3.3.3.3.2.2">𝐖</ci><ci id="S2.E2.m1.3.3.3.3.3.2.3.cmml" xref="S2.E2.m1.3.3.3.3.3.2.3">𝐞</ci></apply></list></apply><apply id="S2.E2.m1.4.4.1.cmml" xref="S2.E2.m1.4.4.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.2.cmml" xref="S2.E2.m1.4.4.1">superscript</csymbol><apply id="S2.E2.m1.4.4.1.1.cmml" xref="S2.E2.m1.4.4.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1">subscript</csymbol><apply id="S2.E2.m1.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2">norm</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1"><minus id="S2.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1"></minus><ci id="S2.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2">𝐗</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3"><times id="S2.E2.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.1"></times><apply id="S2.E2.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2.2">𝐖</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2.3">𝐝</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.3.2">𝐖</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.3.3">𝐞</ci></apply><ci id="S2.E2.m1.4.4.1.1.1.1.1.3.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.4">𝐗</ci></apply></apply></apply><ci id="S2.E2.m1.4.4.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.3">𝐅</ci></apply><cn type="integer" id="S2.E2.m1.4.4.1.3.cmml" xref="S2.E2.m1.4.4.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\mathbf{\operatorname*{arg\,min}_{\textit{$\mathbf{W_{d},W_{e}}$}}\left\|\mathbf{X-W_{d}W_{e}X}\right\|_{F}^{2}}\vspace{-5pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">A traditional biometrics pipeline consists of feature extraction, followed by classification. The feature extraction module extracts meaningful representations from the input, while the classifier learns a boundary in order to distinguish between the representations. Autoencoders have been used as unsupervised models for feature extraction in several applications. In order to learn meaningful representations for specific tasks, researchers have proposed incorporating supervision in the autoencoder model. Most of the existing supervised models do not explicitly encode the class label, but encode only the class information (same/different) for feature learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Encoding class information often leads to reducing the intra-class variability.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.3" class="ltx_p">In this research, we incorporate supervision in the feature extraction module in an attempt to introduce discriminability at the time of feature extraction itself. It is our hypothesis that feature extraction from such a model would facilitate better classification. We propose a model which utilizes the robust feature extraction capability of autoencoders, while learning discriminative features to enhance classification. The proposed architecture, termed as <span id="S2.p4.3.1" class="ltx_text ltx_font_italic">Class-Encoder</span> is built such that the class label of a given training sample is encoded in the learned feature representation, thereby making the model supervised in nature. This is done by incorporating an additional mapping matrix <math id="S2.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S2.p4.1.m1.1a"><mi id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><ci id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">\mathbf{M}</annotation></semantics></math>, which maps the hidden representation <math id="S2.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{W_{e}X}" display="inline"><semantics id="S2.p4.2.m2.1a"><mrow id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml"><msub id="S2.p4.2.m2.1.1.2" xref="S2.p4.2.m2.1.1.2.cmml"><mi id="S2.p4.2.m2.1.1.2.2" xref="S2.p4.2.m2.1.1.2.2.cmml">𝐖</mi><mi id="S2.p4.2.m2.1.1.2.3" xref="S2.p4.2.m2.1.1.2.3.cmml">𝐞</mi></msub><mo lspace="0em" rspace="0em" id="S2.p4.2.m2.1.1.1" xref="S2.p4.2.m2.1.1.1.cmml">​</mo><mi id="S2.p4.2.m2.1.1.3" xref="S2.p4.2.m2.1.1.3.cmml">𝐗</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><apply id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1"><times id="S2.p4.2.m2.1.1.1.cmml" xref="S2.p4.2.m2.1.1.1"></times><apply id="S2.p4.2.m2.1.1.2.cmml" xref="S2.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.p4.2.m2.1.1.2.1.cmml" xref="S2.p4.2.m2.1.1.2">subscript</csymbol><ci id="S2.p4.2.m2.1.1.2.2.cmml" xref="S2.p4.2.m2.1.1.2.2">𝐖</ci><ci id="S2.p4.2.m2.1.1.2.3.cmml" xref="S2.p4.2.m2.1.1.2.3">𝐞</ci></apply><ci id="S2.p4.2.m2.1.1.3.cmml" xref="S2.p4.2.m2.1.1.3">𝐗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">\mathbf{W_{e}X}</annotation></semantics></math> onto it’s corresponding class label <math id="S2.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{C}" display="inline"><semantics id="S2.p4.3.m3.1a"><mi id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml">𝐂</mi><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><ci id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1">𝐂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">\mathbf{C}</annotation></semantics></math>. Mathematically, it can be expressed as:</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_Math" alttext="\mathbf{\min_{\mathbf{M}}\left\|C-MW_{e}X\right\|_{F}^{2}}\vspace{-5pt}" display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><munder id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.2.2" xref="S2.E3.m1.1.1.2.2.cmml">min</mi><mi id="S2.E3.m1.1.1.2.3" xref="S2.E3.m1.1.1.2.3.cmml">𝐌</mi></munder><mo id="S2.E3.m1.1.1a" xref="S2.E3.m1.1.1.cmml">⁡</mo><msubsup id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.2.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.2.cmml">𝐂</mi><mo id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.3.cmml"><msub id="S2.E3.m1.1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.3.2.2" xref="S2.E3.m1.1.1.1.1.1.1.1.3.2.2.cmml">𝐌𝐖</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.3.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.3.2.3.cmml">𝐞</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.1.1.3.1" xref="S2.E3.m1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.E3.m1.1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.3.3.cmml">𝐗</mi></mrow></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mi id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml">𝐅</mi><mn id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml">𝟐</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><apply id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.2">subscript</csymbol><min id="S2.E3.m1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.2.2"></min><ci id="S2.E3.m1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.2.3">𝐌</ci></apply><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1">subscript</csymbol><apply id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><minus id="S2.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1"></minus><ci id="S2.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2">𝐂</ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3"><times id="S2.E3.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3.1"></times><apply id="S2.E3.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3.2.2">𝐌𝐖</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3.2.3">𝐞</ci></apply><ci id="S2.E3.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3.3">𝐗</ci></apply></apply></apply><ci id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3">𝐅</ci></apply><cn type="integer" id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\mathbf{\min_{\mathbf{M}}\left\|C-MW_{e}X\right\|_{F}^{2}}\vspace{-5pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.p4.11" class="ltx_p">where, <math id="S2.p4.4.m1.1" class="ltx_Math" alttext="\mathbf{C}\in R^{l\times n}" display="inline"><semantics id="S2.p4.4.m1.1a"><mrow id="S2.p4.4.m1.1.1" xref="S2.p4.4.m1.1.1.cmml"><mi id="S2.p4.4.m1.1.1.2" xref="S2.p4.4.m1.1.1.2.cmml">𝐂</mi><mo id="S2.p4.4.m1.1.1.1" xref="S2.p4.4.m1.1.1.1.cmml">∈</mo><msup id="S2.p4.4.m1.1.1.3" xref="S2.p4.4.m1.1.1.3.cmml"><mi id="S2.p4.4.m1.1.1.3.2" xref="S2.p4.4.m1.1.1.3.2.cmml">R</mi><mrow id="S2.p4.4.m1.1.1.3.3" xref="S2.p4.4.m1.1.1.3.3.cmml"><mi id="S2.p4.4.m1.1.1.3.3.2" xref="S2.p4.4.m1.1.1.3.3.2.cmml">l</mi><mo lspace="0.222em" rspace="0.222em" id="S2.p4.4.m1.1.1.3.3.1" xref="S2.p4.4.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.p4.4.m1.1.1.3.3.3" xref="S2.p4.4.m1.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.4.m1.1b"><apply id="S2.p4.4.m1.1.1.cmml" xref="S2.p4.4.m1.1.1"><in id="S2.p4.4.m1.1.1.1.cmml" xref="S2.p4.4.m1.1.1.1"></in><ci id="S2.p4.4.m1.1.1.2.cmml" xref="S2.p4.4.m1.1.1.2">𝐂</ci><apply id="S2.p4.4.m1.1.1.3.cmml" xref="S2.p4.4.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p4.4.m1.1.1.3.1.cmml" xref="S2.p4.4.m1.1.1.3">superscript</csymbol><ci id="S2.p4.4.m1.1.1.3.2.cmml" xref="S2.p4.4.m1.1.1.3.2">𝑅</ci><apply id="S2.p4.4.m1.1.1.3.3.cmml" xref="S2.p4.4.m1.1.1.3.3"><times id="S2.p4.4.m1.1.1.3.3.1.cmml" xref="S2.p4.4.m1.1.1.3.3.1"></times><ci id="S2.p4.4.m1.1.1.3.3.2.cmml" xref="S2.p4.4.m1.1.1.3.3.2">𝑙</ci><ci id="S2.p4.4.m1.1.1.3.3.3.cmml" xref="S2.p4.4.m1.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m1.1c">\mathbf{C}\in R^{l\times n}</annotation></semantics></math> (<math id="S2.p4.5.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.p4.5.m2.1a"><mi id="S2.p4.5.m2.1.1" xref="S2.p4.5.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.p4.5.m2.1b"><ci id="S2.p4.5.m2.1.1.cmml" xref="S2.p4.5.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.5.m2.1c">l</annotation></semantics></math> being the number of distinct classes) is a binary vector with its <math id="S2.p4.6.m3.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S2.p4.6.m3.1a"><msup id="S2.p4.6.m3.1.1" xref="S2.p4.6.m3.1.1.cmml"><mi id="S2.p4.6.m3.1.1.2" xref="S2.p4.6.m3.1.1.2.cmml">i</mi><mrow id="S2.p4.6.m3.1.1.3" xref="S2.p4.6.m3.1.1.3.cmml"><mi id="S2.p4.6.m3.1.1.3.2" xref="S2.p4.6.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p4.6.m3.1.1.3.1" xref="S2.p4.6.m3.1.1.3.1.cmml">​</mo><mi id="S2.p4.6.m3.1.1.3.3" xref="S2.p4.6.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.p4.6.m3.1b"><apply id="S2.p4.6.m3.1.1.cmml" xref="S2.p4.6.m3.1.1"><csymbol cd="ambiguous" id="S2.p4.6.m3.1.1.1.cmml" xref="S2.p4.6.m3.1.1">superscript</csymbol><ci id="S2.p4.6.m3.1.1.2.cmml" xref="S2.p4.6.m3.1.1.2">𝑖</ci><apply id="S2.p4.6.m3.1.1.3.cmml" xref="S2.p4.6.m3.1.1.3"><times id="S2.p4.6.m3.1.1.3.1.cmml" xref="S2.p4.6.m3.1.1.3.1"></times><ci id="S2.p4.6.m3.1.1.3.2.cmml" xref="S2.p4.6.m3.1.1.3.2">𝑡</ci><ci id="S2.p4.6.m3.1.1.3.3.cmml" xref="S2.p4.6.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.6.m3.1c">i^{th}</annotation></semantics></math> element set to 1 if the data sample belongs to class <math id="S2.p4.7.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p4.7.m4.1a"><mi id="S2.p4.7.m4.1.1" xref="S2.p4.7.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p4.7.m4.1b"><ci id="S2.p4.7.m4.1.1.cmml" xref="S2.p4.7.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.7.m4.1c">i</annotation></semantics></math>, rest being zero. Matrix <math id="S2.p4.8.m5.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S2.p4.8.m5.1a"><mi id="S2.p4.8.m5.1.1" xref="S2.p4.8.m5.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S2.p4.8.m5.1b"><ci id="S2.p4.8.m5.1.1.cmml" xref="S2.p4.8.m5.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.8.m5.1c">\mathbf{M}</annotation></semantics></math> captures the linear mapping between the feature vector (<math id="S2.p4.9.m6.1" class="ltx_Math" alttext="\mathbf{W_{e}X}" display="inline"><semantics id="S2.p4.9.m6.1a"><mrow id="S2.p4.9.m6.1.1" xref="S2.p4.9.m6.1.1.cmml"><msub id="S2.p4.9.m6.1.1.2" xref="S2.p4.9.m6.1.1.2.cmml"><mi id="S2.p4.9.m6.1.1.2.2" xref="S2.p4.9.m6.1.1.2.2.cmml">𝐖</mi><mi id="S2.p4.9.m6.1.1.2.3" xref="S2.p4.9.m6.1.1.2.3.cmml">𝐞</mi></msub><mo lspace="0em" rspace="0em" id="S2.p4.9.m6.1.1.1" xref="S2.p4.9.m6.1.1.1.cmml">​</mo><mi id="S2.p4.9.m6.1.1.3" xref="S2.p4.9.m6.1.1.3.cmml">𝐗</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.9.m6.1b"><apply id="S2.p4.9.m6.1.1.cmml" xref="S2.p4.9.m6.1.1"><times id="S2.p4.9.m6.1.1.1.cmml" xref="S2.p4.9.m6.1.1.1"></times><apply id="S2.p4.9.m6.1.1.2.cmml" xref="S2.p4.9.m6.1.1.2"><csymbol cd="ambiguous" id="S2.p4.9.m6.1.1.2.1.cmml" xref="S2.p4.9.m6.1.1.2">subscript</csymbol><ci id="S2.p4.9.m6.1.1.2.2.cmml" xref="S2.p4.9.m6.1.1.2.2">𝐖</ci><ci id="S2.p4.9.m6.1.1.2.3.cmml" xref="S2.p4.9.m6.1.1.2.3">𝐞</ci></apply><ci id="S2.p4.9.m6.1.1.3.cmml" xref="S2.p4.9.m6.1.1.3">𝐗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.9.m6.1c">\mathbf{W_{e}X}</annotation></semantics></math>) and it’s corresponding class label. Thus, under a fixed <math id="S2.p4.10.m7.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S2.p4.10.m7.1a"><mi id="S2.p4.10.m7.1.1" xref="S2.p4.10.m7.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S2.p4.10.m7.1b"><ci id="S2.p4.10.m7.1.1.cmml" xref="S2.p4.10.m7.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.10.m7.1c">\mathbf{M}</annotation></semantics></math> and <math id="S2.p4.11.m8.1" class="ltx_Math" alttext="\mathbf{W_{e}}" display="inline"><semantics id="S2.p4.11.m8.1a"><msub id="S2.p4.11.m8.1.1" xref="S2.p4.11.m8.1.1.cmml"><mi id="S2.p4.11.m8.1.1.2" xref="S2.p4.11.m8.1.1.2.cmml">𝐖</mi><mi id="S2.p4.11.m8.1.1.3" xref="S2.p4.11.m8.1.1.3.cmml">𝐞</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p4.11.m8.1b"><apply id="S2.p4.11.m8.1.1.cmml" xref="S2.p4.11.m8.1.1"><csymbol cd="ambiguous" id="S2.p4.11.m8.1.1.1.cmml" xref="S2.p4.11.m8.1.1">subscript</csymbol><ci id="S2.p4.11.m8.1.1.2.cmml" xref="S2.p4.11.m8.1.1.2">𝐖</ci><ci id="S2.p4.11.m8.1.1.3.cmml" xref="S2.p4.11.m8.1.1.3">𝐞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.11.m8.1c">\mathbf{W_{e}}</annotation></semantics></math>, samples belonging to the same class should map to the same class label. Extending Equation <a href="#S2.E2" title="In 2 Proposed Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to incorporate the above term, the loss function of the proposed Class-Encoder can thus be given as:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.6" class="ltx_Math" alttext="\mathbf{\min_{\textit{$\mathbf{M,W_{d},W_{e}}$}}\left\|X-W_{d}W_{e}X\right\|_{F}^{2}+\lambda\left\|C-MW_{e}X\right\|_{F}^{2}}\vspace{-5pt}" display="block"><semantics id="S2.E4.m1.6a"><mrow id="S2.E4.m1.6.6" xref="S2.E4.m1.6.6.cmml"><mrow id="S2.E4.m1.5.5.1" xref="S2.E4.m1.5.5.1.cmml"><munder id="S2.E4.m1.5.5.1.2" xref="S2.E4.m1.5.5.1.2.cmml"><mi id="S2.E4.m1.5.5.1.2.2" xref="S2.E4.m1.5.5.1.2.2.cmml">min</mi><mrow id="S2.E4.m1.4.4.4.4.4" xref="S2.E4.m1.4.4.4.5.cmml"><mi id="S2.E4.m1.2.2.2.2.2" xref="S2.E4.m1.2.2.2.2.2.cmml">𝐌</mi><mo id="S2.E4.m1.4.4.4.4.4.3" xref="S2.E4.m1.4.4.4.5.cmml">,</mo><msub id="S2.E4.m1.3.3.3.3.3.1" xref="S2.E4.m1.3.3.3.3.3.1.cmml"><mi id="S2.E4.m1.3.3.3.3.3.1.2" xref="S2.E4.m1.3.3.3.3.3.1.2.cmml">𝐖</mi><mi mathsize="98%" id="S2.E4.m1.3.3.3.3.3.1.3" xref="S2.E4.m1.3.3.3.3.3.1.3.cmml">𝐝</mi></msub><mo id="S2.E4.m1.4.4.4.4.4.4" xref="S2.E4.m1.4.4.4.5.cmml">,</mo><msub id="S2.E4.m1.4.4.4.4.4.2" xref="S2.E4.m1.4.4.4.4.4.2.cmml"><mi id="S2.E4.m1.4.4.4.4.4.2.2" xref="S2.E4.m1.4.4.4.4.4.2.2.cmml">𝐖</mi><mi mathsize="98%" id="S2.E4.m1.4.4.4.4.4.2.3" xref="S2.E4.m1.4.4.4.4.4.2.3.cmml">𝐞</mi></msub></mrow></munder><mo id="S2.E4.m1.5.5.1a" xref="S2.E4.m1.5.5.1.cmml">⁡</mo><msubsup id="S2.E4.m1.5.5.1.1" xref="S2.E4.m1.5.5.1.1.cmml"><mrow id="S2.E4.m1.5.5.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.2.cmml"><mo id="S2.E4.m1.5.5.1.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E4.m1.5.5.1.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.2.cmml">𝐗</mi><mo id="S2.E4.m1.5.5.1.1.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.E4.m1.5.5.1.1.1.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.cmml"><msub id="S2.E4.m1.5.5.1.1.1.1.1.1.3.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.2.cmml">𝐖</mi><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.3.cmml">𝐝</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.1.1.1.1.1.3.1" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml">​</mo><msub id="S2.E4.m1.5.5.1.1.1.1.1.1.3.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.2.cmml">𝐖</mi><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.3" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.3.cmml">𝐞</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.1.1.1.1.1.3.1a" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.E4.m1.5.5.1.1.1.1.1.1.3.4" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.4.cmml">𝐗</mi></mrow></mrow><mo id="S2.E4.m1.5.5.1.1.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.2.1.cmml">‖</mo></mrow><mi id="S2.E4.m1.5.5.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.3.cmml">𝐅</mi><mn id="S2.E4.m1.5.5.1.1.3" xref="S2.E4.m1.5.5.1.1.3.cmml">𝟐</mn></msubsup></mrow><mo id="S2.E4.m1.6.6.3" xref="S2.E4.m1.6.6.3.cmml">+</mo><mrow id="S2.E4.m1.6.6.2" xref="S2.E4.m1.6.6.2.cmml"><mi id="S2.E4.m1.6.6.2.3" xref="S2.E4.m1.6.6.2.3.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.6.6.2.2" xref="S2.E4.m1.6.6.2.2.cmml">​</mo><msubsup id="S2.E4.m1.6.6.2.1" xref="S2.E4.m1.6.6.2.1.cmml"><mrow id="S2.E4.m1.6.6.2.1.1.1.1" xref="S2.E4.m1.6.6.2.1.1.1.2.cmml"><mo id="S2.E4.m1.6.6.2.1.1.1.1.2" xref="S2.E4.m1.6.6.2.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E4.m1.6.6.2.1.1.1.1.1" xref="S2.E4.m1.6.6.2.1.1.1.1.1.cmml"><mi id="S2.E4.m1.6.6.2.1.1.1.1.1.2" xref="S2.E4.m1.6.6.2.1.1.1.1.1.2.cmml">𝐂</mi><mo id="S2.E4.m1.6.6.2.1.1.1.1.1.1" xref="S2.E4.m1.6.6.2.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.E4.m1.6.6.2.1.1.1.1.1.3" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.cmml"><msub id="S2.E4.m1.6.6.2.1.1.1.1.1.3.2" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.cmml"><mi id="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.2" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.2.cmml">𝐌𝐖</mi><mi id="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.3" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.3.cmml">𝐞</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.6.6.2.1.1.1.1.1.3.1" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.E4.m1.6.6.2.1.1.1.1.1.3.3" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.3.cmml">𝐗</mi></mrow></mrow><mo id="S2.E4.m1.6.6.2.1.1.1.1.3" xref="S2.E4.m1.6.6.2.1.1.1.2.1.cmml">‖</mo></mrow><mi id="S2.E4.m1.6.6.2.1.1.3" xref="S2.E4.m1.6.6.2.1.1.3.cmml">𝐅</mi><mn id="S2.E4.m1.6.6.2.1.3" xref="S2.E4.m1.6.6.2.1.3.cmml">𝟐</mn></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.6b"><apply id="S2.E4.m1.6.6.cmml" xref="S2.E4.m1.6.6"><plus id="S2.E4.m1.6.6.3.cmml" xref="S2.E4.m1.6.6.3"></plus><apply id="S2.E4.m1.5.5.1.cmml" xref="S2.E4.m1.5.5.1"><apply id="S2.E4.m1.5.5.1.2.cmml" xref="S2.E4.m1.5.5.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.2.1.cmml" xref="S2.E4.m1.5.5.1.2">subscript</csymbol><min id="S2.E4.m1.5.5.1.2.2.cmml" xref="S2.E4.m1.5.5.1.2.2"></min><list id="S2.E4.m1.4.4.4.5.cmml" xref="S2.E4.m1.4.4.4.4.4"><ci id="S2.E4.m1.2.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2.2">𝐌</ci><apply id="S2.E4.m1.3.3.3.3.3.1.cmml" xref="S2.E4.m1.3.3.3.3.3.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.3.3.3.1.1.cmml" xref="S2.E4.m1.3.3.3.3.3.1">subscript</csymbol><ci id="S2.E4.m1.3.3.3.3.3.1.2.cmml" xref="S2.E4.m1.3.3.3.3.3.1.2">𝐖</ci><ci id="S2.E4.m1.3.3.3.3.3.1.3.cmml" xref="S2.E4.m1.3.3.3.3.3.1.3">𝐝</ci></apply><apply id="S2.E4.m1.4.4.4.4.4.2.cmml" xref="S2.E4.m1.4.4.4.4.4.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.4.4.4.2.1.cmml" xref="S2.E4.m1.4.4.4.4.4.2">subscript</csymbol><ci id="S2.E4.m1.4.4.4.4.4.2.2.cmml" xref="S2.E4.m1.4.4.4.4.4.2.2">𝐖</ci><ci id="S2.E4.m1.4.4.4.4.4.2.3.cmml" xref="S2.E4.m1.4.4.4.4.4.2.3">𝐞</ci></apply></list></apply><apply id="S2.E4.m1.5.5.1.1.cmml" xref="S2.E4.m1.5.5.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1">superscript</csymbol><apply id="S2.E4.m1.5.5.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1">subscript</csymbol><apply id="S2.E4.m1.5.5.1.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.5.5.1.1.1.1.2.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.2">norm</csymbol><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1"><minus id="S2.E4.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.1"></minus><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.2">𝐗</ci><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3"><times id="S2.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.1"></times><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.2">𝐖</ci><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.2.3">𝐝</ci></apply><apply id="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.2">𝐖</ci><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.3.3">𝐞</ci></apply><ci id="S2.E4.m1.5.5.1.1.1.1.1.1.3.4.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1.1.3.4">𝐗</ci></apply></apply></apply><ci id="S2.E4.m1.5.5.1.1.1.3.cmml" xref="S2.E4.m1.5.5.1.1.1.3">𝐅</ci></apply><cn type="integer" id="S2.E4.m1.5.5.1.1.3.cmml" xref="S2.E4.m1.5.5.1.1.3">2</cn></apply></apply><apply id="S2.E4.m1.6.6.2.cmml" xref="S2.E4.m1.6.6.2"><times id="S2.E4.m1.6.6.2.2.cmml" xref="S2.E4.m1.6.6.2.2"></times><ci id="S2.E4.m1.6.6.2.3.cmml" xref="S2.E4.m1.6.6.2.3">𝜆</ci><apply id="S2.E4.m1.6.6.2.1.cmml" xref="S2.E4.m1.6.6.2.1"><csymbol cd="ambiguous" id="S2.E4.m1.6.6.2.1.2.cmml" xref="S2.E4.m1.6.6.2.1">superscript</csymbol><apply id="S2.E4.m1.6.6.2.1.1.cmml" xref="S2.E4.m1.6.6.2.1"><csymbol cd="ambiguous" id="S2.E4.m1.6.6.2.1.1.2.cmml" xref="S2.E4.m1.6.6.2.1">subscript</csymbol><apply id="S2.E4.m1.6.6.2.1.1.1.2.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.6.6.2.1.1.1.2.1.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.2">norm</csymbol><apply id="S2.E4.m1.6.6.2.1.1.1.1.1.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1"><minus id="S2.E4.m1.6.6.2.1.1.1.1.1.1.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.1"></minus><ci id="S2.E4.m1.6.6.2.1.1.1.1.1.2.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.2">𝐂</ci><apply id="S2.E4.m1.6.6.2.1.1.1.1.1.3.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3"><times id="S2.E4.m1.6.6.2.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.1"></times><apply id="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.1.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.2.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.2">𝐌𝐖</ci><ci id="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.3.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.2.3">𝐞</ci></apply><ci id="S2.E4.m1.6.6.2.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.6.6.2.1.1.1.1.1.3.3">𝐗</ci></apply></apply></apply><ci id="S2.E4.m1.6.6.2.1.1.3.cmml" xref="S2.E4.m1.6.6.2.1.1.3">𝐅</ci></apply><cn type="integer" id="S2.E4.m1.6.6.2.1.3.cmml" xref="S2.E4.m1.6.6.2.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.6c">\mathbf{\min_{\textit{$\mathbf{M,W_{d},W_{e}}$}}\left\|X-W_{d}W_{e}X\right\|_{F}^{2}+\lambda\left\|C-MW_{e}X\right\|_{F}^{2}}\vspace{-5pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.p4.12" class="ltx_p">where, the regularization parameter, <math id="S2.p4.12.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.p4.12.m1.1a"><mi id="S2.p4.12.m1.1.1" xref="S2.p4.12.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S2.p4.12.m1.1b"><ci id="S2.p4.12.m1.1.1.cmml" xref="S2.p4.12.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.12.m1.1c">\lambda</annotation></semantics></math>, controls the relative contribution of the two terms. In Equation <a href="#S2.E4" title="In 2 Proposed Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the first term corresponds to the loss function of the traditional autoencoder. It facilitates learning of parameters such that the reconstruction error is minimized and a meaningful representation is learned. The second term incorporates supervision in the autoencoder formulation and enables the model to learn a discriminative representation of the input data. The weight matrix is learned such that upon projecting the input data on it, the representation maps to a specific class label, thus encoding class-specific information in the feature learning process. Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Proposed Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives an overview of the proposed model, and it’s comparison with the traditional unsupervised autoencoder.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1710.02856/assets/x2.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="149" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Traditional Unsupervised Autoencoder</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1710.02856/assets/x3.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="149" height="165" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Proposed Class-Encoder</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><math id="S2.F2.4.m1.1" class="ltx_Math" alttext="\mathbf{W_{e}}" display="inline"><semantics id="S2.F2.4.m1.1b"><msub id="S2.F2.4.m1.1.1" xref="S2.F2.4.m1.1.1.cmml"><mi id="S2.F2.4.m1.1.1.2" xref="S2.F2.4.m1.1.1.2.cmml">𝐖</mi><mi id="S2.F2.4.m1.1.1.3" xref="S2.F2.4.m1.1.1.3.cmml">𝐞</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.4.m1.1c"><apply id="S2.F2.4.m1.1.1.cmml" xref="S2.F2.4.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.4.m1.1.1.1.cmml" xref="S2.F2.4.m1.1.1">subscript</csymbol><ci id="S2.F2.4.m1.1.1.2.cmml" xref="S2.F2.4.m1.1.1.2">𝐖</ci><ci id="S2.F2.4.m1.1.1.3.cmml" xref="S2.F2.4.m1.1.1.3">𝐞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.m1.1d">\mathbf{W_{e}}</annotation></semantics></math> and <math id="S2.F2.5.m2.1" class="ltx_Math" alttext="\mathbf{W_{d}}" display="inline"><semantics id="S2.F2.5.m2.1b"><msub id="S2.F2.5.m2.1.1" xref="S2.F2.5.m2.1.1.cmml"><mi id="S2.F2.5.m2.1.1.2" xref="S2.F2.5.m2.1.1.2.cmml">𝐖</mi><mi id="S2.F2.5.m2.1.1.3" xref="S2.F2.5.m2.1.1.3.cmml">𝐝</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.5.m2.1c"><apply id="S2.F2.5.m2.1.1.cmml" xref="S2.F2.5.m2.1.1"><csymbol cd="ambiguous" id="S2.F2.5.m2.1.1.1.cmml" xref="S2.F2.5.m2.1.1">subscript</csymbol><ci id="S2.F2.5.m2.1.1.2.cmml" xref="S2.F2.5.m2.1.1.2">𝐖</ci><ci id="S2.F2.5.m2.1.1.3.cmml" xref="S2.F2.5.m2.1.1.3">𝐝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m2.1d">\mathbf{W_{d}}</annotation></semantics></math> correspond to the encoding and decoding weights respectively, while <math id="S2.F2.6.m3.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S2.F2.6.m3.1b"><mi id="S2.F2.6.m3.1.1" xref="S2.F2.6.m3.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S2.F2.6.m3.1c"><ci id="S2.F2.6.m3.1.1.cmml" xref="S2.F2.6.m3.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m3.1d">\mathbf{M}</annotation></semantics></math> captures the linear mapping between the feature vector and its class label.</figcaption>
</figure>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Equation <a href="#S2.E4" title="In 2 Proposed Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> represents a least square formulation, however, the large size of matrices involved and the non-convexity of the problem increases the computational costs. To mitigate the same, the Majorization-Minimization (MM) technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> along with Alternating Direction Method of Multipliers (ADMM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is utilized. The aim of the MM technique is to replace complex equations with simpler and easier optimization steps. Moreover, ADMM does not involve computing the derivatives at each epoch, which leads to significant lower training time as compared to a traditional Stacked Autoencoder.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">For a <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p6.1.m1.1a"><mi id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><ci id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">k</annotation></semantics></math> layer model of Deep Class-Encoder, Equation <a href="#S2.E4" title="In 2 Proposed Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> can be extended as follows:</p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.57" class="ltx_Math" alttext="\begin{gathered}\mathbf{\min_{\textit{$\mathbf{M,W_{d},W_{e}}$}}\left\|X-(W_{d}^{1}W_{d}^{2}...W_{d}^{\textit{k}}(W_{e}^{\textit{k}}...W_{e}^{2}W_{e}^{1}X))\right\|_{F}^{2}}+\\
\sum_{i=1}^{k}\mathbf{\lambda\left\|C-M^{\textit{i}}(W_{e}^{\textit{i}}X^{\textit{i}})\right\|_{F}^{2}}\end{gathered}" display="block"><semantics id="S2.E5.m1.57a"><mtable displaystyle="true" rowspacing="0pt" id="S2.E5.m1.57.57.4" xref="S2.E5.m1.55.55.2.cmml"><mtr id="S2.E5.m1.57.57.4a" xref="S2.E5.m1.55.55.2.cmml"><mtd id="S2.E5.m1.57.57.4b" xref="S2.E5.m1.55.55.2.cmml"><mrow id="S2.E5.m1.56.56.3.54.35.35" xref="S2.E5.m1.55.55.2.cmml"><mrow id="S2.E5.m1.56.56.3.54.35.35.35" xref="S2.E5.m1.55.55.2.cmml"><munder id="S2.E5.m1.56.56.3.54.35.35.35.2" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.2.2.2.2.2.2" xref="S2.E5.m1.2.2.2.2.2.2.cmml">min</mi><mrow id="S2.E5.m1.1.1.1.1.1.1.1.4.4" xref="S2.E5.m1.1.1.1.1.1.1.1.5.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.2.2" xref="S2.E5.m1.1.1.1.1.1.1.1.2.2.cmml">𝐌</mi><mo id="S2.E5.m1.1.1.1.1.1.1.1.4.4.3" xref="S2.E5.m1.1.1.1.1.1.1.1.5.cmml">,</mo><msub id="S2.E5.m1.1.1.1.1.1.1.1.3.3.1" xref="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.2.cmml">𝐖</mi><mi mathsize="98%" id="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.3.cmml">𝐝</mi></msub><mo id="S2.E5.m1.1.1.1.1.1.1.1.4.4.4" xref="S2.E5.m1.1.1.1.1.1.1.1.5.cmml">,</mo><msub id="S2.E5.m1.1.1.1.1.1.1.1.4.4.2" xref="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.2" xref="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.2.cmml">𝐖</mi><mi mathsize="98%" id="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.3" xref="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.3.cmml">𝐞</mi></msub></mrow></munder><mo id="S2.E5.m1.56.56.3.54.35.35.35a" xref="S2.E5.m1.55.55.2.cmml">⁡</mo><msubsup id="S2.E5.m1.56.56.3.54.35.35.35.1" xref="S2.E5.m1.55.55.2.cmml"><mrow id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><mo id="S2.E5.m1.3.3.3.3.3.3" xref="S2.E5.m1.55.55.2.cmml">‖</mo><mrow id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.4.4.4.4.4.4" xref="S2.E5.m1.4.4.4.4.4.4.cmml">𝐗</mi><mo id="S2.E5.m1.5.5.5.5.5.5" xref="S2.E5.m1.5.5.5.5.5.5.cmml">−</mo><mrow id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><mo stretchy="false" id="S2.E5.m1.6.6.6.6.6.6" xref="S2.E5.m1.55.55.2.cmml">(</mo><mrow id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><msubsup id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.7.7.7.7.7.7" xref="S2.E5.m1.7.7.7.7.7.7.cmml">𝐖</mi><mi id="S2.E5.m1.8.8.8.8.8.8.1" xref="S2.E5.m1.8.8.8.8.8.8.1.cmml">𝐝</mi><mn id="S2.E5.m1.9.9.9.9.9.9.1" xref="S2.E5.m1.9.9.9.9.9.9.1.cmml">𝟏</mn></msubsup><mo lspace="0em" rspace="0em" id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.55.55.2.cmml">​</mo><msubsup id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.4" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.10.10.10.10.10.10" xref="S2.E5.m1.10.10.10.10.10.10.cmml">𝐖</mi><mi id="S2.E5.m1.11.11.11.11.11.11.1" xref="S2.E5.m1.11.11.11.11.11.11.1.cmml">𝐝</mi><mn id="S2.E5.m1.12.12.12.12.12.12.1" xref="S2.E5.m1.12.12.12.12.12.12.1.cmml">𝟐</mn></msubsup><mo lspace="0em" rspace="0em" id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.2a" xref="S2.E5.m1.55.55.2.cmml">​</mo><mi mathvariant="normal" id="S2.E5.m1.13.13.13.13.13.13" xref="S2.E5.m1.13.13.13.13.13.13.cmml">…</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.2b" xref="S2.E5.m1.55.55.2.cmml">​</mo><msubsup id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.5" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.14.14.14.14.14.14" xref="S2.E5.m1.14.14.14.14.14.14.cmml">𝐖</mi><mi id="S2.E5.m1.15.15.15.15.15.15.1" xref="S2.E5.m1.15.15.15.15.15.15.1.cmml">𝐝</mi><mtext class="ltx_mathvariant_italic" id="S2.E5.m1.16.16.16.16.16.16.1" xref="S2.E5.m1.16.16.16.16.16.16.1a.cmml">k</mtext></msubsup><mo lspace="0em" rspace="0em" id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.2c" xref="S2.E5.m1.55.55.2.cmml">​</mo><mrow id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><mo stretchy="false" id="S2.E5.m1.17.17.17.17.17.17" xref="S2.E5.m1.55.55.2.cmml">(</mo><mrow id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><msubsup id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.18.18.18.18.18.18" xref="S2.E5.m1.18.18.18.18.18.18.cmml">𝐖</mi><mi id="S2.E5.m1.19.19.19.19.19.19.1" xref="S2.E5.m1.19.19.19.19.19.19.1.cmml">𝐞</mi><mtext class="ltx_mathvariant_italic" id="S2.E5.m1.20.20.20.20.20.20.1" xref="S2.E5.m1.20.20.20.20.20.20.1a.cmml">k</mtext></msubsup><mo lspace="0em" rspace="0em" id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml">​</mo><mi mathvariant="normal" id="S2.E5.m1.21.21.21.21.21.21" xref="S2.E5.m1.21.21.21.21.21.21.cmml">…</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S2.E5.m1.55.55.2.cmml">​</mo><msubsup id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.22.22.22.22.22.22" xref="S2.E5.m1.22.22.22.22.22.22.cmml">𝐖</mi><mi id="S2.E5.m1.23.23.23.23.23.23.1" xref="S2.E5.m1.23.23.23.23.23.23.1.cmml">𝐞</mi><mn id="S2.E5.m1.24.24.24.24.24.24.1" xref="S2.E5.m1.24.24.24.24.24.24.1.cmml">𝟐</mn></msubsup><mo lspace="0em" rspace="0em" id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1.1.1b" xref="S2.E5.m1.55.55.2.cmml">​</mo><msubsup id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1.1.4" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.25.25.25.25.25.25" xref="S2.E5.m1.25.25.25.25.25.25.cmml">𝐖</mi><mi id="S2.E5.m1.26.26.26.26.26.26.1" xref="S2.E5.m1.26.26.26.26.26.26.1.cmml">𝐞</mi><mn id="S2.E5.m1.27.27.27.27.27.27.1" xref="S2.E5.m1.27.27.27.27.27.27.1.cmml">𝟏</mn></msubsup><mo lspace="0em" rspace="0em" id="S2.E5.m1.56.56.3.54.35.35.35.1.1.1.1.1.1.1.1.1.1.1.1c" xref="S2.E5.m1.55.55.2.cmml">​</mo><mi id="S2.E5.m1.28.28.28.28.28.28" xref="S2.E5.m1.28.28.28.28.28.28.cmml">𝐗</mi></mrow><mo stretchy="false" id="S2.E5.m1.29.29.29.29.29.29" xref="S2.E5.m1.55.55.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E5.m1.30.30.30.30.30.30" xref="S2.E5.m1.55.55.2.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.31.31.31.31.31.31" xref="S2.E5.m1.55.55.2.cmml">‖</mo></mrow><mi id="S2.E5.m1.32.32.32.32.32.32.1" xref="S2.E5.m1.32.32.32.32.32.32.1.cmml">𝐅</mi><mn id="S2.E5.m1.33.33.33.33.33.33.1" xref="S2.E5.m1.33.33.33.33.33.33.1.cmml">𝟐</mn></msubsup></mrow><mo id="S2.E5.m1.34.34.34.34.34.34" xref="S2.E5.m1.34.34.34.34.34.34.cmml">+</mo></mrow></mtd></mtr><mtr id="S2.E5.m1.57.57.4c" xref="S2.E5.m1.55.55.2.cmml"><mtd id="S2.E5.m1.57.57.4d" xref="S2.E5.m1.55.55.2.cmml"><mrow id="S2.E5.m1.57.57.4.55.20.20" xref="S2.E5.m1.55.55.2.cmml"><munderover id="S2.E5.m1.57.57.4.55.20.20.21" xref="S2.E5.m1.55.55.2.cmml"><mo movablelimits="false" id="S2.E5.m1.35.35.35.1.1.1" xref="S2.E5.m1.35.35.35.1.1.1.cmml">∑</mo><mrow id="S2.E5.m1.36.36.36.2.2.2.1" xref="S2.E5.m1.36.36.36.2.2.2.1.cmml"><mi id="S2.E5.m1.36.36.36.2.2.2.1.2" xref="S2.E5.m1.36.36.36.2.2.2.1.2.cmml">i</mi><mo id="S2.E5.m1.36.36.36.2.2.2.1.1" xref="S2.E5.m1.36.36.36.2.2.2.1.1.cmml">=</mo><mn id="S2.E5.m1.36.36.36.2.2.2.1.3" xref="S2.E5.m1.36.36.36.2.2.2.1.3.cmml">1</mn></mrow><mi id="S2.E5.m1.37.37.37.3.3.3.1" xref="S2.E5.m1.37.37.37.3.3.3.1.cmml">k</mi></munderover><mrow id="S2.E5.m1.57.57.4.55.20.20.20" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.38.38.38.4.4.4" xref="S2.E5.m1.38.38.38.4.4.4.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.57.57.4.55.20.20.20.2" xref="S2.E5.m1.55.55.2.cmml">​</mo><msubsup id="S2.E5.m1.57.57.4.55.20.20.20.1" xref="S2.E5.m1.55.55.2.cmml"><mrow id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><mo id="S2.E5.m1.39.39.39.5.5.5" xref="S2.E5.m1.55.55.2.cmml">‖</mo><mrow id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.40.40.40.6.6.6" xref="S2.E5.m1.40.40.40.6.6.6.cmml">𝐂</mi><mo id="S2.E5.m1.41.41.41.7.7.7" xref="S2.E5.m1.41.41.41.7.7.7.cmml">−</mo><mrow id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><msup id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1.1.3" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.42.42.42.8.8.8" xref="S2.E5.m1.42.42.42.8.8.8.cmml">𝐌</mi><mtext class="ltx_mathvariant_italic" id="S2.E5.m1.43.43.43.9.9.9.1" xref="S2.E5.m1.43.43.43.9.9.9.1a.cmml">i</mtext></msup><mo lspace="0em" rspace="0em" id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1.1.2" xref="S2.E5.m1.55.55.2.cmml">​</mo><mrow id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><mo stretchy="false" id="S2.E5.m1.44.44.44.10.10.10" xref="S2.E5.m1.55.55.2.cmml">(</mo><mrow id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml"><msubsup id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.45.45.45.11.11.11" xref="S2.E5.m1.45.45.45.11.11.11.cmml">𝐖</mi><mi id="S2.E5.m1.46.46.46.12.12.12.1" xref="S2.E5.m1.46.46.46.12.12.12.1.cmml">𝐞</mi><mtext class="ltx_mathvariant_italic" id="S2.E5.m1.47.47.47.13.13.13.1" xref="S2.E5.m1.47.47.47.13.13.13.1a.cmml">i</mtext></msubsup><mo lspace="0em" rspace="0em" id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.55.55.2.cmml">​</mo><msup id="S2.E5.m1.57.57.4.55.20.20.20.1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.55.55.2.cmml"><mi id="S2.E5.m1.48.48.48.14.14.14" xref="S2.E5.m1.48.48.48.14.14.14.cmml">𝐗</mi><mtext class="ltx_mathvariant_italic" id="S2.E5.m1.49.49.49.15.15.15.1" xref="S2.E5.m1.49.49.49.15.15.15.1a.cmml">i</mtext></msup></mrow><mo stretchy="false" id="S2.E5.m1.50.50.50.16.16.16" xref="S2.E5.m1.55.55.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E5.m1.51.51.51.17.17.17" xref="S2.E5.m1.55.55.2.cmml">‖</mo></mrow><mi id="S2.E5.m1.52.52.52.18.18.18.1" xref="S2.E5.m1.52.52.52.18.18.18.1.cmml">𝐅</mi><mn id="S2.E5.m1.53.53.53.19.19.19.1" xref="S2.E5.m1.53.53.53.19.19.19.1.cmml">𝟐</mn></msubsup></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E5.m1.57b"><apply id="S2.E5.m1.55.55.2.cmml" xref="S2.E5.m1.57.57.4"><plus id="S2.E5.m1.34.34.34.34.34.34.cmml" xref="S2.E5.m1.34.34.34.34.34.34"></plus><apply id="S2.E5.m1.54.54.1.1.cmml" xref="S2.E5.m1.57.57.4"><apply id="S2.E5.m1.54.54.1.1.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><min id="S2.E5.m1.2.2.2.2.2.2.cmml" xref="S2.E5.m1.2.2.2.2.2.2"></min><list id="S2.E5.m1.1.1.1.1.1.1.1.5.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.4.4"><ci id="S2.E5.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.2.2">𝐌</ci><apply id="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.3.3.1">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.2">𝐖</ci><ci id="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.3.3.1.3">𝐝</ci></apply><apply id="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.4.4.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.4.4.2">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.2">𝐖</ci><ci id="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.4.4.2.3">𝐞</ci></apply></list></apply><apply id="S2.E5.m1.54.54.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="latexml" id="S2.E5.m1.54.54.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.57.57.4">delimited-∥∥</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><minus id="S2.E5.m1.5.5.5.5.5.5.cmml" xref="S2.E5.m1.5.5.5.5.5.5"></minus><ci id="S2.E5.m1.4.4.4.4.4.4.cmml" xref="S2.E5.m1.4.4.4.4.4.4">𝐗</ci><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><times id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4"></times><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><ci id="S2.E5.m1.7.7.7.7.7.7.cmml" xref="S2.E5.m1.7.7.7.7.7.7">𝐖</ci><ci id="S2.E5.m1.8.8.8.8.8.8.1.cmml" xref="S2.E5.m1.8.8.8.8.8.8.1">𝐝</ci></apply><cn type="integer" id="S2.E5.m1.9.9.9.9.9.9.1.cmml" xref="S2.E5.m1.9.9.9.9.9.9.1">1</cn></apply><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><ci id="S2.E5.m1.10.10.10.10.10.10.cmml" xref="S2.E5.m1.10.10.10.10.10.10">𝐖</ci><ci id="S2.E5.m1.11.11.11.11.11.11.1.cmml" xref="S2.E5.m1.11.11.11.11.11.11.1">𝐝</ci></apply><cn type="integer" id="S2.E5.m1.12.12.12.12.12.12.1.cmml" xref="S2.E5.m1.12.12.12.12.12.12.1">2</cn></apply><ci id="S2.E5.m1.13.13.13.13.13.13.cmml" xref="S2.E5.m1.13.13.13.13.13.13">…</ci><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.6.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.6.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.6.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><ci id="S2.E5.m1.14.14.14.14.14.14.cmml" xref="S2.E5.m1.14.14.14.14.14.14">𝐖</ci><ci id="S2.E5.m1.15.15.15.15.15.15.1.cmml" xref="S2.E5.m1.15.15.15.15.15.15.1">𝐝</ci></apply><ci id="S2.E5.m1.16.16.16.16.16.16.1a.cmml" xref="S2.E5.m1.16.16.16.16.16.16.1"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.E5.m1.16.16.16.16.16.16.1.cmml" xref="S2.E5.m1.16.16.16.16.16.16.1">k</mtext></ci></apply><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><times id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"></times><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><ci id="S2.E5.m1.18.18.18.18.18.18.cmml" xref="S2.E5.m1.18.18.18.18.18.18">𝐖</ci><ci id="S2.E5.m1.19.19.19.19.19.19.1.cmml" xref="S2.E5.m1.19.19.19.19.19.19.1">𝐞</ci></apply><ci id="S2.E5.m1.20.20.20.20.20.20.1a.cmml" xref="S2.E5.m1.20.20.20.20.20.20.1"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.E5.m1.20.20.20.20.20.20.1.cmml" xref="S2.E5.m1.20.20.20.20.20.20.1">k</mtext></ci></apply><ci id="S2.E5.m1.21.21.21.21.21.21.cmml" xref="S2.E5.m1.21.21.21.21.21.21">…</ci><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><ci id="S2.E5.m1.22.22.22.22.22.22.cmml" xref="S2.E5.m1.22.22.22.22.22.22">𝐖</ci><ci id="S2.E5.m1.23.23.23.23.23.23.1.cmml" xref="S2.E5.m1.23.23.23.23.23.23.1">𝐞</ci></apply><cn type="integer" id="S2.E5.m1.24.24.24.24.24.24.1.cmml" xref="S2.E5.m1.24.24.24.24.24.24.1">2</cn></apply><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.54.54.1.1.1.1.1.1.1.1.1.1.1.1.1.5.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><ci id="S2.E5.m1.25.25.25.25.25.25.cmml" xref="S2.E5.m1.25.25.25.25.25.25">𝐖</ci><ci id="S2.E5.m1.26.26.26.26.26.26.1.cmml" xref="S2.E5.m1.26.26.26.26.26.26.1">𝐞</ci></apply><cn type="integer" id="S2.E5.m1.27.27.27.27.27.27.1.cmml" xref="S2.E5.m1.27.27.27.27.27.27.1">1</cn></apply><ci id="S2.E5.m1.28.28.28.28.28.28.cmml" xref="S2.E5.m1.28.28.28.28.28.28">𝐗</ci></apply></apply></apply></apply><ci id="S2.E5.m1.32.32.32.32.32.32.1.cmml" xref="S2.E5.m1.32.32.32.32.32.32.1">𝐅</ci></apply><cn type="integer" id="S2.E5.m1.33.33.33.33.33.33.1.cmml" xref="S2.E5.m1.33.33.33.33.33.33.1">2</cn></apply></apply><apply id="S2.E5.m1.55.55.2.2.cmml" xref="S2.E5.m1.57.57.4"><apply id="S2.E5.m1.55.55.2.2.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.55.55.2.2.2.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.55.55.2.2.2.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.55.55.2.2.2.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><sum id="S2.E5.m1.35.35.35.1.1.1.cmml" xref="S2.E5.m1.35.35.35.1.1.1"></sum><apply id="S2.E5.m1.36.36.36.2.2.2.1.cmml" xref="S2.E5.m1.36.36.36.2.2.2.1"><eq id="S2.E5.m1.36.36.36.2.2.2.1.1.cmml" xref="S2.E5.m1.36.36.36.2.2.2.1.1"></eq><ci id="S2.E5.m1.36.36.36.2.2.2.1.2.cmml" xref="S2.E5.m1.36.36.36.2.2.2.1.2">𝑖</ci><cn type="integer" id="S2.E5.m1.36.36.36.2.2.2.1.3.cmml" xref="S2.E5.m1.36.36.36.2.2.2.1.3">1</cn></apply></apply><ci id="S2.E5.m1.37.37.37.3.3.3.1.cmml" xref="S2.E5.m1.37.37.37.3.3.3.1">𝑘</ci></apply><apply id="S2.E5.m1.55.55.2.2.1.cmml" xref="S2.E5.m1.57.57.4"><times id="S2.E5.m1.55.55.2.2.1.2.cmml" xref="S2.E5.m1.57.57.4"></times><ci id="S2.E5.m1.38.38.38.4.4.4.cmml" xref="S2.E5.m1.38.38.38.4.4.4">𝜆</ci><apply id="S2.E5.m1.55.55.2.2.1.1.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.55.55.2.2.1.1.2.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.55.55.2.2.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.55.55.2.2.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><apply id="S2.E5.m1.55.55.2.2.1.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="latexml" id="S2.E5.m1.55.55.2.2.1.1.1.1.2.1.cmml" xref="S2.E5.m1.57.57.4">delimited-∥∥</csymbol><apply id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><minus id="S2.E5.m1.41.41.41.7.7.7.cmml" xref="S2.E5.m1.41.41.41.7.7.7"></minus><ci id="S2.E5.m1.40.40.40.6.6.6.cmml" xref="S2.E5.m1.40.40.40.6.6.6">𝐂</ci><apply id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><times id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4"></times><apply id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><ci id="S2.E5.m1.42.42.42.8.8.8.cmml" xref="S2.E5.m1.42.42.42.8.8.8">𝐌</ci><ci id="S2.E5.m1.43.43.43.9.9.9.1a.cmml" xref="S2.E5.m1.43.43.43.9.9.9.1"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.E5.m1.43.43.43.9.9.9.1.cmml" xref="S2.E5.m1.43.43.43.9.9.9.1">i</mtext></ci></apply><apply id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"><times id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.57.57.4"></times><apply id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><apply id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E5.m1.57.57.4">subscript</csymbol><ci id="S2.E5.m1.45.45.45.11.11.11.cmml" xref="S2.E5.m1.45.45.45.11.11.11">𝐖</ci><ci id="S2.E5.m1.46.46.46.12.12.12.1.cmml" xref="S2.E5.m1.46.46.46.12.12.12.1">𝐞</ci></apply><ci id="S2.E5.m1.47.47.47.13.13.13.1a.cmml" xref="S2.E5.m1.47.47.47.13.13.13.1"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.E5.m1.47.47.47.13.13.13.1.cmml" xref="S2.E5.m1.47.47.47.13.13.13.1">i</mtext></ci></apply><apply id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.57.57.4"><csymbol cd="ambiguous" id="S2.E5.m1.55.55.2.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.57.57.4">superscript</csymbol><ci id="S2.E5.m1.48.48.48.14.14.14.cmml" xref="S2.E5.m1.48.48.48.14.14.14">𝐗</ci><ci id="S2.E5.m1.49.49.49.15.15.15.1a.cmml" xref="S2.E5.m1.49.49.49.15.15.15.1"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.E5.m1.49.49.49.15.15.15.1.cmml" xref="S2.E5.m1.49.49.49.15.15.15.1">i</mtext></ci></apply></apply></apply></apply></apply><ci id="S2.E5.m1.52.52.52.18.18.18.1.cmml" xref="S2.E5.m1.52.52.52.18.18.18.1">𝐅</ci></apply><cn type="integer" id="S2.E5.m1.53.53.53.19.19.19.1.cmml" xref="S2.E5.m1.53.53.53.19.19.19.1">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.57c">\begin{gathered}\mathbf{\min_{\textit{$\mathbf{M,W_{d},W_{e}}$}}\left\|X-(W_{d}^{1}W_{d}^{2}...W_{d}^{\textit{k}}(W_{e}^{\textit{k}}...W_{e}^{2}W_{e}^{1}X))\right\|_{F}^{2}}+\\
\sum_{i=1}^{k}\mathbf{\lambda\left\|C-M^{\textit{i}}(W_{e}^{\textit{i}}X^{\textit{i}})\right\|_{F}^{2}}\end{gathered}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.p6.10" class="ltx_p">where, <math id="S2.p6.2.m1.1" class="ltx_Math" alttext="\mathbf{W_{e}^{\textit{k}}}" display="inline"><semantics id="S2.p6.2.m1.1a"><msubsup id="S2.p6.2.m1.1.1" xref="S2.p6.2.m1.1.1.cmml"><mi id="S2.p6.2.m1.1.1.2.2" xref="S2.p6.2.m1.1.1.2.2.cmml">𝐖</mi><mi id="S2.p6.2.m1.1.1.2.3" xref="S2.p6.2.m1.1.1.2.3.cmml">𝐞</mi><mtext class="ltx_mathvariant_italic" id="S2.p6.2.m1.1.1.3" xref="S2.p6.2.m1.1.1.3a.cmml">k</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S2.p6.2.m1.1b"><apply id="S2.p6.2.m1.1.1.cmml" xref="S2.p6.2.m1.1.1"><csymbol cd="ambiguous" id="S2.p6.2.m1.1.1.1.cmml" xref="S2.p6.2.m1.1.1">superscript</csymbol><apply id="S2.p6.2.m1.1.1.2.cmml" xref="S2.p6.2.m1.1.1"><csymbol cd="ambiguous" id="S2.p6.2.m1.1.1.2.1.cmml" xref="S2.p6.2.m1.1.1">subscript</csymbol><ci id="S2.p6.2.m1.1.1.2.2.cmml" xref="S2.p6.2.m1.1.1.2.2">𝐖</ci><ci id="S2.p6.2.m1.1.1.2.3.cmml" xref="S2.p6.2.m1.1.1.2.3">𝐞</ci></apply><ci id="S2.p6.2.m1.1.1.3a.cmml" xref="S2.p6.2.m1.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.p6.2.m1.1.1.3.cmml" xref="S2.p6.2.m1.1.1.3">k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m1.1c">\mathbf{W_{e}^{\textit{k}}}</annotation></semantics></math>, <math id="S2.p6.3.m2.1" class="ltx_Math" alttext="\mathbf{W_{d}^{\textit{k}}}" display="inline"><semantics id="S2.p6.3.m2.1a"><msubsup id="S2.p6.3.m2.1.1" xref="S2.p6.3.m2.1.1.cmml"><mi id="S2.p6.3.m2.1.1.2.2" xref="S2.p6.3.m2.1.1.2.2.cmml">𝐖</mi><mi id="S2.p6.3.m2.1.1.2.3" xref="S2.p6.3.m2.1.1.2.3.cmml">𝐝</mi><mtext class="ltx_mathvariant_italic" id="S2.p6.3.m2.1.1.3" xref="S2.p6.3.m2.1.1.3a.cmml">k</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S2.p6.3.m2.1b"><apply id="S2.p6.3.m2.1.1.cmml" xref="S2.p6.3.m2.1.1"><csymbol cd="ambiguous" id="S2.p6.3.m2.1.1.1.cmml" xref="S2.p6.3.m2.1.1">superscript</csymbol><apply id="S2.p6.3.m2.1.1.2.cmml" xref="S2.p6.3.m2.1.1"><csymbol cd="ambiguous" id="S2.p6.3.m2.1.1.2.1.cmml" xref="S2.p6.3.m2.1.1">subscript</csymbol><ci id="S2.p6.3.m2.1.1.2.2.cmml" xref="S2.p6.3.m2.1.1.2.2">𝐖</ci><ci id="S2.p6.3.m2.1.1.2.3.cmml" xref="S2.p6.3.m2.1.1.2.3">𝐝</ci></apply><ci id="S2.p6.3.m2.1.1.3a.cmml" xref="S2.p6.3.m2.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.p6.3.m2.1.1.3.cmml" xref="S2.p6.3.m2.1.1.3">k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.3.m2.1c">\mathbf{W_{d}^{\textit{k}}}</annotation></semantics></math>, and <math id="S2.p6.4.m3.1" class="ltx_Math" alttext="\mathbf{M^{\textit{k}}}" display="inline"><semantics id="S2.p6.4.m3.1a"><msup id="S2.p6.4.m3.1.1" xref="S2.p6.4.m3.1.1.cmml"><mi id="S2.p6.4.m3.1.1.2" xref="S2.p6.4.m3.1.1.2.cmml">𝐌</mi><mtext class="ltx_mathvariant_italic" id="S2.p6.4.m3.1.1.3" xref="S2.p6.4.m3.1.1.3a.cmml">k</mtext></msup><annotation-xml encoding="MathML-Content" id="S2.p6.4.m3.1b"><apply id="S2.p6.4.m3.1.1.cmml" xref="S2.p6.4.m3.1.1"><csymbol cd="ambiguous" id="S2.p6.4.m3.1.1.1.cmml" xref="S2.p6.4.m3.1.1">superscript</csymbol><ci id="S2.p6.4.m3.1.1.2.cmml" xref="S2.p6.4.m3.1.1.2">𝐌</ci><ci id="S2.p6.4.m3.1.1.3a.cmml" xref="S2.p6.4.m3.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.p6.4.m3.1.1.3.cmml" xref="S2.p6.4.m3.1.1.3">k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.4.m3.1c">\mathbf{M^{\textit{k}}}</annotation></semantics></math> correspond to the encoding weights, decoding weights and the mapping matrix of the <math id="S2.p6.5.m4.1" class="ltx_Math" alttext="k^{th}" display="inline"><semantics id="S2.p6.5.m4.1a"><msup id="S2.p6.5.m4.1.1" xref="S2.p6.5.m4.1.1.cmml"><mi id="S2.p6.5.m4.1.1.2" xref="S2.p6.5.m4.1.1.2.cmml">k</mi><mrow id="S2.p6.5.m4.1.1.3" xref="S2.p6.5.m4.1.1.3.cmml"><mi id="S2.p6.5.m4.1.1.3.2" xref="S2.p6.5.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p6.5.m4.1.1.3.1" xref="S2.p6.5.m4.1.1.3.1.cmml">​</mo><mi id="S2.p6.5.m4.1.1.3.3" xref="S2.p6.5.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.p6.5.m4.1b"><apply id="S2.p6.5.m4.1.1.cmml" xref="S2.p6.5.m4.1.1"><csymbol cd="ambiguous" id="S2.p6.5.m4.1.1.1.cmml" xref="S2.p6.5.m4.1.1">superscript</csymbol><ci id="S2.p6.5.m4.1.1.2.cmml" xref="S2.p6.5.m4.1.1.2">𝑘</ci><apply id="S2.p6.5.m4.1.1.3.cmml" xref="S2.p6.5.m4.1.1.3"><times id="S2.p6.5.m4.1.1.3.1.cmml" xref="S2.p6.5.m4.1.1.3.1"></times><ci id="S2.p6.5.m4.1.1.3.2.cmml" xref="S2.p6.5.m4.1.1.3.2">𝑡</ci><ci id="S2.p6.5.m4.1.1.3.3.cmml" xref="S2.p6.5.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.5.m4.1c">k^{th}</annotation></semantics></math> layer, respectively. <math id="S2.p6.6.m5.1" class="ltx_Math" alttext="\mathbf{X^{\textit{k}}}" display="inline"><semantics id="S2.p6.6.m5.1a"><msup id="S2.p6.6.m5.1.1" xref="S2.p6.6.m5.1.1.cmml"><mi id="S2.p6.6.m5.1.1.2" xref="S2.p6.6.m5.1.1.2.cmml">𝐗</mi><mtext class="ltx_mathvariant_italic" id="S2.p6.6.m5.1.1.3" xref="S2.p6.6.m5.1.1.3a.cmml">k</mtext></msup><annotation-xml encoding="MathML-Content" id="S2.p6.6.m5.1b"><apply id="S2.p6.6.m5.1.1.cmml" xref="S2.p6.6.m5.1.1"><csymbol cd="ambiguous" id="S2.p6.6.m5.1.1.1.cmml" xref="S2.p6.6.m5.1.1">superscript</csymbol><ci id="S2.p6.6.m5.1.1.2.cmml" xref="S2.p6.6.m5.1.1.2">𝐗</ci><ci id="S2.p6.6.m5.1.1.3a.cmml" xref="S2.p6.6.m5.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.p6.6.m5.1.1.3.cmml" xref="S2.p6.6.m5.1.1.3">k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.6.m5.1c">\mathbf{X^{\textit{k}}}</annotation></semantics></math> refers to the input to the <math id="S2.p6.7.m6.1" class="ltx_Math" alttext="k^{th}" display="inline"><semantics id="S2.p6.7.m6.1a"><msup id="S2.p6.7.m6.1.1" xref="S2.p6.7.m6.1.1.cmml"><mi id="S2.p6.7.m6.1.1.2" xref="S2.p6.7.m6.1.1.2.cmml">k</mi><mrow id="S2.p6.7.m6.1.1.3" xref="S2.p6.7.m6.1.1.3.cmml"><mi id="S2.p6.7.m6.1.1.3.2" xref="S2.p6.7.m6.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p6.7.m6.1.1.3.1" xref="S2.p6.7.m6.1.1.3.1.cmml">​</mo><mi id="S2.p6.7.m6.1.1.3.3" xref="S2.p6.7.m6.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.p6.7.m6.1b"><apply id="S2.p6.7.m6.1.1.cmml" xref="S2.p6.7.m6.1.1"><csymbol cd="ambiguous" id="S2.p6.7.m6.1.1.1.cmml" xref="S2.p6.7.m6.1.1">superscript</csymbol><ci id="S2.p6.7.m6.1.1.2.cmml" xref="S2.p6.7.m6.1.1.2">𝑘</ci><apply id="S2.p6.7.m6.1.1.3.cmml" xref="S2.p6.7.m6.1.1.3"><times id="S2.p6.7.m6.1.1.3.1.cmml" xref="S2.p6.7.m6.1.1.3.1"></times><ci id="S2.p6.7.m6.1.1.3.2.cmml" xref="S2.p6.7.m6.1.1.3.2">𝑡</ci><ci id="S2.p6.7.m6.1.1.3.3.cmml" xref="S2.p6.7.m6.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.7.m6.1c">k^{th}</annotation></semantics></math> layer of the model, which is defined as <math id="S2.p6.8.m7.1" class="ltx_Math" alttext="\mathbf{W_{e}^{\textit{{k-1}}}X^{\textit{{k-1}}}}" display="inline"><semantics id="S2.p6.8.m7.1a"><mrow id="S2.p6.8.m7.1.1" xref="S2.p6.8.m7.1.1.cmml"><msubsup id="S2.p6.8.m7.1.1.2" xref="S2.p6.8.m7.1.1.2.cmml"><mi id="S2.p6.8.m7.1.1.2.2.2" xref="S2.p6.8.m7.1.1.2.2.2.cmml">𝐖</mi><mi id="S2.p6.8.m7.1.1.2.2.3" xref="S2.p6.8.m7.1.1.2.2.3.cmml">𝐞</mi><mtext class="ltx_mathvariant_italic" id="S2.p6.8.m7.1.1.2.3" xref="S2.p6.8.m7.1.1.2.3a.cmml">k-1</mtext></msubsup><mo lspace="0em" rspace="0em" id="S2.p6.8.m7.1.1.1" xref="S2.p6.8.m7.1.1.1.cmml">​</mo><msup id="S2.p6.8.m7.1.1.3" xref="S2.p6.8.m7.1.1.3.cmml"><mi id="S2.p6.8.m7.1.1.3.2" xref="S2.p6.8.m7.1.1.3.2.cmml">𝐗</mi><mtext class="ltx_mathvariant_italic" id="S2.p6.8.m7.1.1.3.3" xref="S2.p6.8.m7.1.1.3.3a.cmml">k-1</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.8.m7.1b"><apply id="S2.p6.8.m7.1.1.cmml" xref="S2.p6.8.m7.1.1"><times id="S2.p6.8.m7.1.1.1.cmml" xref="S2.p6.8.m7.1.1.1"></times><apply id="S2.p6.8.m7.1.1.2.cmml" xref="S2.p6.8.m7.1.1.2"><csymbol cd="ambiguous" id="S2.p6.8.m7.1.1.2.1.cmml" xref="S2.p6.8.m7.1.1.2">superscript</csymbol><apply id="S2.p6.8.m7.1.1.2.2.cmml" xref="S2.p6.8.m7.1.1.2"><csymbol cd="ambiguous" id="S2.p6.8.m7.1.1.2.2.1.cmml" xref="S2.p6.8.m7.1.1.2">subscript</csymbol><ci id="S2.p6.8.m7.1.1.2.2.2.cmml" xref="S2.p6.8.m7.1.1.2.2.2">𝐖</ci><ci id="S2.p6.8.m7.1.1.2.2.3.cmml" xref="S2.p6.8.m7.1.1.2.2.3">𝐞</ci></apply><ci id="S2.p6.8.m7.1.1.2.3a.cmml" xref="S2.p6.8.m7.1.1.2.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.p6.8.m7.1.1.2.3.cmml" xref="S2.p6.8.m7.1.1.2.3">k-1</mtext></ci></apply><apply id="S2.p6.8.m7.1.1.3.cmml" xref="S2.p6.8.m7.1.1.3"><csymbol cd="ambiguous" id="S2.p6.8.m7.1.1.3.1.cmml" xref="S2.p6.8.m7.1.1.3">superscript</csymbol><ci id="S2.p6.8.m7.1.1.3.2.cmml" xref="S2.p6.8.m7.1.1.3.2">𝐗</ci><ci id="S2.p6.8.m7.1.1.3.3a.cmml" xref="S2.p6.8.m7.1.1.3.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S2.p6.8.m7.1.1.3.3.cmml" xref="S2.p6.8.m7.1.1.3.3">k-1</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.8.m7.1c">\mathbf{W_{e}^{\textit{{k-1}}}X^{\textit{{k-1}}}}</annotation></semantics></math>. At the first layer, <math id="S2.p6.9.m8.1" class="ltx_Math" alttext="\mathbf{X^{1}}=\mathbf{X}" display="inline"><semantics id="S2.p6.9.m8.1a"><mrow id="S2.p6.9.m8.1.1" xref="S2.p6.9.m8.1.1.cmml"><msup id="S2.p6.9.m8.1.1.2" xref="S2.p6.9.m8.1.1.2.cmml"><mi id="S2.p6.9.m8.1.1.2.2" xref="S2.p6.9.m8.1.1.2.2.cmml">𝐗</mi><mn id="S2.p6.9.m8.1.1.2.3" xref="S2.p6.9.m8.1.1.2.3.cmml">𝟏</mn></msup><mo id="S2.p6.9.m8.1.1.1" xref="S2.p6.9.m8.1.1.1.cmml">=</mo><mi id="S2.p6.9.m8.1.1.3" xref="S2.p6.9.m8.1.1.3.cmml">𝐗</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.9.m8.1b"><apply id="S2.p6.9.m8.1.1.cmml" xref="S2.p6.9.m8.1.1"><eq id="S2.p6.9.m8.1.1.1.cmml" xref="S2.p6.9.m8.1.1.1"></eq><apply id="S2.p6.9.m8.1.1.2.cmml" xref="S2.p6.9.m8.1.1.2"><csymbol cd="ambiguous" id="S2.p6.9.m8.1.1.2.1.cmml" xref="S2.p6.9.m8.1.1.2">superscript</csymbol><ci id="S2.p6.9.m8.1.1.2.2.cmml" xref="S2.p6.9.m8.1.1.2.2">𝐗</ci><cn type="integer" id="S2.p6.9.m8.1.1.2.3.cmml" xref="S2.p6.9.m8.1.1.2.3">1</cn></apply><ci id="S2.p6.9.m8.1.1.3.cmml" xref="S2.p6.9.m8.1.1.3">𝐗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.9.m8.1c">\mathbf{X^{1}}=\mathbf{X}</annotation></semantics></math>, that is, the input data. Thus, for a <math id="S2.p6.10.m9.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p6.10.m9.1a"><mi id="S2.p6.10.m9.1.1" xref="S2.p6.10.m9.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p6.10.m9.1b"><ci id="S2.p6.10.m9.1.1.cmml" xref="S2.p6.10.m9.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.10.m9.1c">k</annotation></semantics></math>-layer model, the proposed Class-Encoder aims to learn discriminative representations at each layer by modeling the class label information during the feature learning process.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Ethnicity and Gender Classification using Deep Class-Encoder</h2>

<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1710.02856/assets/x4.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="109" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>ND-GFI Dataset</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1710.02856/assets/x5.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="109" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>ND-Iris-0405 Dataset</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1710.02856/assets/x6.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="109" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Multi-Ethnicity Iris Dataset</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Sample images from the three datasets used for performing gender and ethnicity classification on iris images.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Deep Class-Encoder is used for the task of gender and ethnicity classification of iris images. For both the problems, experiments are performed on two datasets each. Details regarding the algorithm, datasets used, and experimental protocols are given below.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Ethnicity Classification</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.16" class="ltx_p">The proposed <span id="S3.SS1.p1.16.1" class="ltx_text ltx_font_italic">Deep Class-Encoder</span> is used to perform ethnicity classification using Equation <a href="#S2.E5" title="In 2 Proposed Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Here, <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{X}</annotation></semantics></math> is a <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="m\times n" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑚</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">m\times n</annotation></semantics></math> matrix which contains the input images, where <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">m</annotation></semantics></math> is the size of the input image and <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">n</annotation></semantics></math> is the number of input samples. In this case, iris images captured in the NIR spectrum are given as input, resized to <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="48\times 64" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mn id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">48</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><times id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"></times><cn type="integer" id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">48</cn><cn type="integer" id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">48\times 64</annotation></semantics></math>, therefore <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">m</annotation></semantics></math> being equal to 3,072. <math id="S3.SS1.p1.7.m7.2" class="ltx_Math" alttext="\mathbf{W_{d},W_{e}}" display="inline"><semantics id="S3.SS1.p1.7.m7.2a"><mrow id="S3.SS1.p1.7.m7.2.2.2" xref="S3.SS1.p1.7.m7.2.2.3.cmml"><msub id="S3.SS1.p1.7.m7.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.1.1.2" xref="S3.SS1.p1.7.m7.1.1.1.1.2.cmml">𝐖</mi><mi id="S3.SS1.p1.7.m7.1.1.1.1.3" xref="S3.SS1.p1.7.m7.1.1.1.1.3.cmml">𝐝</mi></msub><mo id="S3.SS1.p1.7.m7.2.2.2.3" xref="S3.SS1.p1.7.m7.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.7.m7.2.2.2.2" xref="S3.SS1.p1.7.m7.2.2.2.2.cmml"><mi id="S3.SS1.p1.7.m7.2.2.2.2.2" xref="S3.SS1.p1.7.m7.2.2.2.2.2.cmml">𝐖</mi><mi id="S3.SS1.p1.7.m7.2.2.2.2.3" xref="S3.SS1.p1.7.m7.2.2.2.2.3.cmml">𝐞</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.2b"><list id="S3.SS1.p1.7.m7.2.2.3.cmml" xref="S3.SS1.p1.7.m7.2.2.2"><apply id="S3.SS1.p1.7.m7.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.2">𝐖</ci><ci id="S3.SS1.p1.7.m7.1.1.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.3">𝐝</ci></apply><apply id="S3.SS1.p1.7.m7.2.2.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.2.2.2.2.1.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.7.m7.2.2.2.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2.2">𝐖</ci><ci id="S3.SS1.p1.7.m7.2.2.2.2.3.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2.3">𝐞</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.2c">\mathbf{W_{d},W_{e}}</annotation></semantics></math> are the weight parameters learned at the time of training. <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="\mathbf{C}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">𝐂</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝐂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\mathbf{C}</annotation></semantics></math> is the class label matrix of dimension <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="l\times n" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><mrow id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">l</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.9.m9.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><times id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1"></times><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">𝑙</ci><ci id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">l\times n</annotation></semantics></math>, where <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><mi id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><ci id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">l</annotation></semantics></math> is the number of distinct classes. A two layer Deep Class-Encoder of dimension [<math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="3072" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><mn id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml">3072</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><cn type="integer" id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">3072</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">3072</annotation></semantics></math>, <math id="S3.SS1.p1.12.m12.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S3.SS1.p1.12.m12.1a"><mn id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><cn type="integer" id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">768</annotation></semantics></math>, <math id="S3.SS1.p1.13.m13.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S3.SS1.p1.13.m13.1a"><mn id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><cn type="integer" id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">768</annotation></semantics></math>] is learned. In order to evaluate the efficacy of the proposed model, results and comparison have been shown with two classifiers: Random Decision Forest (RDF) and a two layer Neural Network (NNet) having dimension [<math id="S3.SS1.p1.14.m14.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S3.SS1.p1.14.m14.1a"><mn id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><cn type="integer" id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">768</annotation></semantics></math>, <math id="S3.SS1.p1.15.m15.1" class="ltx_Math" alttext="192" display="inline"><semantics id="S3.SS1.p1.15.m15.1a"><mn id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml">192</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><cn type="integer" id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1">192</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">192</annotation></semantics></math>, <math id="S3.SS1.p1.16.m16.1" class="ltx_Math" alttext="48" display="inline"><semantics id="S3.SS1.p1.16.m16.1a"><mn id="S3.SS1.p1.16.m16.1.1" xref="S3.SS1.p1.16.m16.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m16.1b"><cn type="integer" id="S3.SS1.p1.16.m16.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m16.1c">48</annotation></semantics></math>].</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Datasets and Experimental Protocol</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Two datasets, ND-Iris-0405 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and a combined Multi-Ethnicity Iris Dataset, have been used in this research. 
<br class="ltx_break"><span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">ND-Iris-0405 dataset</span> consists of 64,980 images corresponding to 356 subjects, out of which 158 subjects are Caucasian (White), 82 are Asian, and the remaining 24 belong to other ethnicities. Since the number of samples belonging to <span id="S3.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">other</span> ethnicity are very limited, images corresponding to the Caucasian and Asian subjects have been chosen for evaluation, thereby resulting in a two-class problem (Caucasian or Asian), consisting of 60,259 images. Out of these, 26,272 images (containing equal number of Caucasian and Asian images) are used for training, while the remaining images form the test set.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p"><span id="S3.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Multi-Ethnicity Iris dataset</span> is created by combining three existing datasets, due to the lack of publicly available iris datasets containing images of multiple ethnicities. The dataset consists of images from:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">CASIA-Iris V3 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> collected by the Chinese Academy of Sciences’ Institute of Automation (CASIA): The dataset consists of three parts, CASIA-Iris-Interval, CASIA-Iris-Lamp, and CASIA-Iris-Twin. Out of these, CASIA-Iris Lamp and CASIA-Iris Twin contain 18,197 images of only Chinese ethnicity, that have been used.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">IMP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>: This dataset contains images collected in the visible and NIR spectrum, and night-time mode. Out of these, 630 images of Indian ethnicity captured in the NIR spectrum are used.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">ND-Iris-0405 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>: As mentioned previously, this dataset contains images of subjects belonging to Asian, Caucasian, or other ethnicities. Since Asian might correspond to Chinese or Indian as well, and further categorization has not been provided in the dataset, we only use 41,518 images pertaining to Caucasian subjects in the Multi-Ethnicity Iris dataset.</p>
</div>
</li>
</ul>
<p id="S3.SS1.SSS1.p2.2" class="ltx_p">Therefore, the Multi-Ethnicity Iris dataset consists of 60,310 images of Chinese, Indian, and Caucasian ethnicities. 1,302 images are chosen for creating the training partition, which consists of equal number of images from all three classes (Chinese, Indian, and Caucasian), and the remaining images are used to create the test set.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental protocols for ethnicity and gender classification.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:79.7pt;">
<span id="S3.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1" class="ltx_p"><span id="S3.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:65.4pt;"><span id="S3.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">No. of Training Images</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p" style="width:62.6pt;"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">No. of Testing Images</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_tt" colspan="3">Ethnicity Classification</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:79.7pt;">
<span id="S3.T1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.1.1.1" class="ltx_p">ND-Iris-0405 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></span>
</span>
</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.2.1.1" class="ltx_p" style="width:65.4pt;">26,272</span>
</span>
</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.3.1.1" class="ltx_p" style="width:62.6pt;">33,987</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:79.7pt;">
<span id="S3.T1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.1.1.1" class="ltx_p">Multi-Ethnicity Iris</span>
</span>
</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.2.1.1" class="ltx_p" style="width:65.4pt;">1,302</span>
</span>
</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.3.1.1" class="ltx_p" style="width:62.6pt;">59,008</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" colspan="3">Gender Classification</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:79.7pt;">
<span id="S3.T1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.1.1.1" class="ltx_p">ND-Iris-0405 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></span>
</span>
</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.2.1.1" class="ltx_p" style="width:65.4pt;">42,899</span>
</span>
</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.3.1.1" class="ltx_p" style="width:62.6pt;">22,081</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:79.7pt;">
<span id="S3.T1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.1.1.1" class="ltx_p">ND-GFI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span>
</span>
</td>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.2.1.1" class="ltx_p" style="width:65.4pt;">2,399</span>
</span>
</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.3.1.1" class="ltx_p" style="width:62.6pt;">600</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.8.8.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:79.7pt;">
<span id="S3.T1.1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.1.1.1" class="ltx_p">UND_V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span>
</span>
</td>
<td id="S3.T1.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.2.1.1" class="ltx_p" style="width:65.4pt;">-</span>
</span>
</td>
<td id="S3.T1.1.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.1.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.3.1.1" class="ltx_p" style="width:62.6pt;">1,944</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Gender Classification</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.6" class="ltx_p">Deep Class-Encoder is also used for performing gender classification on iris images. Similar to the previous experiments, the images are down-sampled to <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="48\times 64" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">48</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">48</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">48\times 64</annotation></semantics></math>. Thus, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathbf{X}</annotation></semantics></math> is of dimension <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="3072\times n" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mn id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">3072</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><times id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">3072</cn><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">3072\times n</annotation></semantics></math>, where <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">n</annotation></semantics></math> is the number of input images. <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{C}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">𝐂</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝐂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">\mathbf{C}</annotation></semantics></math> is a <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="2\times n" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mrow id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mn id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.6.m6.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><times id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1"></times><cn type="integer" id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">2</cn><ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">2\times n</annotation></semantics></math> binary-matrix containing the class labels. The architecture of the feature extractors and classifiers is kept consistent with that described in Section <a href="#S3.SS1" title="3.1 Ethnicity Classification ‣ 3 Ethnicity and Gender Classification using Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Datasets and Experimental Protocol</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Experiments have been performed on two datasets: ND-Iris-0405 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and ND-Gender From Iris (GFI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. 
<br class="ltx_break"><span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">ND-Iris-0405 dataset</span> consists of 64,980 images corresponding to 158 females and 198 males. 70% data of each class is used for training (resulting in 42,899 images), while the remaining form the test set. 
<br class="ltx_break"><span id="S3.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_bold">ND-Gender From Iris (ND-GFI) dataset</span> consists of 3,000 images from 750 males and 750 females. The dataset consists of a pre-defined protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, where 80% of the data (per class) is used for training, and the remaining 20% is used for testing. A separate subject-disjoint set (UND_V) containing 1,944 images has also been provided for evaluating the performance of the trained model. The same protocol is followed for the evaluation of the proposed model. All protocols ensure mutually exclusive training and testing sets, such that there is no image which occurs in both the partitions. Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Ethnicity and Gender Classification using Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents sample images from all three datasets, and Table <a href="#S3.T1" title="Table 1 ‣ 3.1.1 Datasets and Experimental Protocol ‣ 3.1 Ethnicity Classification ‣ 3 Ethnicity and Gender Classification using Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the protocols followed for the experimental evaluation.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ethnicity classification accuracy (%) on the ND-Iris-0405 dataset with two classifiers: Random Decision Forest (RDF) and Neural Network (NNet).</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Algorithm</span></td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">RDF</span></span>
</span>
</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T2.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">NNet</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">Stacked Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T2.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.2.1.1" class="ltx_p" style="width:28.5pt;">87.24</span>
</span>
</td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S3.T2.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.3.1.1" class="ltx_p" style="width:28.5pt;">80.05</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<td id="S3.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Stacked Denoising Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.2.1.1" class="ltx_p" style="width:28.5pt;">85.20</span>
</span>
</td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.3.1.1" class="ltx_p" style="width:28.5pt;">64.51</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<td id="S3.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep Belief Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.2.1.1" class="ltx_p" style="width:28.5pt;">85.12</span>
</span>
</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.3.1.1" class="ltx_p" style="width:28.5pt;">87.43</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<td id="S3.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Discriminative RBM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t" colspan="2">90.33</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<td id="S3.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.1.6.6.1.1" class="ltx_text ltx_font_bold">Proposed Deep Class-Encoder</span></td>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T2.1.6.6.2.1.1.1" class="ltx_text ltx_font_bold">89.35</span></span>
</span>
</td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S3.T2.1.6.6.3.1.1.1" class="ltx_text ltx_font_bold">94.33</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results and Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The proposed model has been compared with existing Deep Learning models for performing gender and ethnicity classification on iris images. Comparison has been performed with Stacked Autoencoder (SAE), Stacked Denoising Autoencoder (SDAE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, Deep Belief Network (DBN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, Discriminative Restricted Boltzmann Machine (DRBM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, i.e. a Convolutional Neural Network based model. To be consistent with the proposed model, feature extraction models (SAE, SDAE, DBN) have the same architecture as that of the proposed Deep Class-Encoder (described in Section <a href="#S3.SS1" title="3.1 Ethnicity Classification ‣ 3 Ethnicity and Gender Classification using Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). This is followed by learning a classifier (RDF or NNet) for classification. Owing to the class imbalance in the test samples, mean class-wise accuracy has been reported throughout this paper. Since no existing Commercial-Off-The-Shelf system predicts the attributes of gender and ethnicity for iris images, a comparison could not have been drawn for the same. Task specific analysis for the proposed Deep Class-Encoder model is given below.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ethnicity classification accuracy (%) on the Multi-Ethnicity Iris dataset with two classifiers: RDF and NNet.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Algorithm</span></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">RDF</span></span>
</span>
</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">NNet</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<td id="S4.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">Stacked Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T3.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S4.T3.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.2.2.1.1" class="ltx_p" style="width:28.5pt;">95.76</span>
</span>
</td>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S4.T3.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.2.3.1.1" class="ltx_p" style="width:28.5pt;">90.42</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<td id="S4.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Stacked Denoising Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T3.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.3.2.1.1" class="ltx_p" style="width:28.5pt;">95.13</span>
</span>
</td>
<td id="S4.T3.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.3.3.1.1" class="ltx_p" style="width:28.5pt;">87.3</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<td id="S4.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep Belief Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T3.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.4.2.1.1" class="ltx_p" style="width:28.5pt;">96.97</span>
</span>
</td>
<td id="S4.T3.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.4.3.1.1" class="ltx_p" style="width:28.5pt;">95.22</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.5.5" class="ltx_tr">
<td id="S4.T3.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Discriminative RBM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S4.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t" colspan="2">95.92</td>
</tr>
<tr id="S4.T3.1.6.6" class="ltx_tr">
<td id="S4.T3.1.6.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.6.6.1.1" class="ltx_text ltx_font_bold">Proposed Deep Class-Encoder</span></td>
<td id="S4.T3.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T3.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.6.6.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.6.6.2.1.1.1" class="ltx_text ltx_font_bold">97.22</span></span>
</span>
</td>
<td id="S4.T3.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T3.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.6.6.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.6.6.3.1.1.1" class="ltx_text ltx_font_bold">97.38</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Ethnicity Classification</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Tables <a href="#S3.T2" title="Table 2 ‣ 3.2.1 Datasets and Experimental Protocol ‣ 3.2 Gender Classification ‣ 3 Ethnicity and Gender Classification using Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.T3" title="Table 3 ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> present the accuracy for ethnicity classification on ND-Iris-0405 and Multi-Ethnicity Iris dataset, respectively. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Ethnicity Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> also presents the Receiver Operative Characteristic (ROC) curve obtained on the ND-Iris-0405 dataset for ethnicity classification of Asians versus Caucasians. On the ND-Iris-0405 dataset, the proposed model achieves a classification accuracy of 89.35% and 94.35% with RDF and NNet classifier, respectively (Table <a href="#S3.T2" title="Table 2 ‣ 3.2.1 Datasets and Experimental Protocol ‣ 3.2 Gender Classification ‣ 3 Ethnicity and Gender Classification using Deep Class-Encoder ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Compared with other unsupervised feature learning models (SAE, SDAE, and DBM), Deep Class-Encoder presents an improvement of at least 7%. Direct comparison with SAE (difference of around 14% in accuracy) shows the benefit of encoding class labels in the proposed model. Comparison with Discriminative RBM, which is a supervised Deep Learning architecture, also presents an improvement of 4%. The key difference between Discriminative RBM and proposed Deep Class-Encoder is that the former models the joint probability of the sample and the label, while the later tries to learn representations such that under a fixed learned mapping, the hidden representations map to the class labels. It can thus be observed that the introduction of the mapping matrix facilitates learning of discriminative features. Table <a href="#S4.T4" title="Table 4 ‣ 4.1 Ethnicity Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives the confusion matrix obtained with the proposed Deep Class-Encoder and Neural Network. The low mis-classification rates for both the classes suggest that the proposed model is able to encode meaningful and discriminative representations across classes.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1710.02856/assets/x7.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="182" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Ethnicity classification on the ND-Iris-0405 dataset. For all models, the best performance between RDF and NNet is plotted.</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Confusion matrix of Deep Class-Encoder on the ND-Iris-0405 dataset for ethnicity classification.</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="width:8.5pt;"></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Predicted</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" style="width:8.5pt;">
<span id="S4.T4.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.2.1.1.1.1" class="ltx_p">
<span id="S4.T4.1.2.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:29.2pt;vertical-align:-11.1pt;"><span class="ltx_transformed_inner" style="width:29.2pt;transform:translate(-11.11pt,0pt) rotate(-90deg) ;">
<span id="S4.T4.1.2.1.1.1.1.1.1" class="ltx_p"><span id="S4.T4.1.2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Actual</span></span>
</span></span></span>
</span>
</th>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:45.5pt;"></td>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:45.5pt;">
<span id="S4.T4.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.2.1.3.1.1" class="ltx_p">Asian</span>
</span>
</td>
<td id="S4.T4.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_tt" style="width:45.5pt;">
<span id="S4.T4.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.2.1.4.1.1" class="ltx_p">Caucasian</span>
</span>
</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="width:8.5pt;"></th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:45.5pt;">
<span id="S4.T4.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.3.2.2.1.1" class="ltx_p">Asian</span>
</span>
</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:45.5pt;">
<span id="S4.T4.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.3.2.3.1.1" class="ltx_p">93.78%</span>
</span>
</td>
<td id="S4.T4.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:45.5pt;">
<span id="S4.T4.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.3.2.4.1.1" class="ltx_p">6.22%</span>
</span>
</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<th id="S4.T4.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr" style="width:8.5pt;"></th>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:45.5pt;">
<span id="S4.T4.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.4.3.2.1.1" class="ltx_p">Caucasian</span>
</span>
</td>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:45.5pt;">
<span id="S4.T4.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.4.3.3.1.1" class="ltx_p">5.12%</span>
</span>
</td>
<td id="S4.T4.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:45.5pt;">
<span id="S4.T4.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.4.3.4.1.1" class="ltx_p">94.88%</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Similar results can be observed from Table <a href="#S4.T3" title="Table 3 ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for ethnicity classification on Multi-Ethnicity Iris dataset. In this case, the classifier aims to classify the images as either Caucasian, Chinese, or Indian. The proposed model achieves a classification accuracy of 97.38% (with Neural Network), which shows improvement over other comparative feature extraction models. Moreover, the variation in classification performance obtained with RDF and NNet for Deep Class-Encoder in within 1%, whereas the difference is as high as 8% for other comparative models (SDAE). This observation further instantiates that the proposed model learns robust features which are independent of the classifier used in the pipeline. Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 Ethnicity Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> gives the bar graph with classification accuracies of all models for all three classes. The proposed model achieves at least 96% classification accuracy for all three classes, thus promoting the feature learning process of Deep Class-Encoder. Interestingly, the model mis-classifies only <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">one</span> sample of Indian ethnicity. As shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.1 Ethnicity Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, overall illumination and size variations along with the presence of artifacts such as hair bangs render samples challenging for ethnicity classification.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1710.02856/assets/x8.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="246" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Classification performance (%) of all algorithms on all three classes for the Multi-Ethnicity Iris dataset.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1710.02856/assets/x9.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="246" height="61" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Sample images mis-classified by the proposed Deep Class-Encoder and Neural Network model for the Multi-Ethnicity Iris dataset. Artifacts such as hair (bangs) and incomplete iris information result in mis-classification.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1710.02856/assets/x10.png" id="S4.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="182" height="119" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>ND-Iris-0405 Dataset</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1710.02856/assets/x11.png" id="S4.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="182" height="118" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>ND-GFI Dataset</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>ROC curves for gender classification. For each model, the best performing classifier (RDF or NNet) is plotted.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">It is important to note that no direct comparison could be drawn with existing work on ethnicity classification, since there is no fixed protocol or datasets which are used for evaluation. While there exists works utilizing the ND-Iris-0405 dataset, however, experiments are performed on a very small subset of the dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Gender Classification</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Tables <a href="#S4.T5" title="Table 5 ‣ 4.2 Gender Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S4.T6" title="Table 6 ‣ 4.2 Gender Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> present the gender classification accuracy on ND-Iris-0405, and ND-GFI dataset, respectively. Figure <a href="#S4.F7" title="Figure 7 ‣ 4.1 Ethnicity Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> also presents the ROC curves obtained on both the datasets. On the ND-Iris-0405 dataset, the proposed Deep Class-Encoder achieves a classification accuracy of 82.53% (with Neural Network), which is at least 5.5% better than other comparative models. Figure <a href="#S4.F9" title="Figure 9 ‣ 4.2 Gender Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> presents sample images mis-classified by the proposed model. It can be observed that iris images having poor illumination or partial information serve as challenging samples.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Experiments are also performed on the ND-GFI dataset, where, the Deep Class-Encoder achieved an accuracy of 83.17%, showing at least 5% improvement over other deep learning based comparative models. In literature, Tapai <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> achieve an accuracy of 84.83% on the same protocol, which means that the proposed model performs 10 extra mis-classifications. The proposed Deep Class-Encoder achieves a classification accuracy of 79.25% on the UND_V dataset. This dataset is provided with the ND-GFI dataset and models the real world scenario of disjoint subjects in the training and testing set. The protocol mentioned by the authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> has been followed for evaluation, using which they report a classification accuracy of 77.5% (state-of-the-art for this dataset). This results in a difference of 34 correctly classified samples, thereby promoting the generalization abilities of Deep Class-Encoder.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Gender classification accuracy (%) on the ND-Iris 0405 dataset with two classifiers: RDF and NNet.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Algorithm</span></td>
<td id="S4.T5.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.1.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T5.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">RDF</span></span>
</span>
</td>
<td id="S4.T5.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.1.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T5.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">NNet</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.1.2.2" class="ltx_tr">
<td id="S4.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">Stacked Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T5.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S4.T5.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.2.2.1.1" class="ltx_p" style="width:28.5pt;">74.73</span>
</span>
</td>
<td id="S4.T5.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S4.T5.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.2.3.1.1" class="ltx_p" style="width:28.5pt;">66.71</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.3.3" class="ltx_tr">
<td id="S4.T5.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Stacked Denoising Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T5.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.3.2.1.1" class="ltx_p" style="width:28.5pt;">69.23</span>
</span>
</td>
<td id="S4.T5.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.3.3.1.1" class="ltx_p" style="width:28.5pt;">56.07</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.4.4" class="ltx_tr">
<td id="S4.T5.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep Belief Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T5.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.4.4.2.1.1" class="ltx_p" style="width:28.5pt;">75.74</span>
</span>
</td>
<td id="S4.T5.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.4.4.3.1.1" class="ltx_p" style="width:28.5pt;">76.03</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.5.5" class="ltx_tr">
<td id="S4.T5.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Discriminative RBM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S4.T5.1.5.5.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t" colspan="2">70.76</td>
</tr>
<tr id="S4.T5.1.6.6" class="ltx_tr">
<td id="S4.T5.1.6.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T5.1.6.6.1.1" class="ltx_text ltx_font_bold">Proposed Deep Class-Encoder</span></td>
<td id="S4.T5.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.6.6.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T5.1.6.6.2.1.1.1" class="ltx_text ltx_font_bold">80.06</span></span>
</span>
</td>
<td id="S4.T5.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.6.6.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T5.1.6.6.3.1.1.1" class="ltx_text ltx_font_bold">82.53</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/1710.02856/assets/x12.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="115" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Bar graph presenting the gender classification accuracy obtained on the UND_V dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. </figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Figure <a href="#S4.F8" title="Figure 8 ‣ 4.2 Gender Classification ‣ 4 Experimental Results and Analysis ‣ Gender and Ethnicity Classification of Iris Images using Deep Class-Encoder" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> presents a bar graph with the classification accuracies of a CNN based feature extractor, AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Tapia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, and the proposed model. Comparison with Convolutional Neural Network based feature extractor, AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> on the UND_V dataset further promotes the utility of the proposed model. Upon using the pre-trained model for feature extraction only, the proposed model achieves at least 16% improved performance. A major advantage of the proposed Deep Class-Encoder is in terms of the training time. As compared to Stacked Autoencoders, Deep Class-Encoder takes one fourth the total training time. This computational advantage is primarily due to the Majorization-Minimization and Alternating Direction Method of Multipliers based optimization which does not involve computing derivatives at each epoch.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Gender classification accuracy (%) on the ND-GFI dataset with two classifiers: RDF and NNet.</figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Algorithm</span></td>
<td id="S4.T6.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.1.1.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T6.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">RDF</span></span>
</span>
</td>
<td id="S4.T6.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.1.1.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T6.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">NNet</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.1.2.2" class="ltx_tr">
<td id="S4.T6.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">Stacked Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T6.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S4.T6.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.2.2.2.1.1" class="ltx_p" style="width:28.5pt;">64.17</span>
</span>
</td>
<td id="S4.T6.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S4.T6.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.2.2.3.1.1" class="ltx_p" style="width:28.5pt;">65.33</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.3.3" class="ltx_tr">
<td id="S4.T6.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Stacked Denoising Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T6.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.3.3.2.1.1" class="ltx_p" style="width:28.5pt;">71.33</span>
</span>
</td>
<td id="S4.T6.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.3.3.3.1.1" class="ltx_p" style="width:28.5pt;">66.00</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.4.4" class="ltx_tr">
<td id="S4.T6.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep Belief Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T6.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.4.4.2.1.1" class="ltx_p" style="width:28.5pt;">67.17</span>
</span>
</td>
<td id="S4.T6.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T6.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.4.4.3.1.1" class="ltx_p" style="width:28.5pt;">71.83</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.5.5" class="ltx_tr">
<td id="S4.T6.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Discriminative RBM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S4.T6.1.5.5.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t" colspan="2">78.67</td>
</tr>
<tr id="S4.T6.1.6.6" class="ltx_tr">
<td id="S4.T6.1.6.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T6.1.6.6.1.1" class="ltx_text ltx_font_bold">Proposed Deep Class-Encoder</span></td>
<td id="S4.T6.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T6.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.6.6.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T6.1.6.6.2.1.1.1" class="ltx_text ltx_font_bold">78.17</span></span>
</span>
</td>
<td id="S4.T6.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T6.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.6.6.3.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T6.1.6.6.3.1.1.1" class="ltx_text ltx_font_bold">83.17</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/1710.02856/assets/x13.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="231" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Sample mis-classifications by the proposed model. It can be observed that incomplete, poorly illuminated, and hidden iris tend to be more difficult to classify.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This research addresses the task of gender and ethnicity classification of iris images. A supervised autoencoder model, termed as Deep Class-Encoder has been proposed for the given task. Deep Class-Encoder utilizes the class labels at the time of feature learning, in order to learn discriminative features. The efficacy of the proposed model is evaluated on two datasets each, for gender and ethnicity classification. Experimental evaluation and results further promote the utility of the proposed model for learning class-specific discriminative features.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgment</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This research is partially supported by MEITY (Government of India), India. S. Nagpal is partially supported through TCS PhD fellowship. M. Vatsa, R. Singh, and A. Majumdar are partially supported through Infosys Center for Artificial Intelligence. We also thank NVIDIA Corp. for Tesla K40 GPU for research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
CASIA iris image database.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:90%;">http://biometrics.idealtest.org/</span><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Y. Bengio, A. Courville, and P. Vincent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Representation learning: A review and new perspectives.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">,
35(8):1798–1828, 2013.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Distributed optimization and statistical learning via the alternating
direction method of multipliers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Foundations and Trends in Machine Learning</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 3(1):1–122, 2011.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
A. Dantcheva, P. Elia, and A. Ross.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">What else does your biometric data reveal? a survey on soft
biometrics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">,
11(3):441–467, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
M. Figueiredo, J. Bioucas-Dias, and R. Nowak.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Majorization-minimization algorithms for wavelet-based image
restoration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 16(12):2980–2991, 2007.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
S. Gao, Y. Zhang, K. Jia, J. Lu, and Y. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Single sample face recognition via learning deep supervised
autoencoders.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">,
10(10):2108–2118, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
G. E. Hinton, S. Osindero, and Y.-W. Teh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">A fast learning algorithm for deep belief nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural computation</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 18(7):1527–1554, 2006.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
G. E. Hinton and R. R. Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Reducing the dimensionality of data with neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Science</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 313(5786):504–507, 2006.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
A. Krizhevsky, I. Sutskever, and G. E. Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">ImageNet classification with deep convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages
1097–1105. 2012.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
S. Lagree and K. W. Bowyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Ethnicity prediction based on iris texture features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Midwest Artificial Intelligence and Cognitive Science</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages
225–230, 2011.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
S. Lagree and K. W. Bowyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Predicting ethnicity and gender from iris texture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Technologies for Homeland
Security</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 440–445, 2011.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
H. Larochelle and Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Classification using discriminative restricted boltzmann machines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages
536–543, 2008.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Y. Lecun, Y. Bengio, and G. Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 521(7553):436–444, 5 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
A. Majumdar, R. Singh, and M. Vatsa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Face verification via class sparsity based supervised encoding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">,
39(6):1273–1280, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
I. Nigam, M. Vatsa, and R. Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Ocular biometrics: A survey of modalities and fusion approaches.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Information Fusion</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 26:1–35, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
U. Park and A. K. Jain.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Face matching and retrieval using soft biometrics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">,
5(3):406–415, 2010.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
P. J. Phillips, W. T. Scruggs, A. J. O’Toole, P. J. Flynn, K. W. Bowyer, C. L.
Schott, and M. Sharpe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">FRVT 2006 and ICE 2006 large-scale experimental results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">,
32(5):831–846, 2010.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
X. Qiu, Z. Sun, and T. Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Global texture analysis of iris images for ethnic classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Biometrics</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 411–418,
2005.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
X. Qiu, Z. Sun, and T. Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Learning appearance primitives of iris images for ethnic
classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Image Processing</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, volume 2,
pages II – 405–II – 408, 2007.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
A. Sharma, S. Verma, M. Vatsa, and R. Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">On cross spectral periocular recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Image Processing</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages
5007–5011, 2014.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
J. E. Tapia, C. A. Perez, and K. W. Bowyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Gender Classification from Iris Images Using Fusion of Uniform
Local Binary Patterns</span><span id="bib.bib21.3.2" class="ltx_text" style="font-size:90%;">, pages 751–763.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
J. E. Tapia, C. A. Perez, and K. W. Bowyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Gender classification from the same iris code used for recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">,
11(8):1760–1770, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
V. Thomas, N. V. Chawla, K. W. Bowyer, and P. J. Flynn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Learning to predict gender from iris images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Biometrics: Theory,
Applications, and Systems</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 1–5, 2007.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Stacked denoising autoencoders: Learning useful representations in a
deep network with a local denoising criterion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 11:3371–3408, 2010.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
A. Zarei and D. Mou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Artificial neural network for prediction of ethnicity based on iris
texture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning and
Applications</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, volume 1, pages 514–519, 2012.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
X. Zheng, Z. Wu, H. Meng, and L. Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Contrastive auto-encoder for phoneme recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2014 IEEE International Conference on Acoustics, Speech and
Signal Processing</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 2529–2533, 2014.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1710.02855" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/land_of_honey_and_milk" rel="nofollow" aria-hidden="true" tabindex="-1"></a>
    <a href="/log/1710.02856" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1710.02856">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1710.02856" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1710.02857" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 04:09:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
