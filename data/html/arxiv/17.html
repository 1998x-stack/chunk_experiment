<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1710.02410] End-to-end Driving via Conditional Imitation Learning</title><meta property="og:description" content="Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained eâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="End-to-end Driving via Conditional Imitation Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="End-to-end Driving via Conditional Imitation Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1710.02410">
<link rel="canonical" target="_blank" href="https://ar5iv.labs.arxiv.org/html/1710.02410">

<!--Generated on Sun Mar  3 03:26:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">End-to-end Driving via Conditional Imitation Learning
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felipe Codevilla<sup id="6.2.1" class="ltx_sup"><span id="6.2.1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matthias MÃ¼ller<sup id="7.2.1" class="ltx_sup"><span id="7.2.1.1" class="ltx_text ltx_font_italic">1,3</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Antonio LÃ³pez<sup id="8.2.1" class="ltx_sup">2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vladlen Koltun<sup id="9.2.1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexey Dosovitskiy<sup id="10.2.1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="11.1" class="ltx_p">Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands.</p>
</div>
<div id="p2" class="ltx_para ltx_align_center">
<table id="p2.6" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="p2.3.3" class="ltx_tr">
<td id="p2.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/Map3D_PhysicalSystem.jpg" id="p2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="182" height="150" alt="[Uncaptioned image]"></td>
<td id="p2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/FPV_PhysicalSystem.jpg" id="p2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="310" height="150" alt="[Uncaptioned image]"></td>
<td id="p2.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/Turn_PhysicalSystem.jpg" id="p2.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="191" height="150" alt="[Uncaptioned image]"></td>
</tr>
<tr id="p2.6.6" class="ltx_tr">
<td id="p2.4.4.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/Map3D_Carla.jpg" id="p2.4.4.1.g1" class="ltx_graphics ltx_img_square" width="182" height="150" alt="[Uncaptioned image]"></td>
<td id="p2.5.5.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/FPV_Carla.jpg" id="p2.5.5.2.g1" class="ltx_graphics ltx_img_landscape" width="310" height="150" alt="[Uncaptioned image]"></td>
<td id="p2.6.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/Turn_Carla.jpg" id="p2.6.6.3.g1" class="ltx_graphics ltx_img_landscape" width="191" height="150" alt="[Uncaptioned image]"></td>
</tr>
<tr id="p2.6.7.1" class="ltx_tr">
<td id="p2.6.7.1.1" class="ltx_td ltx_align_center"><span id="p2.6.7.1.1.1" class="ltx_text" style="font-size:90%;">(a) Aerial view of test environment</span></td>
<td id="p2.6.7.1.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="p2.6.7.1.2.1" class="ltx_text" style="font-size:90%;">(b) Vision-based driving, view from onboard camera</span></td>
<td id="p2.6.7.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="p2.6.7.1.3.1" class="ltx_text" style="font-size:90%;">(c) Side view of vehicle</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_para">Figure 1: </span>Conditional imitation learning allows an autonomous vehicle trained end-to-end to be directed by high-level commands. (a) We train and evaluate robotic vehicles in the physical world (top) and in simulated urban environments (bottom). (b) The vehicles drive based on video from a forward-facing onboard camera. At the time these images were taken, the vehicle was given the command â€œturn right at the next intersectionâ€. (c) The trained controller handles sensorimotor coordination (staying on the road, avoiding collisions) and follows the provided commands.</figcaption>
</div>
<section id="S1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Imitation learning is receiving renewed interest as a promising approach to training autonomous driving systems.Â 
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><sup id="footnote1.1" class="ltx_sup">1</sup>Intel Labs</span></span></span>
<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><sup id="footnote2.1" class="ltx_sup">2</sup>Computer Vision Center &amp; Univ. Autonoma de Barcelona</span></span></span>
<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><sup id="footnote3.1" class="ltx_sup">3</sup>King Abdullah University of Science &amp; Technology</span></span></span>
Demonstrations of human driving are easy to collect at scale. Given such demonstrations, imitation learning can be used to train a model that maps perceptual inputs to control commands; for example, mapping camera images to steering and acceleration. This approach has been applied to lane followingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and off-road obstacle avoidanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. However, these systems have characteristic limitations. For example, the network trained by Bojarski et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> was given control over lane and road following only. When a lane change or a turn from one road to another were required, the human driver had to take controlÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Why has imitation learning not scaled up to fully autonomous urban driving? One limitation is in the assumption that the optimal action can be inferred from the perceptual input alone.
This assumption often does not hold in practice: for instance, when a car approaches an intersection, the camera input is not sufficient to predict whether the car should turn left, right, or go straight.
Mathematically, the mapping from the image to the control command is no longer a function. Fitting a function approximator is thus bound to run into difficulties. This had already been observed in the work of Pomerleau: â€œCurrently upon reaching a fork, the network may output two widely discrepant travel directions, one for each choice. The result is often an oscillation in the dictated travel directionâ€Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Even if the network can resolve the ambiguity in favor of some course of action, it may not be the one desired by the passenger, who lacks a communication channel for controlling the network itself.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we address this challenge with conditional imitation learning. At training time, the model is given not only the perceptual input and the control signal, but also a representation of the expertâ€™s intention.
At test time, the network can be given corresponding commands, which resolve the ambiguity in the perceptuomotor mapping and allow the trained model to be controlled by a passenger or a topological planner, just as mapping applications and passengers provide turn-by-turn directions to human drivers. The trained network is thus freed from the task of planning and can devote its representational capacity to driving. This enables scaling imitation learning to vision-based driving in complex urban environments.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We evaluate the presented approach in realistic simulations of urban driving and on a 1/5 scale robotic truck. Both systems are shown in FigureÂ <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">End-to-end Driving via Conditional Imitation Learning</span></span>.
Simulation allows us to thoroughly analyze the importance of different modeling decisions, carefully compare the approach to relevant baselines, and conduct detailed ablation studies. Experiments with the physical system demonstrate that the approach can be successfully deployed in the physical world. Recordings of both systems are provided in the supplementary video.</p>
</div>
</section>
<section id="S2" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Imitation learning has been applied to a variety of tasks, including articulated motionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, autonomous flightÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, modeling navigational behaviorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, off-road drivingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and road followingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Technically, these applications differ in the input representation (raw sensory input or hand-crafted features), the control signal being predicted, the learning algorithms, and the learned representations. Most relevant to our work are the systems of PomerleauÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, LeCun et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, and Bojarski et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, who used ground vehicles and trained deep networks to predict the driverâ€™s actions from camera input. These studies focused on purely reactive tasks, such as lane following or obstacle avoidance. In comparison, we develop a command-conditional formulation that enables the application of imitation learning to more complex urban driving. Another difference is that we learn to control not only steering but also acceleration and braking, enabling the model to assume full control of the car.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The decomposition of complex tasks into simpler sub-tasks has been studied from several perspectives. In robotics, movement primitives have been used as building blocks for advanced motor skillsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Movement primitives represent a simple motion, such as a strike or a throw, by a parameterized dynamical system. In comparison, the policies we consider have much richer parameterizations and address more complex sensorimotor tasks that couple perception and control, such as finding the next opportunity to turn right and then making the turn while avoiding dynamic obstacles.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In reinforcement learning, hierarchical approaches aim to construct multiple levels of temporally extended sub-policiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The options framework is a prominent example of such hierarchical decompositionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Basic motor skills that are learned in this framework can be transferred across tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Hierarchical approaches have also been combined with deep learning and applied to raw sensory inputÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In these works, the main aim is to learn purely from experience and discover hierarchical structure automatically. This is hard and is in general an open problem, particularly for sensorimotor skills with the complexity we consider. In contrast, we focus on imitation learning, and we provide additional information on the expertâ€™s intentions during demonstration. This formulation makes the learning problem more tractable and yields a human-controllable policy.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Adjacent to hierarchical methods is the idea of learning multi-purpose and parameterized controllers. Parameterized goals have been used to train motion controllers in roboticsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Schaul et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> proposed a general framework for reinforcement learning with parameterized value functions, shared across states and goals. Dosovitskiy and KoltunÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> studied families of parameterized goals in the context of navigation in three-dimensional environments. Javdani et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> studied a scenario where a robot assists a human and changes its behavior depending on its estimate of the humanâ€™s goal. Our work shares the idea of training a conditional controller, but differs in the model architecture, the application domain (vision-based autonomous driving), and the learning method (conditional imitation learning).</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Autonomous driving is the subject of intensive researchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Broadly speaking, approaches differ in their level of modularity. On one side are highly tuned systems that deploy an array of computer vision algorithms to create a model of the environment, which is then used for planning and controlÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. On the opposite side are end-to-end approaches that train function approximators to map sensory input to control commandsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Our approach is on the end-to-end side of the spectrum, but in addition to sensory input the controller is provided with commands that specify the driverâ€™s intent. This resolves some of the ambiguity in the perceptuomotor mapping and creates a communication channel that can be used to guide the autonomous car as one would guide a chauffeur.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Human guidance of robot actions has been studied extensivelyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. These works tackle the challenging problem of parsing natural language instructions. Our work does not address natural language communication; we limit commands to a predefined vocabulary such as â€œturn right at the next intersectionâ€, â€œturn left at the next intersectionâ€, and â€œkeep straightâ€.
On the other hand, our work deals with end-to-end vision-based driving using deep networks. Systems in this domain have been limited to imitating the expert without the ability to naturally accept commands after deploymentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. We introduce such ability into deep networks for end-to-end vision-based driving.</p>
</div>
</section>
<section id="S3" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Conditional Imitation Learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.6" class="ltx_p">We begin by describing the standard imitation learning setup and then proceed to our command-conditional formulation. Consider a controller that interacts with the environment over discrete time steps. At each time step <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">t</annotation></semantics></math>, the controller receives an observation <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{o}_{t}" display="inline"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">ğ¨</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğ¨</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathbf{o}_{t}</annotation></semantics></math> and takes an action <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{a}_{t}" display="inline"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">ğš</mi><mi id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ğš</ci><ci id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathbf{a}_{t}</annotation></semantics></math>.
The basic idea behind imitation learning is to train a controller that mimics an expert. The training data is a set of observation-action pairs <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{D}=\{\left\langle\mathbf{o}_{i},\mathbf{a}_{i}\right\rangle\}_{i=1}^{N}" display="inline"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">ğ’Ÿ</mi><mo id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">=</mo><msubsup id="S3.p1.4.m4.1.1.1" xref="S3.p1.4.m4.1.1.1.cmml"><mrow id="S3.p1.4.m4.1.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.p1.4.m4.1.1.1.1.1.1.2" xref="S3.p1.4.m4.1.1.1.1.1.2.cmml">{</mo><mrow id="S3.p1.4.m4.1.1.1.1.1.1.1.2" xref="S3.p1.4.m4.1.1.1.1.1.1.1.3.cmml"><mo id="S3.p1.4.m4.1.1.1.1.1.1.1.2.3" xref="S3.p1.4.m4.1.1.1.1.1.1.1.3.cmml">âŸ¨</mo><msub id="S3.p1.4.m4.1.1.1.1.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.2" xref="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.2.cmml">ğ¨</mi><mi id="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.3" xref="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p1.4.m4.1.1.1.1.1.1.1.2.4" xref="S3.p1.4.m4.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.p1.4.m4.1.1.1.1.1.1.1.2.2" xref="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.2" xref="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.2.cmml">ğš</mi><mi id="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.3" xref="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.p1.4.m4.1.1.1.1.1.1.1.2.5" xref="S3.p1.4.m4.1.1.1.1.1.1.1.3.cmml">âŸ©</mo></mrow><mo stretchy="false" id="S3.p1.4.m4.1.1.1.1.1.1.3" xref="S3.p1.4.m4.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p1.4.m4.1.1.1.1.3" xref="S3.p1.4.m4.1.1.1.1.3.cmml"><mi id="S3.p1.4.m4.1.1.1.1.3.2" xref="S3.p1.4.m4.1.1.1.1.3.2.cmml">i</mi><mo id="S3.p1.4.m4.1.1.1.1.3.1" xref="S3.p1.4.m4.1.1.1.1.3.1.cmml">=</mo><mn id="S3.p1.4.m4.1.1.1.1.3.3" xref="S3.p1.4.m4.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p1.4.m4.1.1.1.3" xref="S3.p1.4.m4.1.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><eq id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2"></eq><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">ğ’Ÿ</ci><apply id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1">superscript</csymbol><apply id="S3.p1.4.m4.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1">subscript</csymbol><set id="S3.p1.4.m4.1.1.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1"><list id="S3.p1.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.2"><apply id="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.2">ğ¨</ci><ci id="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.2">ğš</ci><ci id="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply></list></set><apply id="S3.p1.4.m4.1.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.1.3"><eq id="S3.p1.4.m4.1.1.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.1.1.3.1"></eq><ci id="S3.p1.4.m4.1.1.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.p1.4.m4.1.1.1.1.3.3.cmml" xref="S3.p1.4.m4.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p1.4.m4.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathcal{D}=\{\left\langle\mathbf{o}_{i},\mathbf{a}_{i}\right\rangle\}_{i=1}^{N}</annotation></semantics></math> generated by the expert. The assumption is that the expert is successful at performing the task of interest and that a controller trained to mimic the expert will also perform the task well. This is a supervised learning problem, in which the parameters <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="\bm{\theta}" display="inline"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">ğœ½</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ğœ½</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\bm{\theta}</annotation></semantics></math> of a function approximator <math id="S3.p1.6.m6.2" class="ltx_Math" alttext="F(\mathbf{o};\bm{\theta})" display="inline"><semantics id="S3.p1.6.m6.2a"><mrow id="S3.p1.6.m6.2.3" xref="S3.p1.6.m6.2.3.cmml"><mi id="S3.p1.6.m6.2.3.2" xref="S3.p1.6.m6.2.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.p1.6.m6.2.3.1" xref="S3.p1.6.m6.2.3.1.cmml">â€‹</mo><mrow id="S3.p1.6.m6.2.3.3.2" xref="S3.p1.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S3.p1.6.m6.2.3.3.2.1" xref="S3.p1.6.m6.2.3.3.1.cmml">(</mo><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">ğ¨</mi><mo id="S3.p1.6.m6.2.3.3.2.2" xref="S3.p1.6.m6.2.3.3.1.cmml">;</mo><mi id="S3.p1.6.m6.2.2" xref="S3.p1.6.m6.2.2.cmml">ğœ½</mi><mo stretchy="false" id="S3.p1.6.m6.2.3.3.2.3" xref="S3.p1.6.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.2b"><apply id="S3.p1.6.m6.2.3.cmml" xref="S3.p1.6.m6.2.3"><times id="S3.p1.6.m6.2.3.1.cmml" xref="S3.p1.6.m6.2.3.1"></times><ci id="S3.p1.6.m6.2.3.2.cmml" xref="S3.p1.6.m6.2.3.2">ğ¹</ci><list id="S3.p1.6.m6.2.3.3.1.cmml" xref="S3.p1.6.m6.2.3.3.2"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ğ¨</ci><ci id="S3.p1.6.m6.2.2.cmml" xref="S3.p1.6.m6.2.2">ğœ½</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.2c">F(\mathbf{o};\bm{\theta})</annotation></semantics></math> must be optimized to fit the mapping of observations to actions:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\operatorname*{minimize}\limits_{\bm{\theta}}\sum_{i}\ell\big{(}F(\mathbf{o}_{i};\bm{\theta}),\mathbf{a}_{i}\big{)}." display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><munder id="S3.E1.m1.2.2.1.1.4" xref="S3.E1.m1.2.2.1.1.4.cmml"><mo id="S3.E1.m1.2.2.1.1.4.2" xref="S3.E1.m1.2.2.1.1.4.2.cmml">minimize</mo><mi id="S3.E1.m1.2.2.1.1.4.3" xref="S3.E1.m1.2.2.1.1.4.3.cmml">ğœ½</mi></munder><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><munder id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml"><mo movablelimits="false" id="S3.E1.m1.2.2.1.1.2.3.2" xref="S3.E1.m1.2.2.1.1.2.3.2.cmml">âˆ‘</mo><mi id="S3.E1.m1.2.2.1.1.2.3.3" xref="S3.E1.m1.2.2.1.1.2.3.3.cmml">i</mi></munder><mrow id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.2.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.2.1.1.2.2.4" xref="S3.E1.m1.2.2.1.1.2.2.4.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml"><mo maxsize="120%" minsize="120%" id="S3.E1.m1.2.2.1.1.2.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ¨</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">ğœ½</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.2.2.2.2.4" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.2.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.2.cmml">ğš</mi><mi id="S3.E1.m1.2.2.1.1.2.2.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.3.cmml">i</mi></msub><mo maxsize="120%" minsize="120%" id="S3.E1.m1.2.2.1.1.2.2.2.2.5" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><times id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"></times><apply id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.cmml" xref="S3.E1.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.4.2.cmml" xref="S3.E1.m1.2.2.1.1.4.2">minimize</ci><ci id="S3.E1.m1.2.2.1.1.4.3.cmml" xref="S3.E1.m1.2.2.1.1.4.3">ğœ½</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><apply id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.3">subscript</csymbol><sum id="S3.E1.m1.2.2.1.1.2.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.3.2"></sum><ci id="S3.E1.m1.2.2.1.1.2.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3.3">ğ‘–</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2"><times id="S3.E1.m1.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.3"></times><ci id="S3.E1.m1.2.2.1.1.2.2.4.cmml" xref="S3.E1.m1.2.2.1.1.2.2.4">â„“</ci><interval closure="open" id="S3.E1.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2"><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">ğ¹</ci><list id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1"><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">ğ¨</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğœ½</ci></list></apply><apply id="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.2">ğš</ci><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\operatorname*{minimize}\limits_{\bm{\theta}}\sum_{i}\ell\big{(}F(\mathbf{o}_{i};\bm{\theta}),\mathbf{a}_{i}\big{)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.3" class="ltx_p">An implicit assumption behind this formulation is that the expertâ€™s actions are fully explained by the observations; that is, there exists a function <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">E</annotation></semantics></math> that maps observations to the expertâ€™s actions: <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{a}_{i}=E(\mathbf{o}_{i})" display="inline"><semantics id="S3.p2.2.m2.1a"><mrow id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><msub id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml"><mi id="S3.p2.2.m2.1.1.3.2" xref="S3.p2.2.m2.1.1.3.2.cmml">ğš</mi><mi id="S3.p2.2.m2.1.1.3.3" xref="S3.p2.2.m2.1.1.3.3.cmml">i</mi></msub><mo id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">=</mo><mrow id="S3.p2.2.m2.1.1.1" xref="S3.p2.2.m2.1.1.1.cmml"><mi id="S3.p2.2.m2.1.1.1.3" xref="S3.p2.2.m2.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.p2.2.m2.1.1.1.2" xref="S3.p2.2.m2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.p2.2.m2.1.1.1.1.1" xref="S3.p2.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p2.2.m2.1.1.1.1.1.2" xref="S3.p2.2.m2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.p2.2.m2.1.1.1.1.1.1" xref="S3.p2.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.p2.2.m2.1.1.1.1.1.1.2" xref="S3.p2.2.m2.1.1.1.1.1.1.2.cmml">ğ¨</mi><mi id="S3.p2.2.m2.1.1.1.1.1.1.3" xref="S3.p2.2.m2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p2.2.m2.1.1.1.1.1.3" xref="S3.p2.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><eq id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2"></eq><apply id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.3.1.cmml" xref="S3.p2.2.m2.1.1.3">subscript</csymbol><ci id="S3.p2.2.m2.1.1.3.2.cmml" xref="S3.p2.2.m2.1.1.3.2">ğš</ci><ci id="S3.p2.2.m2.1.1.3.3.cmml" xref="S3.p2.2.m2.1.1.3.3">ğ‘–</ci></apply><apply id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1"><times id="S3.p2.2.m2.1.1.1.2.cmml" xref="S3.p2.2.m2.1.1.1.2"></times><ci id="S3.p2.2.m2.1.1.1.3.cmml" xref="S3.p2.2.m2.1.1.1.3">ğ¸</ci><apply id="S3.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1.2">ğ¨</ci><ci id="S3.p2.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\mathbf{a}_{i}=E(\mathbf{o}_{i})</annotation></semantics></math>. If this assumption holds, a sufficiently expressive approximator will be able to fit the function <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">E</annotation></semantics></math> given enough data. This explains the success of imitation learning on tasks such as lane following.
However, in more complex scenarios the assumption that the mapping of observations to actions is a function breaks down. Consider a driver approaching an intersection. The driverâ€™s subsequent actions are not explained by the observations, but are additionally affected by the driverâ€™s internal state, such as the intended destination. The same observations could lead to different actions, depending on this latent state. This could be modeled as stochasticity, but a stochastic formulation misses the underlying causes of the behavior.
Moreover, even if a controller trained to imitate demonstrations of urban driving did learn to make turns and avoid collisions, it would still not constitute a useful driving system. It would wander the streets, making arbitrary decisions at intersections. A passenger in such a vehicle would not be able to communicate the intended direction of travel to the controller, or give it commands regarding which turns to take.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.3" class="ltx_p">To address this, we begin by explicitly modeling the expertâ€™s internal state by a vector <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{h}" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">ğ¡</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ğ¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\mathbf{h}</annotation></semantics></math>, which together with the observation explains the expertâ€™s action: <math id="S3.p3.2.m2.2" class="ltx_Math" alttext="\mathbf{a}_{i}=E(\mathbf{o}_{i},\mathbf{h}_{i})" display="inline"><semantics id="S3.p3.2.m2.2a"><mrow id="S3.p3.2.m2.2.2" xref="S3.p3.2.m2.2.2.cmml"><msub id="S3.p3.2.m2.2.2.4" xref="S3.p3.2.m2.2.2.4.cmml"><mi id="S3.p3.2.m2.2.2.4.2" xref="S3.p3.2.m2.2.2.4.2.cmml">ğš</mi><mi id="S3.p3.2.m2.2.2.4.3" xref="S3.p3.2.m2.2.2.4.3.cmml">i</mi></msub><mo id="S3.p3.2.m2.2.2.3" xref="S3.p3.2.m2.2.2.3.cmml">=</mo><mrow id="S3.p3.2.m2.2.2.2" xref="S3.p3.2.m2.2.2.2.cmml"><mi id="S3.p3.2.m2.2.2.2.4" xref="S3.p3.2.m2.2.2.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.p3.2.m2.2.2.2.3" xref="S3.p3.2.m2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.p3.2.m2.2.2.2.2.2" xref="S3.p3.2.m2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.p3.2.m2.2.2.2.2.2.3" xref="S3.p3.2.m2.2.2.2.2.3.cmml">(</mo><msub id="S3.p3.2.m2.1.1.1.1.1.1" xref="S3.p3.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.p3.2.m2.1.1.1.1.1.1.2" xref="S3.p3.2.m2.1.1.1.1.1.1.2.cmml">ğ¨</mi><mi id="S3.p3.2.m2.1.1.1.1.1.1.3" xref="S3.p3.2.m2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p3.2.m2.2.2.2.2.2.4" xref="S3.p3.2.m2.2.2.2.2.3.cmml">,</mo><msub id="S3.p3.2.m2.2.2.2.2.2.2" xref="S3.p3.2.m2.2.2.2.2.2.2.cmml"><mi id="S3.p3.2.m2.2.2.2.2.2.2.2" xref="S3.p3.2.m2.2.2.2.2.2.2.2.cmml">ğ¡</mi><mi id="S3.p3.2.m2.2.2.2.2.2.2.3" xref="S3.p3.2.m2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p3.2.m2.2.2.2.2.2.5" xref="S3.p3.2.m2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.2b"><apply id="S3.p3.2.m2.2.2.cmml" xref="S3.p3.2.m2.2.2"><eq id="S3.p3.2.m2.2.2.3.cmml" xref="S3.p3.2.m2.2.2.3"></eq><apply id="S3.p3.2.m2.2.2.4.cmml" xref="S3.p3.2.m2.2.2.4"><csymbol cd="ambiguous" id="S3.p3.2.m2.2.2.4.1.cmml" xref="S3.p3.2.m2.2.2.4">subscript</csymbol><ci id="S3.p3.2.m2.2.2.4.2.cmml" xref="S3.p3.2.m2.2.2.4.2">ğš</ci><ci id="S3.p3.2.m2.2.2.4.3.cmml" xref="S3.p3.2.m2.2.2.4.3">ğ‘–</ci></apply><apply id="S3.p3.2.m2.2.2.2.cmml" xref="S3.p3.2.m2.2.2.2"><times id="S3.p3.2.m2.2.2.2.3.cmml" xref="S3.p3.2.m2.2.2.2.3"></times><ci id="S3.p3.2.m2.2.2.2.4.cmml" xref="S3.p3.2.m2.2.2.2.4">ğ¸</ci><interval closure="open" id="S3.p3.2.m2.2.2.2.2.3.cmml" xref="S3.p3.2.m2.2.2.2.2.2"><apply id="S3.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p3.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.p3.2.m2.1.1.1.1.1.1.2">ğ¨</ci><ci id="S3.p3.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.p3.2.m2.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.p3.2.m2.2.2.2.2.2.2.cmml" xref="S3.p3.2.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p3.2.m2.2.2.2.2.2.2.1.cmml" xref="S3.p3.2.m2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.p3.2.m2.2.2.2.2.2.2.2.cmml" xref="S3.p3.2.m2.2.2.2.2.2.2.2">ğ¡</ci><ci id="S3.p3.2.m2.2.2.2.2.2.2.3.cmml" xref="S3.p3.2.m2.2.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.2c">\mathbf{a}_{i}=E(\mathbf{o}_{i},\mathbf{h}_{i})</annotation></semantics></math>. Vector <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{h}" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">ğ¡</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">ğ¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">\mathbf{h}</annotation></semantics></math> can include information about the expertâ€™s intentions, goals, and prior knowledge. The standard imitation learning objective can then be rewritten as</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\operatorname*{minimize}\limits_{\bm{\theta}}\sum_{i}\ell\big{(}F(\mathbf{o}_{i};\bm{\theta}),E(\mathbf{o}_{i},\mathbf{h}_{i})\big{)}." display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><munder id="S3.E2.m1.2.2.1.1.4" xref="S3.E2.m1.2.2.1.1.4.cmml"><mo id="S3.E2.m1.2.2.1.1.4.2" xref="S3.E2.m1.2.2.1.1.4.2.cmml">minimize</mo><mi id="S3.E2.m1.2.2.1.1.4.3" xref="S3.E2.m1.2.2.1.1.4.3.cmml">ğœ½</mi></munder><mo lspace="0.167em" rspace="0em" id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml">â€‹</mo><mrow id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><munder id="S3.E2.m1.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.3.cmml"><mo movablelimits="false" id="S3.E2.m1.2.2.1.1.2.3.2" xref="S3.E2.m1.2.2.1.1.2.3.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.2.2.1.1.2.3.3" xref="S3.E2.m1.2.2.1.1.2.3.3.cmml">i</mi></munder><mrow id="S3.E2.m1.2.2.1.1.2.2" xref="S3.E2.m1.2.2.1.1.2.2.cmml"><mi mathvariant="normal" id="S3.E2.m1.2.2.1.1.2.2.4" xref="S3.E2.m1.2.2.1.1.2.2.4.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.3.cmml">â€‹</mo><mrow id="S3.E2.m1.2.2.1.1.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.3.cmml"><mo maxsize="120%" minsize="120%" id="S3.E2.m1.2.2.1.1.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.3.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ¨</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">ğœ½</mi><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.2.2.2.2.4" xref="S3.E2.m1.2.2.1.1.2.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.2.2.1.1.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.4" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.3.cmml">(</mo><msub id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.2.cmml">ğ¨</mi><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.4" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.3.cmml">,</mo><msub id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.cmml">ğ¡</mi><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.5" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%" id="S3.E2.m1.2.2.1.1.2.2.2.2.5" xref="S3.E2.m1.2.2.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"></times><apply id="S3.E2.m1.2.2.1.1.4.cmml" xref="S3.E2.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.4.1.cmml" xref="S3.E2.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.4.2.cmml" xref="S3.E2.m1.2.2.1.1.4.2">minimize</ci><ci id="S3.E2.m1.2.2.1.1.4.3.cmml" xref="S3.E2.m1.2.2.1.1.4.3">ğœ½</ci></apply><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><apply id="S3.E2.m1.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.2.3">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.2.3.2"></sum><ci id="S3.E2.m1.2.2.1.1.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3.3">ğ‘–</ci></apply><apply id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2"><times id="S3.E2.m1.2.2.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.3"></times><ci id="S3.E2.m1.2.2.1.1.2.2.4.cmml" xref="S3.E2.m1.2.2.1.1.2.2.4">â„“</ci><interval closure="open" id="S3.E2.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3">ğ¹</ci><list id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">ğ¨</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğœ½</ci></list></apply><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2"><times id="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.3"></times><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.4.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.4">ğ¸</ci><interval closure="open" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2"><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.2">ğ¨</ci><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.2">ğ¡</ci><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\operatorname*{minimize}\limits_{\bm{\theta}}\sum_{i}\ell\big{(}F(\mathbf{o}_{i};\bm{\theta}),E(\mathbf{o}_{i},\mathbf{h}_{i})\big{)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.p3.4" class="ltx_p">It is now clear that the expertâ€™s action is affected by information that is not provided to the controller <math id="S3.p3.4.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.p3.4.m1.1a"><mi id="S3.p3.4.m1.1.1" xref="S3.p3.4.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m1.1b"><ci id="S3.p3.4.m1.1.1.cmml" xref="S3.p3.4.m1.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m1.1c">F</annotation></semantics></math>.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.5" class="ltx_p">We expose the latent state <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{h}" display="inline"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">ğ¡</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">ğ¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">\mathbf{h}</annotation></semantics></math> to the controller by introducing an additional command input: <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{c}=\mathbf{c}(\mathbf{h})" display="inline"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.2" xref="S3.p4.2.m2.1.2.cmml"><mi id="S3.p4.2.m2.1.2.2" xref="S3.p4.2.m2.1.2.2.cmml">ğœ</mi><mo id="S3.p4.2.m2.1.2.1" xref="S3.p4.2.m2.1.2.1.cmml">=</mo><mrow id="S3.p4.2.m2.1.2.3" xref="S3.p4.2.m2.1.2.3.cmml"><mi id="S3.p4.2.m2.1.2.3.2" xref="S3.p4.2.m2.1.2.3.2.cmml">ğœ</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.2.3.1" xref="S3.p4.2.m2.1.2.3.1.cmml">â€‹</mo><mrow id="S3.p4.2.m2.1.2.3.3.2" xref="S3.p4.2.m2.1.2.3.cmml"><mo stretchy="false" id="S3.p4.2.m2.1.2.3.3.2.1" xref="S3.p4.2.m2.1.2.3.cmml">(</mo><mi id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">ğ¡</mi><mo stretchy="false" id="S3.p4.2.m2.1.2.3.3.2.2" xref="S3.p4.2.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.2.cmml" xref="S3.p4.2.m2.1.2"><eq id="S3.p4.2.m2.1.2.1.cmml" xref="S3.p4.2.m2.1.2.1"></eq><ci id="S3.p4.2.m2.1.2.2.cmml" xref="S3.p4.2.m2.1.2.2">ğœ</ci><apply id="S3.p4.2.m2.1.2.3.cmml" xref="S3.p4.2.m2.1.2.3"><times id="S3.p4.2.m2.1.2.3.1.cmml" xref="S3.p4.2.m2.1.2.3.1"></times><ci id="S3.p4.2.m2.1.2.3.2.cmml" xref="S3.p4.2.m2.1.2.3.2">ğœ</ci><ci id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">ğ¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">\mathbf{c}=\mathbf{c}(\mathbf{h})</annotation></semantics></math>.
At training time, the command <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="S3.p4.3.m3.1a"><mi id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">ğœ</mi><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><ci id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">\mathbf{c}</annotation></semantics></math> is provided by the expert.
It need not constitute the entire latent state <math id="S3.p4.4.m4.1" class="ltx_Math" alttext="\mathbf{h}" display="inline"><semantics id="S3.p4.4.m4.1a"><mi id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml">ğ¡</mi><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><ci id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1">ğ¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">\mathbf{h}</annotation></semantics></math>, but should provide useful information about the expertâ€™s decision-making.
For example, human drivers already use turn signals to communicate their intent when approaching intersections; these turn signals can be used as commands in our formulation.
At test time, commands can be used to affect the behavior of the controller.
These test-time commands can come from a human user or a planning module.
In urban driving, a typical command would be â€œturn right at the next intersectionâ€, which can be provided by a navigation system or a passenger.
The training dataset becomes <math id="S3.p4.5.m5.1" class="ltx_Math" alttext="\mathcal{D}=\{\left\langle\mathbf{o}_{i},\mathbf{c}_{i},\mathbf{a}_{i}\right\rangle\}_{i=1}^{N}" display="inline"><semantics id="S3.p4.5.m5.1a"><mrow id="S3.p4.5.m5.1.1" xref="S3.p4.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p4.5.m5.1.1.3" xref="S3.p4.5.m5.1.1.3.cmml">ğ’Ÿ</mi><mo id="S3.p4.5.m5.1.1.2" xref="S3.p4.5.m5.1.1.2.cmml">=</mo><msubsup id="S3.p4.5.m5.1.1.1" xref="S3.p4.5.m5.1.1.1.cmml"><mrow id="S3.p4.5.m5.1.1.1.1.1.1" xref="S3.p4.5.m5.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.p4.5.m5.1.1.1.1.1.1.2" xref="S3.p4.5.m5.1.1.1.1.1.2.cmml">{</mo><mrow id="S3.p4.5.m5.1.1.1.1.1.1.1.3" xref="S3.p4.5.m5.1.1.1.1.1.1.1.4.cmml"><mo id="S3.p4.5.m5.1.1.1.1.1.1.1.3.4" xref="S3.p4.5.m5.1.1.1.1.1.1.1.4.cmml">âŸ¨</mo><msub id="S3.p4.5.m5.1.1.1.1.1.1.1.1.1" xref="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.2" xref="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.2.cmml">ğ¨</mi><mi id="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.3" xref="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p4.5.m5.1.1.1.1.1.1.1.3.5" xref="S3.p4.5.m5.1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.p4.5.m5.1.1.1.1.1.1.1.2.2" xref="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.2" xref="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.2.cmml">ğœ</mi><mi id="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.3" xref="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.p4.5.m5.1.1.1.1.1.1.1.3.6" xref="S3.p4.5.m5.1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.p4.5.m5.1.1.1.1.1.1.1.3.3" xref="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.2" xref="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.2.cmml">ğš</mi><mi id="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.3" xref="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub><mo id="S3.p4.5.m5.1.1.1.1.1.1.1.3.7" xref="S3.p4.5.m5.1.1.1.1.1.1.1.4.cmml">âŸ©</mo></mrow><mo stretchy="false" id="S3.p4.5.m5.1.1.1.1.1.1.3" xref="S3.p4.5.m5.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p4.5.m5.1.1.1.1.3" xref="S3.p4.5.m5.1.1.1.1.3.cmml"><mi id="S3.p4.5.m5.1.1.1.1.3.2" xref="S3.p4.5.m5.1.1.1.1.3.2.cmml">i</mi><mo id="S3.p4.5.m5.1.1.1.1.3.1" xref="S3.p4.5.m5.1.1.1.1.3.1.cmml">=</mo><mn id="S3.p4.5.m5.1.1.1.1.3.3" xref="S3.p4.5.m5.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p4.5.m5.1.1.1.3" xref="S3.p4.5.m5.1.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.5.m5.1b"><apply id="S3.p4.5.m5.1.1.cmml" xref="S3.p4.5.m5.1.1"><eq id="S3.p4.5.m5.1.1.2.cmml" xref="S3.p4.5.m5.1.1.2"></eq><ci id="S3.p4.5.m5.1.1.3.cmml" xref="S3.p4.5.m5.1.1.3">ğ’Ÿ</ci><apply id="S3.p4.5.m5.1.1.1.cmml" xref="S3.p4.5.m5.1.1.1"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.1.2.cmml" xref="S3.p4.5.m5.1.1.1">superscript</csymbol><apply id="S3.p4.5.m5.1.1.1.1.cmml" xref="S3.p4.5.m5.1.1.1"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.1.1.2.cmml" xref="S3.p4.5.m5.1.1.1">subscript</csymbol><set id="S3.p4.5.m5.1.1.1.1.1.2.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1"><list id="S3.p4.5.m5.1.1.1.1.1.1.1.4.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.3"><apply id="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.2">ğ¨</ci><ci id="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.2">ğœ</ci><ci id="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply><apply id="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.2">ğš</ci><ci id="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.p4.5.m5.1.1.1.1.1.1.1.3.3.3">ğ‘–</ci></apply></list></set><apply id="S3.p4.5.m5.1.1.1.1.3.cmml" xref="S3.p4.5.m5.1.1.1.1.3"><eq id="S3.p4.5.m5.1.1.1.1.3.1.cmml" xref="S3.p4.5.m5.1.1.1.1.3.1"></eq><ci id="S3.p4.5.m5.1.1.1.1.3.2.cmml" xref="S3.p4.5.m5.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.p4.5.m5.1.1.1.1.3.3.cmml" xref="S3.p4.5.m5.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p4.5.m5.1.1.1.3.cmml" xref="S3.p4.5.m5.1.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m5.1c">\mathcal{D}=\{\left\langle\mathbf{o}_{i},\mathbf{c}_{i},\mathbf{a}_{i}\right\rangle\}_{i=1}^{N}</annotation></semantics></math>.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">The command-conditional imitation learning objective is</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\operatorname*{minimize}\limits_{\bm{\theta}}\sum_{i}\ell\big{(}F(\mathbf{o}_{i},\mathbf{c}_{i};\bm{\theta}),\mathbf{a}_{i}\big{)}." display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><munder id="S3.E3.m1.2.2.1.1.4" xref="S3.E3.m1.2.2.1.1.4.cmml"><mo id="S3.E3.m1.2.2.1.1.4.2" xref="S3.E3.m1.2.2.1.1.4.2.cmml">minimize</mo><mi id="S3.E3.m1.2.2.1.1.4.3" xref="S3.E3.m1.2.2.1.1.4.3.cmml">ğœ½</mi></munder><mo lspace="0.167em" rspace="0em" id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml"><munder id="S3.E3.m1.2.2.1.1.2.3" xref="S3.E3.m1.2.2.1.1.2.3.cmml"><mo movablelimits="false" id="S3.E3.m1.2.2.1.1.2.3.2" xref="S3.E3.m1.2.2.1.1.2.3.2.cmml">âˆ‘</mo><mi id="S3.E3.m1.2.2.1.1.2.3.3" xref="S3.E3.m1.2.2.1.1.2.3.3.cmml">i</mi></munder><mrow id="S3.E3.m1.2.2.1.1.2.2" xref="S3.E3.m1.2.2.1.1.2.2.cmml"><mi mathvariant="normal" id="S3.E3.m1.2.2.1.1.2.2.4" xref="S3.E3.m1.2.2.1.1.2.2.4.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.3.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.1.1.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml"><mo maxsize="120%" minsize="120%" id="S3.E3.m1.2.2.1.1.2.2.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.4" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.4.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ¨</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.4" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.cmml">ğœ</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.5" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">;</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">ğœ½</mi><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.6" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.2.2.2.2.4" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E3.m1.2.2.1.1.2.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.cmml">ğš</mi><mi id="S3.E3.m1.2.2.1.1.2.2.2.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.3.cmml">i</mi></msub><mo maxsize="120%" minsize="120%" id="S3.E3.m1.2.2.1.1.2.2.2.2.5" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><times id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"></times><apply id="S3.E3.m1.2.2.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.4.1.cmml" xref="S3.E3.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.4.2.cmml" xref="S3.E3.m1.2.2.1.1.4.2">minimize</ci><ci id="S3.E3.m1.2.2.1.1.4.3.cmml" xref="S3.E3.m1.2.2.1.1.4.3">ğœ½</ci></apply><apply id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"><apply id="S3.E3.m1.2.2.1.1.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.3.1.cmml" xref="S3.E3.m1.2.2.1.1.2.3">subscript</csymbol><sum id="S3.E3.m1.2.2.1.1.2.3.2.cmml" xref="S3.E3.m1.2.2.1.1.2.3.2"></sum><ci id="S3.E3.m1.2.2.1.1.2.3.3.cmml" xref="S3.E3.m1.2.2.1.1.2.3.3">ğ‘–</ci></apply><apply id="S3.E3.m1.2.2.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2"><times id="S3.E3.m1.2.2.1.1.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3"></times><ci id="S3.E3.m1.2.2.1.1.2.2.4.cmml" xref="S3.E3.m1.2.2.1.1.2.2.4">â„“</ci><interval closure="open" id="S3.E3.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2"><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3"></times><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.4">ğ¹</ci><vector id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2"><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">ğ¨</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.2">ğœ</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.2.2.3">ğ‘–</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğœ½</ci></vector></apply><apply id="S3.E3.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2">ğš</ci><ci id="S3.E3.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\operatorname*{minimize}\limits_{\bm{\theta}}\sum_{i}\ell\big{(}F(\mathbf{o}_{i},\mathbf{c}_{i};\bm{\theta}),\mathbf{a}_{i}\big{)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">In contrast with objectiveÂ (<a href="#S3.E2" title="In III Conditional Imitation Learning â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), the learner is informed about the expertâ€™s latent state and can use this additional information in predicting the action. This setting is illustrated in FigureÂ <a href="#S3.F2" title="Figure 2 â€£ III Conditional Imitation Learning â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1710.02410/assets/images_compressed/high_level.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>High-level overview. The controller receives an observation <math id="S3.F2.4.m1.1" class="ltx_Math" alttext="\mathbf{o}_{t}" display="inline"><semantics id="S3.F2.4.m1.1b"><msub id="S3.F2.4.m1.1.1" xref="S3.F2.4.m1.1.1.cmml"><mi id="S3.F2.4.m1.1.1.2" xref="S3.F2.4.m1.1.1.2.cmml">ğ¨</mi><mi id="S3.F2.4.m1.1.1.3" xref="S3.F2.4.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.4.m1.1c"><apply id="S3.F2.4.m1.1.1.cmml" xref="S3.F2.4.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.4.m1.1.1.1.cmml" xref="S3.F2.4.m1.1.1">subscript</csymbol><ci id="S3.F2.4.m1.1.1.2.cmml" xref="S3.F2.4.m1.1.1.2">ğ¨</ci><ci id="S3.F2.4.m1.1.1.3.cmml" xref="S3.F2.4.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m1.1d">\mathbf{o}_{t}</annotation></semantics></math> from the environment and a command <math id="S3.F2.5.m2.1" class="ltx_Math" alttext="\mathbf{c}_{t}" display="inline"><semantics id="S3.F2.5.m2.1b"><msub id="S3.F2.5.m2.1.1" xref="S3.F2.5.m2.1.1.cmml"><mi id="S3.F2.5.m2.1.1.2" xref="S3.F2.5.m2.1.1.2.cmml">ğœ</mi><mi id="S3.F2.5.m2.1.1.3" xref="S3.F2.5.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.5.m2.1c"><apply id="S3.F2.5.m2.1.1.cmml" xref="S3.F2.5.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.5.m2.1.1.1.cmml" xref="S3.F2.5.m2.1.1">subscript</csymbol><ci id="S3.F2.5.m2.1.1.2.cmml" xref="S3.F2.5.m2.1.1.2">ğœ</ci><ci id="S3.F2.5.m2.1.1.3.cmml" xref="S3.F2.5.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m2.1d">\mathbf{c}_{t}</annotation></semantics></math>. It produces an action <math id="S3.F2.6.m3.1" class="ltx_Math" alttext="\mathbf{a}_{t}" display="inline"><semantics id="S3.F2.6.m3.1b"><msub id="S3.F2.6.m3.1.1" xref="S3.F2.6.m3.1.1.cmml"><mi id="S3.F2.6.m3.1.1.2" xref="S3.F2.6.m3.1.1.2.cmml">ğš</mi><mi id="S3.F2.6.m3.1.1.3" xref="S3.F2.6.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.6.m3.1c"><apply id="S3.F2.6.m3.1.1.cmml" xref="S3.F2.6.m3.1.1"><csymbol cd="ambiguous" id="S3.F2.6.m3.1.1.1.cmml" xref="S3.F2.6.m3.1.1">subscript</csymbol><ci id="S3.F2.6.m3.1.1.2.cmml" xref="S3.F2.6.m3.1.1.2">ğš</ci><ci id="S3.F2.6.m3.1.1.3.cmml" xref="S3.F2.6.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m3.1d">\mathbf{a}_{t}</annotation></semantics></math> that affects the environment, advancing to the next time step.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We now describe a practical implementation of command-conditional imitation learning.
Code is available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/carla-simulator/imitation-learning</span>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<table id="S4.F3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F3.2.2" class="ltx_tr">
<td id="S4.F3.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/net_arch.jpg" id="S4.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="261" height="138" alt="Refer to caption"></td>
<td id="S4.F3.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/net_arch_branch.jpg" id="S4.F3.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="333" height="138" alt="Refer to caption"></td>
</tr>
<tr id="S4.F3.2.3.1" class="ltx_tr">
<td id="S4.F3.2.3.1.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S4.F3.2.3.1.2" class="ltx_td ltx_nopad_l ltx_align_center">(b)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Two network architectures for command-conditional imitation learning. (a) <span id="S4.F3.5.1" class="ltx_text ltx_font_typewriter">command input</span>: the command is processed as input by the network, together with the image and the measurements. The same architecture can be used for goal-conditional learning (one of the baselines in our experiments), by replacing the command by a vector pointing to the goal. (b) <span id="S4.F3.6.2" class="ltx_text ltx_font_typewriter">branched</span>: the command acts as a switch that selects between specialized sub-modules.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Network Architecture</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.9" class="ltx_p">Assume that each observation <math id="S4.SS1.p1.1.m1.2" class="ltx_Math" alttext="\mathbf{o}=\left\langle\mathbf{i},\mathbf{m}\right\rangle" display="inline"><semantics id="S4.SS1.p1.1.m1.2a"><mrow id="S4.SS1.p1.1.m1.2.3" xref="S4.SS1.p1.1.m1.2.3.cmml"><mi id="S4.SS1.p1.1.m1.2.3.2" xref="S4.SS1.p1.1.m1.2.3.2.cmml">ğ¨</mi><mo id="S4.SS1.p1.1.m1.2.3.1" xref="S4.SS1.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S4.SS1.p1.1.m1.2.3.3.2" xref="S4.SS1.p1.1.m1.2.3.3.1.cmml"><mo id="S4.SS1.p1.1.m1.2.3.3.2.1" xref="S4.SS1.p1.1.m1.2.3.3.1.cmml">âŸ¨</mo><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">ğ¢</mi><mo id="S4.SS1.p1.1.m1.2.3.3.2.2" xref="S4.SS1.p1.1.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS1.p1.1.m1.2.2" xref="S4.SS1.p1.1.m1.2.2.cmml">ğ¦</mi><mo id="S4.SS1.p1.1.m1.2.3.3.2.3" xref="S4.SS1.p1.1.m1.2.3.3.1.cmml">âŸ©</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.2b"><apply id="S4.SS1.p1.1.m1.2.3.cmml" xref="S4.SS1.p1.1.m1.2.3"><eq id="S4.SS1.p1.1.m1.2.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.1"></eq><ci id="S4.SS1.p1.1.m1.2.3.2.cmml" xref="S4.SS1.p1.1.m1.2.3.2">ğ¨</ci><list id="S4.SS1.p1.1.m1.2.3.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.3.2"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ¢</ci><ci id="S4.SS1.p1.1.m1.2.2.cmml" xref="S4.SS1.p1.1.m1.2.2">ğ¦</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.2c">\mathbf{o}=\left\langle\mathbf{i},\mathbf{m}\right\rangle</annotation></semantics></math> comprises an image <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{i}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">ğ¢</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">ğ¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\mathbf{i}</annotation></semantics></math> and a low-dimensional vector <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{m}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">ğ¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">ğ¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\mathbf{m}</annotation></semantics></math> that we refer to as measurements, following Dosovitskiy and KoltunÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The controller <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">F</annotation></semantics></math> is represented by a deep network. The network takes the image <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{i}" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">ğ¢</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">ğ¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\mathbf{i}</annotation></semantics></math>, the measurements <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{m}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mi id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">ğ¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">ğ¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">\mathbf{m}</annotation></semantics></math>, and the command <math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><mi id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">ğœ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><ci id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">\mathbf{c}</annotation></semantics></math> as inputs, and produces an action <math id="S4.SS1.p1.8.m8.1" class="ltx_Math" alttext="\mathbf{a}" display="inline"><semantics id="S4.SS1.p1.8.m8.1a"><mi id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">ğš</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><ci id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1">ğš</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">\mathbf{a}</annotation></semantics></math> as its output. The action space can be discrete, continuous, or a hybrid of these. In our driving experiments, the action space is continuous and two-dimensional: steering angle and acceleration.
The acceleration can be negative, which corresponds to braking or driving backwards.
The command <math id="S4.SS1.p1.9.m9.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="S4.SS1.p1.9.m9.1a"><mi id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml">ğœ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.1b"><ci id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.1c">\mathbf{c}</annotation></semantics></math> is a categorical variable represented by a one-hot vector.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.4" class="ltx_p">We study two approaches to incorporating the command <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">ğœ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\mathbf{c}</annotation></semantics></math> into the network.
The first architecture is illustrated in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ IV Methodology â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a).
The network takes the command as an input, alongside the image and the measurements.
These three inputs are processed independently by three modules: an image module <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="I(\mathbf{i})" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.2" xref="S4.SS1.p2.2.m2.1.2.cmml"><mi id="S4.SS1.p2.2.m2.1.2.2" xref="S4.SS1.p2.2.m2.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.2.1" xref="S4.SS1.p2.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S4.SS1.p2.2.m2.1.2.3.2" xref="S4.SS1.p2.2.m2.1.2.cmml"><mo stretchy="false" id="S4.SS1.p2.2.m2.1.2.3.2.1" xref="S4.SS1.p2.2.m2.1.2.cmml">(</mo><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">ğ¢</mi><mo stretchy="false" id="S4.SS1.p2.2.m2.1.2.3.2.2" xref="S4.SS1.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.2.cmml" xref="S4.SS1.p2.2.m2.1.2"><times id="S4.SS1.p2.2.m2.1.2.1.cmml" xref="S4.SS1.p2.2.m2.1.2.1"></times><ci id="S4.SS1.p2.2.m2.1.2.2.cmml" xref="S4.SS1.p2.2.m2.1.2.2">ğ¼</ci><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">ğ¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">I(\mathbf{i})</annotation></semantics></math>, a measurement module <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="M(\mathbf{m})" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.2" xref="S4.SS1.p2.3.m3.1.2.cmml"><mi id="S4.SS1.p2.3.m3.1.2.2" xref="S4.SS1.p2.3.m3.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.2.1" xref="S4.SS1.p2.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S4.SS1.p2.3.m3.1.2.3.2" xref="S4.SS1.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S4.SS1.p2.3.m3.1.2.3.2.1" xref="S4.SS1.p2.3.m3.1.2.cmml">(</mo><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">ğ¦</mi><mo stretchy="false" id="S4.SS1.p2.3.m3.1.2.3.2.2" xref="S4.SS1.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.2.cmml" xref="S4.SS1.p2.3.m3.1.2"><times id="S4.SS1.p2.3.m3.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.2.1"></times><ci id="S4.SS1.p2.3.m3.1.2.2.cmml" xref="S4.SS1.p2.3.m3.1.2.2">ğ‘€</ci><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">ğ¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">M(\mathbf{m})</annotation></semantics></math>, and a command module <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="C(\mathbf{c})" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.2" xref="S4.SS1.p2.4.m4.1.2.cmml"><mi id="S4.SS1.p2.4.m4.1.2.2" xref="S4.SS1.p2.4.m4.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.4.m4.1.2.1" xref="S4.SS1.p2.4.m4.1.2.1.cmml">â€‹</mo><mrow id="S4.SS1.p2.4.m4.1.2.3.2" xref="S4.SS1.p2.4.m4.1.2.cmml"><mo stretchy="false" id="S4.SS1.p2.4.m4.1.2.3.2.1" xref="S4.SS1.p2.4.m4.1.2.cmml">(</mo><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">ğœ</mi><mo stretchy="false" id="S4.SS1.p2.4.m4.1.2.3.2.2" xref="S4.SS1.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.2.cmml" xref="S4.SS1.p2.4.m4.1.2"><times id="S4.SS1.p2.4.m4.1.2.1.cmml" xref="S4.SS1.p2.4.m4.1.2.1"></times><ci id="S4.SS1.p2.4.m4.1.2.2.cmml" xref="S4.SS1.p2.4.m4.1.2.2">ğ¶</ci><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">C(\mathbf{c})</annotation></semantics></math>. The image module is implemented as a convolutional network, the other two modules as fully-connected networks. The outputs of these modules are concatenated into a joint representation:</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.7" class="ltx_Math" alttext="\mathbf{j}=J(\mathbf{i},\mathbf{m},\mathbf{c})=\left\langle I(\mathbf{i}),M(\mathbf{m}),C(\mathbf{c})\right\rangle." display="block"><semantics id="S4.E4.m1.7a"><mrow id="S4.E4.m1.7.7.1" xref="S4.E4.m1.7.7.1.1.cmml"><mrow id="S4.E4.m1.7.7.1.1" xref="S4.E4.m1.7.7.1.1.cmml"><mi id="S4.E4.m1.7.7.1.1.5" xref="S4.E4.m1.7.7.1.1.5.cmml">ğ£</mi><mo id="S4.E4.m1.7.7.1.1.6" xref="S4.E4.m1.7.7.1.1.6.cmml">=</mo><mrow id="S4.E4.m1.7.7.1.1.7" xref="S4.E4.m1.7.7.1.1.7.cmml"><mi id="S4.E4.m1.7.7.1.1.7.2" xref="S4.E4.m1.7.7.1.1.7.2.cmml">J</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.7.7.1.1.7.1" xref="S4.E4.m1.7.7.1.1.7.1.cmml">â€‹</mo><mrow id="S4.E4.m1.7.7.1.1.7.3.2" xref="S4.E4.m1.7.7.1.1.7.3.1.cmml"><mo stretchy="false" id="S4.E4.m1.7.7.1.1.7.3.2.1" xref="S4.E4.m1.7.7.1.1.7.3.1.cmml">(</mo><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">ğ¢</mi><mo id="S4.E4.m1.7.7.1.1.7.3.2.2" xref="S4.E4.m1.7.7.1.1.7.3.1.cmml">,</mo><mi id="S4.E4.m1.2.2" xref="S4.E4.m1.2.2.cmml">ğ¦</mi><mo id="S4.E4.m1.7.7.1.1.7.3.2.3" xref="S4.E4.m1.7.7.1.1.7.3.1.cmml">,</mo><mi id="S4.E4.m1.3.3" xref="S4.E4.m1.3.3.cmml">ğœ</mi><mo stretchy="false" id="S4.E4.m1.7.7.1.1.7.3.2.4" xref="S4.E4.m1.7.7.1.1.7.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.7.7.1.1.8" xref="S4.E4.m1.7.7.1.1.8.cmml">=</mo><mrow id="S4.E4.m1.7.7.1.1.3.3" xref="S4.E4.m1.7.7.1.1.3.4.cmml"><mo id="S4.E4.m1.7.7.1.1.3.3.4" xref="S4.E4.m1.7.7.1.1.3.4.cmml">âŸ¨</mo><mrow id="S4.E4.m1.7.7.1.1.1.1.1" xref="S4.E4.m1.7.7.1.1.1.1.1.cmml"><mi id="S4.E4.m1.7.7.1.1.1.1.1.2" xref="S4.E4.m1.7.7.1.1.1.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.7.7.1.1.1.1.1.1" xref="S4.E4.m1.7.7.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S4.E4.m1.7.7.1.1.1.1.1.3.2" xref="S4.E4.m1.7.7.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.7.7.1.1.1.1.1.3.2.1" xref="S4.E4.m1.7.7.1.1.1.1.1.cmml">(</mo><mi id="S4.E4.m1.4.4" xref="S4.E4.m1.4.4.cmml">ğ¢</mi><mo stretchy="false" id="S4.E4.m1.7.7.1.1.1.1.1.3.2.2" xref="S4.E4.m1.7.7.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.7.7.1.1.3.3.5" xref="S4.E4.m1.7.7.1.1.3.4.cmml">,</mo><mrow id="S4.E4.m1.7.7.1.1.2.2.2" xref="S4.E4.m1.7.7.1.1.2.2.2.cmml"><mi id="S4.E4.m1.7.7.1.1.2.2.2.2" xref="S4.E4.m1.7.7.1.1.2.2.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.7.7.1.1.2.2.2.1" xref="S4.E4.m1.7.7.1.1.2.2.2.1.cmml">â€‹</mo><mrow id="S4.E4.m1.7.7.1.1.2.2.2.3.2" xref="S4.E4.m1.7.7.1.1.2.2.2.cmml"><mo stretchy="false" id="S4.E4.m1.7.7.1.1.2.2.2.3.2.1" xref="S4.E4.m1.7.7.1.1.2.2.2.cmml">(</mo><mi id="S4.E4.m1.5.5" xref="S4.E4.m1.5.5.cmml">ğ¦</mi><mo stretchy="false" id="S4.E4.m1.7.7.1.1.2.2.2.3.2.2" xref="S4.E4.m1.7.7.1.1.2.2.2.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.7.7.1.1.3.3.6" xref="S4.E4.m1.7.7.1.1.3.4.cmml">,</mo><mrow id="S4.E4.m1.7.7.1.1.3.3.3" xref="S4.E4.m1.7.7.1.1.3.3.3.cmml"><mi id="S4.E4.m1.7.7.1.1.3.3.3.2" xref="S4.E4.m1.7.7.1.1.3.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.7.7.1.1.3.3.3.1" xref="S4.E4.m1.7.7.1.1.3.3.3.1.cmml">â€‹</mo><mrow id="S4.E4.m1.7.7.1.1.3.3.3.3.2" xref="S4.E4.m1.7.7.1.1.3.3.3.cmml"><mo stretchy="false" id="S4.E4.m1.7.7.1.1.3.3.3.3.2.1" xref="S4.E4.m1.7.7.1.1.3.3.3.cmml">(</mo><mi id="S4.E4.m1.6.6" xref="S4.E4.m1.6.6.cmml">ğœ</mi><mo stretchy="false" id="S4.E4.m1.7.7.1.1.3.3.3.3.2.2" xref="S4.E4.m1.7.7.1.1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.7.7.1.1.3.3.7" xref="S4.E4.m1.7.7.1.1.3.4.cmml">âŸ©</mo></mrow></mrow><mo lspace="0em" id="S4.E4.m1.7.7.1.2" xref="S4.E4.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.7b"><apply id="S4.E4.m1.7.7.1.1.cmml" xref="S4.E4.m1.7.7.1"><and id="S4.E4.m1.7.7.1.1a.cmml" xref="S4.E4.m1.7.7.1"></and><apply id="S4.E4.m1.7.7.1.1b.cmml" xref="S4.E4.m1.7.7.1"><eq id="S4.E4.m1.7.7.1.1.6.cmml" xref="S4.E4.m1.7.7.1.1.6"></eq><ci id="S4.E4.m1.7.7.1.1.5.cmml" xref="S4.E4.m1.7.7.1.1.5">ğ£</ci><apply id="S4.E4.m1.7.7.1.1.7.cmml" xref="S4.E4.m1.7.7.1.1.7"><times id="S4.E4.m1.7.7.1.1.7.1.cmml" xref="S4.E4.m1.7.7.1.1.7.1"></times><ci id="S4.E4.m1.7.7.1.1.7.2.cmml" xref="S4.E4.m1.7.7.1.1.7.2">ğ½</ci><vector id="S4.E4.m1.7.7.1.1.7.3.1.cmml" xref="S4.E4.m1.7.7.1.1.7.3.2"><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">ğ¢</ci><ci id="S4.E4.m1.2.2.cmml" xref="S4.E4.m1.2.2">ğ¦</ci><ci id="S4.E4.m1.3.3.cmml" xref="S4.E4.m1.3.3">ğœ</ci></vector></apply></apply><apply id="S4.E4.m1.7.7.1.1c.cmml" xref="S4.E4.m1.7.7.1"><eq id="S4.E4.m1.7.7.1.1.8.cmml" xref="S4.E4.m1.7.7.1.1.8"></eq><share href="#S4.E4.m1.7.7.1.1.7.cmml" id="S4.E4.m1.7.7.1.1d.cmml" xref="S4.E4.m1.7.7.1"></share><list id="S4.E4.m1.7.7.1.1.3.4.cmml" xref="S4.E4.m1.7.7.1.1.3.3"><apply id="S4.E4.m1.7.7.1.1.1.1.1.cmml" xref="S4.E4.m1.7.7.1.1.1.1.1"><times id="S4.E4.m1.7.7.1.1.1.1.1.1.cmml" xref="S4.E4.m1.7.7.1.1.1.1.1.1"></times><ci id="S4.E4.m1.7.7.1.1.1.1.1.2.cmml" xref="S4.E4.m1.7.7.1.1.1.1.1.2">ğ¼</ci><ci id="S4.E4.m1.4.4.cmml" xref="S4.E4.m1.4.4">ğ¢</ci></apply><apply id="S4.E4.m1.7.7.1.1.2.2.2.cmml" xref="S4.E4.m1.7.7.1.1.2.2.2"><times id="S4.E4.m1.7.7.1.1.2.2.2.1.cmml" xref="S4.E4.m1.7.7.1.1.2.2.2.1"></times><ci id="S4.E4.m1.7.7.1.1.2.2.2.2.cmml" xref="S4.E4.m1.7.7.1.1.2.2.2.2">ğ‘€</ci><ci id="S4.E4.m1.5.5.cmml" xref="S4.E4.m1.5.5">ğ¦</ci></apply><apply id="S4.E4.m1.7.7.1.1.3.3.3.cmml" xref="S4.E4.m1.7.7.1.1.3.3.3"><times id="S4.E4.m1.7.7.1.1.3.3.3.1.cmml" xref="S4.E4.m1.7.7.1.1.3.3.3.1"></times><ci id="S4.E4.m1.7.7.1.1.3.3.3.2.cmml" xref="S4.E4.m1.7.7.1.1.3.3.3.2">ğ¶</ci><ci id="S4.E4.m1.6.6.cmml" xref="S4.E4.m1.6.6">ğœ</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.7c">\mathbf{j}=J(\mathbf{i},\mathbf{m},\mathbf{c})=\left\langle I(\mathbf{i}),M(\mathbf{m}),C(\mathbf{c})\right\rangle.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p2.5" class="ltx_p">The control module, implemented as a fully-connected network, takes this joint representation and outputs an action <math id="S4.SS1.p2.5.m1.1" class="ltx_Math" alttext="A(\mathbf{j})" display="inline"><semantics id="S4.SS1.p2.5.m1.1a"><mrow id="S4.SS1.p2.5.m1.1.2" xref="S4.SS1.p2.5.m1.1.2.cmml"><mi id="S4.SS1.p2.5.m1.1.2.2" xref="S4.SS1.p2.5.m1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.5.m1.1.2.1" xref="S4.SS1.p2.5.m1.1.2.1.cmml">â€‹</mo><mrow id="S4.SS1.p2.5.m1.1.2.3.2" xref="S4.SS1.p2.5.m1.1.2.cmml"><mo stretchy="false" id="S4.SS1.p2.5.m1.1.2.3.2.1" xref="S4.SS1.p2.5.m1.1.2.cmml">(</mo><mi id="S4.SS1.p2.5.m1.1.1" xref="S4.SS1.p2.5.m1.1.1.cmml">ğ£</mi><mo stretchy="false" id="S4.SS1.p2.5.m1.1.2.3.2.2" xref="S4.SS1.p2.5.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m1.1b"><apply id="S4.SS1.p2.5.m1.1.2.cmml" xref="S4.SS1.p2.5.m1.1.2"><times id="S4.SS1.p2.5.m1.1.2.1.cmml" xref="S4.SS1.p2.5.m1.1.2.1"></times><ci id="S4.SS1.p2.5.m1.1.2.2.cmml" xref="S4.SS1.p2.5.m1.1.2.2">ğ´</ci><ci id="S4.SS1.p2.5.m1.1.1.cmml" xref="S4.SS1.p2.5.m1.1.1">ğ£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m1.1c">A(\mathbf{j})</annotation></semantics></math>.
We refer to this architecture as <span id="S4.SS1.p2.5.1" class="ltx_text ltx_font_typewriter">command input</span>.
It is applicable to both continuous and discrete commands of arbitrary dimensionality.
However, the network is not forced to take the commands into account, which can lead to suboptimal performance in practice.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.5" class="ltx_p">We therefore designed an alternative architecture, shown in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ IV Methodology â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b).
The image and measurement modules are as described above, but the command module is removed. Instead, we assume a discrete set of commands <math id="S4.SS1.p3.1.m1.3" class="ltx_Math" alttext="{\mathcal{C}=\{\mathbf{c}^{0},\ldots,\mathbf{c}^{K}\}}" display="inline"><semantics id="S4.SS1.p3.1.m1.3a"><mrow id="S4.SS1.p3.1.m1.3.3" xref="S4.SS1.p3.1.m1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.1.m1.3.3.4" xref="S4.SS1.p3.1.m1.3.3.4.cmml">ğ’</mi><mo id="S4.SS1.p3.1.m1.3.3.3" xref="S4.SS1.p3.1.m1.3.3.3.cmml">=</mo><mrow id="S4.SS1.p3.1.m1.3.3.2.2" xref="S4.SS1.p3.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.3.3.2.2.3" xref="S4.SS1.p3.1.m1.3.3.2.3.cmml">{</mo><msup id="S4.SS1.p3.1.m1.2.2.1.1.1" xref="S4.SS1.p3.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.2.2.1.1.1.2" xref="S4.SS1.p3.1.m1.2.2.1.1.1.2.cmml">ğœ</mi><mn id="S4.SS1.p3.1.m1.2.2.1.1.1.3" xref="S4.SS1.p3.1.m1.2.2.1.1.1.3.cmml">0</mn></msup><mo id="S4.SS1.p3.1.m1.3.3.2.2.4" xref="S4.SS1.p3.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">â€¦</mi><mo id="S4.SS1.p3.1.m1.3.3.2.2.5" xref="S4.SS1.p3.1.m1.3.3.2.3.cmml">,</mo><msup id="S4.SS1.p3.1.m1.3.3.2.2.2" xref="S4.SS1.p3.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS1.p3.1.m1.3.3.2.2.2.2" xref="S4.SS1.p3.1.m1.3.3.2.2.2.2.cmml">ğœ</mi><mi id="S4.SS1.p3.1.m1.3.3.2.2.2.3" xref="S4.SS1.p3.1.m1.3.3.2.2.2.3.cmml">K</mi></msup><mo stretchy="false" id="S4.SS1.p3.1.m1.3.3.2.2.6" xref="S4.SS1.p3.1.m1.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.3b"><apply id="S4.SS1.p3.1.m1.3.3.cmml" xref="S4.SS1.p3.1.m1.3.3"><eq id="S4.SS1.p3.1.m1.3.3.3.cmml" xref="S4.SS1.p3.1.m1.3.3.3"></eq><ci id="S4.SS1.p3.1.m1.3.3.4.cmml" xref="S4.SS1.p3.1.m1.3.3.4">ğ’</ci><set id="S4.SS1.p3.1.m1.3.3.2.3.cmml" xref="S4.SS1.p3.1.m1.3.3.2.2"><apply id="S4.SS1.p3.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.p3.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.2.2.1.1.1">superscript</csymbol><ci id="S4.SS1.p3.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.2.2.1.1.1.2">ğœ</ci><cn type="integer" id="S4.SS1.p3.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.2.2.1.1.1.3">0</cn></apply><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">â€¦</ci><apply id="S4.SS1.p3.1.m1.3.3.2.2.2.cmml" xref="S4.SS1.p3.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS1.p3.1.m1.3.3.2.2.2">superscript</csymbol><ci id="S4.SS1.p3.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS1.p3.1.m1.3.3.2.2.2.2">ğœ</ci><ci id="S4.SS1.p3.1.m1.3.3.2.2.2.3.cmml" xref="S4.SS1.p3.1.m1.3.3.2.2.2.3">ğ¾</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.3c">{\mathcal{C}=\{\mathbf{c}^{0},\ldots,\mathbf{c}^{K}\}}</annotation></semantics></math> (including a default command <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{c}^{0}" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><msup id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">ğœ</mi><mn id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml">0</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">superscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">ğœ</ci><cn type="integer" id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\mathbf{c}^{0}</annotation></semantics></math> corresponding to no specific command given) and introduce a specialist branch <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="A^{i}" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><msup id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mi id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">A</mi><mi id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">superscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">ğ´</ci><ci id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">A^{i}</annotation></semantics></math> for each of the commands <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{c}^{i}" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><msup id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><mi id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2.cmml">ğœ</mi><mi id="S4.SS1.p3.4.m4.1.1.3" xref="S4.SS1.p3.4.m4.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">superscript</csymbol><ci id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2">ğœ</ci><ci id="S4.SS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">\mathbf{c}^{i}</annotation></semantics></math>.
The command <math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{c}" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mi id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml">ğœ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><ci id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">\mathbf{c}</annotation></semantics></math> acts as a switch that selects which branch is used at any given time.
The output of the network is thus</p>
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.5" class="ltx_Math" alttext="F(\mathbf{i},\mathbf{m},\mathbf{c}^{i})=A^{i}(J(\mathbf{i},\mathbf{m}))." display="block"><semantics id="S4.E5.m1.5a"><mrow id="S4.E5.m1.5.5.1" xref="S4.E5.m1.5.5.1.1.cmml"><mrow id="S4.E5.m1.5.5.1.1" xref="S4.E5.m1.5.5.1.1.cmml"><mrow id="S4.E5.m1.5.5.1.1.1" xref="S4.E5.m1.5.5.1.1.1.cmml"><mi id="S4.E5.m1.5.5.1.1.1.3" xref="S4.E5.m1.5.5.1.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.5.5.1.1.1.2" xref="S4.E5.m1.5.5.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E5.m1.5.5.1.1.1.1.1" xref="S4.E5.m1.5.5.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E5.m1.5.5.1.1.1.1.1.2" xref="S4.E5.m1.5.5.1.1.1.1.2.cmml">(</mo><mi id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml">ğ¢</mi><mo id="S4.E5.m1.5.5.1.1.1.1.1.3" xref="S4.E5.m1.5.5.1.1.1.1.2.cmml">,</mo><mi id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml">ğ¦</mi><mo id="S4.E5.m1.5.5.1.1.1.1.1.4" xref="S4.E5.m1.5.5.1.1.1.1.2.cmml">,</mo><msup id="S4.E5.m1.5.5.1.1.1.1.1.1" xref="S4.E5.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.5.5.1.1.1.1.1.1.2" xref="S4.E5.m1.5.5.1.1.1.1.1.1.2.cmml">ğœ</mi><mi id="S4.E5.m1.5.5.1.1.1.1.1.1.3" xref="S4.E5.m1.5.5.1.1.1.1.1.1.3.cmml">i</mi></msup><mo stretchy="false" id="S4.E5.m1.5.5.1.1.1.1.1.5" xref="S4.E5.m1.5.5.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.5.5.1.1.3" xref="S4.E5.m1.5.5.1.1.3.cmml">=</mo><mrow id="S4.E5.m1.5.5.1.1.2" xref="S4.E5.m1.5.5.1.1.2.cmml"><msup id="S4.E5.m1.5.5.1.1.2.3" xref="S4.E5.m1.5.5.1.1.2.3.cmml"><mi id="S4.E5.m1.5.5.1.1.2.3.2" xref="S4.E5.m1.5.5.1.1.2.3.2.cmml">A</mi><mi id="S4.E5.m1.5.5.1.1.2.3.3" xref="S4.E5.m1.5.5.1.1.2.3.3.cmml">i</mi></msup><mo lspace="0em" rspace="0em" id="S4.E5.m1.5.5.1.1.2.2" xref="S4.E5.m1.5.5.1.1.2.2.cmml">â€‹</mo><mrow id="S4.E5.m1.5.5.1.1.2.1.1" xref="S4.E5.m1.5.5.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.5.5.1.1.2.1.1.2" xref="S4.E5.m1.5.5.1.1.2.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.5.5.1.1.2.1.1.1" xref="S4.E5.m1.5.5.1.1.2.1.1.1.cmml"><mi id="S4.E5.m1.5.5.1.1.2.1.1.1.2" xref="S4.E5.m1.5.5.1.1.2.1.1.1.2.cmml">J</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.5.5.1.1.2.1.1.1.1" xref="S4.E5.m1.5.5.1.1.2.1.1.1.1.cmml">â€‹</mo><mrow id="S4.E5.m1.5.5.1.1.2.1.1.1.3.2" xref="S4.E5.m1.5.5.1.1.2.1.1.1.3.1.cmml"><mo stretchy="false" id="S4.E5.m1.5.5.1.1.2.1.1.1.3.2.1" xref="S4.E5.m1.5.5.1.1.2.1.1.1.3.1.cmml">(</mo><mi id="S4.E5.m1.3.3" xref="S4.E5.m1.3.3.cmml">ğ¢</mi><mo id="S4.E5.m1.5.5.1.1.2.1.1.1.3.2.2" xref="S4.E5.m1.5.5.1.1.2.1.1.1.3.1.cmml">,</mo><mi id="S4.E5.m1.4.4" xref="S4.E5.m1.4.4.cmml">ğ¦</mi><mo stretchy="false" id="S4.E5.m1.5.5.1.1.2.1.1.1.3.2.3" xref="S4.E5.m1.5.5.1.1.2.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E5.m1.5.5.1.1.2.1.1.3" xref="S4.E5.m1.5.5.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E5.m1.5.5.1.2" xref="S4.E5.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.5b"><apply id="S4.E5.m1.5.5.1.1.cmml" xref="S4.E5.m1.5.5.1"><eq id="S4.E5.m1.5.5.1.1.3.cmml" xref="S4.E5.m1.5.5.1.1.3"></eq><apply id="S4.E5.m1.5.5.1.1.1.cmml" xref="S4.E5.m1.5.5.1.1.1"><times id="S4.E5.m1.5.5.1.1.1.2.cmml" xref="S4.E5.m1.5.5.1.1.1.2"></times><ci id="S4.E5.m1.5.5.1.1.1.3.cmml" xref="S4.E5.m1.5.5.1.1.1.3">ğ¹</ci><vector id="S4.E5.m1.5.5.1.1.1.1.2.cmml" xref="S4.E5.m1.5.5.1.1.1.1.1"><ci id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1">ğ¢</ci><ci id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2">ğ¦</ci><apply id="S4.E5.m1.5.5.1.1.1.1.1.1.cmml" xref="S4.E5.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.5.5.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E5.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.5.5.1.1.1.1.1.1.2">ğœ</ci><ci id="S4.E5.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.5.5.1.1.1.1.1.1.3">ğ‘–</ci></apply></vector></apply><apply id="S4.E5.m1.5.5.1.1.2.cmml" xref="S4.E5.m1.5.5.1.1.2"><times id="S4.E5.m1.5.5.1.1.2.2.cmml" xref="S4.E5.m1.5.5.1.1.2.2"></times><apply id="S4.E5.m1.5.5.1.1.2.3.cmml" xref="S4.E5.m1.5.5.1.1.2.3"><csymbol cd="ambiguous" id="S4.E5.m1.5.5.1.1.2.3.1.cmml" xref="S4.E5.m1.5.5.1.1.2.3">superscript</csymbol><ci id="S4.E5.m1.5.5.1.1.2.3.2.cmml" xref="S4.E5.m1.5.5.1.1.2.3.2">ğ´</ci><ci id="S4.E5.m1.5.5.1.1.2.3.3.cmml" xref="S4.E5.m1.5.5.1.1.2.3.3">ğ‘–</ci></apply><apply id="S4.E5.m1.5.5.1.1.2.1.1.1.cmml" xref="S4.E5.m1.5.5.1.1.2.1.1"><times id="S4.E5.m1.5.5.1.1.2.1.1.1.1.cmml" xref="S4.E5.m1.5.5.1.1.2.1.1.1.1"></times><ci id="S4.E5.m1.5.5.1.1.2.1.1.1.2.cmml" xref="S4.E5.m1.5.5.1.1.2.1.1.1.2">ğ½</ci><interval closure="open" id="S4.E5.m1.5.5.1.1.2.1.1.1.3.1.cmml" xref="S4.E5.m1.5.5.1.1.2.1.1.1.3.2"><ci id="S4.E5.m1.3.3.cmml" xref="S4.E5.m1.3.3">ğ¢</ci><ci id="S4.E5.m1.4.4.cmml" xref="S4.E5.m1.4.4">ğ¦</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.5c">F(\mathbf{i},\mathbf{m},\mathbf{c}^{i})=A^{i}(J(\mathbf{i},\mathbf{m})).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p3.6" class="ltx_p">We refer to this architecture as <span id="S4.SS1.p3.6.1" class="ltx_text ltx_font_typewriter">branched</span>.
The branches <math id="S4.SS1.p3.6.m1.1" class="ltx_Math" alttext="A^{i}" display="inline"><semantics id="S4.SS1.p3.6.m1.1a"><msup id="S4.SS1.p3.6.m1.1.1" xref="S4.SS1.p3.6.m1.1.1.cmml"><mi id="S4.SS1.p3.6.m1.1.1.2" xref="S4.SS1.p3.6.m1.1.1.2.cmml">A</mi><mi id="S4.SS1.p3.6.m1.1.1.3" xref="S4.SS1.p3.6.m1.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m1.1b"><apply id="S4.SS1.p3.6.m1.1.1.cmml" xref="S4.SS1.p3.6.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.6.m1.1.1.1.cmml" xref="S4.SS1.p3.6.m1.1.1">superscript</csymbol><ci id="S4.SS1.p3.6.m1.1.1.2.cmml" xref="S4.SS1.p3.6.m1.1.1.2">ğ´</ci><ci id="S4.SS1.p3.6.m1.1.1.3.cmml" xref="S4.SS1.p3.6.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m1.1c">A^{i}</annotation></semantics></math> are forced to learn sub-policies that correspond to different commands. In a driving scenario, one module might specialize in lane following, another in right turns, and a third in left turns. All modules share the perception stream.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Network Details</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.8" class="ltx_p">For all controllers, the observation <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{o}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">ğ¨</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\mathbf{o}</annotation></semantics></math> is the currently observed image at <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="200\mathbin{\!\times\!}88" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">200</mn><mo lspace="0.052em" rspace="0.052em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">88</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">200</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">200\mathbin{\!\times\!}88</annotation></semantics></math> pixel resolution. For the measurement <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{m}" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">ğ¦</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">ğ¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\mathbf{m}</annotation></semantics></math>, we used the current speed of the car, if available (in the physical system the speed estimates were very noisy and we refrained from using them). All networks are composed of modules with identical architectures (e.g., the ConvNet architecture is the same in all conditions). The differences are in the configuration of modules and branches as can be seen in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ IV Methodology â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The image module consists ofÂ 8 convolutional andÂ 2 fully connected layers. The convolution kernel size isÂ 5 in the first layer andÂ 3 in the following layers. The first, third, and fifth convolutional layers have a stride ofÂ 2. The number of channels increases from <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mn id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><cn type="integer" id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">32</annotation></semantics></math> in the first convolutional layer to <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mn id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><cn type="integer" id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">256</annotation></semantics></math> in the last. Fully-connected layers contain <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mn id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><cn type="integer" id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">512</annotation></semantics></math> units each. All modules with the exception of the image module are implemented as standard multilayer perceptrons. We used ReLU nonlinearities after all hidden layers, performed batch normalization after convolutional layers, applied <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mrow id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml"><mn id="S4.SS2.p1.7.m7.1.1.2" xref="S4.SS2.p1.7.m7.1.1.2.cmml">50</mn><mo id="S4.SS2.p1.7.m7.1.1.1" xref="S4.SS2.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><apply id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"><csymbol cd="latexml" id="S4.SS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.p1.7.m7.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">50\%</annotation></semantics></math> dropout after fully-connected hidden layers, and used <math id="S4.SS2.p1.8.m8.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.SS2.p1.8.m8.1a"><mrow id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml"><mn id="S4.SS2.p1.8.m8.1.1.2" xref="S4.SS2.p1.8.m8.1.1.2.cmml">20</mn><mo id="S4.SS2.p1.8.m8.1.1.1" xref="S4.SS2.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><apply id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1"><csymbol cd="latexml" id="S4.SS2.p1.8.m8.1.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.8.m8.1.1.2.cmml" xref="S4.SS2.p1.8.m8.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">20\%</annotation></semantics></math> dropout after convolutional layers.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p">Actions are two-dimensional vectors that collate steering angle and acceleration: <math id="S4.SS2.p2.1.m1.2" class="ltx_Math" alttext="{\mathbf{a}=\left\langle s,a\right\rangle}" display="inline"><semantics id="S4.SS2.p2.1.m1.2a"><mrow id="S4.SS2.p2.1.m1.2.3" xref="S4.SS2.p2.1.m1.2.3.cmml"><mi id="S4.SS2.p2.1.m1.2.3.2" xref="S4.SS2.p2.1.m1.2.3.2.cmml">ğš</mi><mo id="S4.SS2.p2.1.m1.2.3.1" xref="S4.SS2.p2.1.m1.2.3.1.cmml">=</mo><mrow id="S4.SS2.p2.1.m1.2.3.3.2" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml"><mo id="S4.SS2.p2.1.m1.2.3.3.2.1" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">âŸ¨</mo><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">s</mi><mo id="S4.SS2.p2.1.m1.2.3.3.2.2" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS2.p2.1.m1.2.2" xref="S4.SS2.p2.1.m1.2.2.cmml">a</mi><mo id="S4.SS2.p2.1.m1.2.3.3.2.3" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">âŸ©</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.2b"><apply id="S4.SS2.p2.1.m1.2.3.cmml" xref="S4.SS2.p2.1.m1.2.3"><eq id="S4.SS2.p2.1.m1.2.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.1"></eq><ci id="S4.SS2.p2.1.m1.2.3.2.cmml" xref="S4.SS2.p2.1.m1.2.3.2">ğš</ci><list id="S4.SS2.p2.1.m1.2.3.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.3.2"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ğ‘ </ci><ci id="S4.SS2.p2.1.m1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.2c">{\mathbf{a}=\left\langle s,a\right\rangle}</annotation></semantics></math>. Given a predicted action <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{a}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">ğš</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">ğš</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\mathbf{a}</annotation></semantics></math> and a ground truth action <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{a}_{\text{gt}}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><msub id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">ğš</mi><mtext id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3a.cmml">gt</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">ğš</ci><ci id="S4.SS2.p2.3.m3.1.1.3a.cmml" xref="S4.SS2.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">gt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\mathbf{a}_{\text{gt}}</annotation></semantics></math>, the per-sample loss function is defined as</p>
<table id="S8.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S4.E6">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex1.m1.2" class="ltx_Math" alttext="\displaystyle\ell(\mathbf{a},\mathbf{a}_{\text{gt}})" display="inline"><semantics id="S4.Ex1.m1.2a"><mrow id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml"><mi mathvariant="normal" id="S4.Ex1.m1.2.2.3" xref="S4.Ex1.m1.2.2.3.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.2.2.2" xref="S4.Ex1.m1.2.2.2.cmml">â€‹</mo><mrow id="S4.Ex1.m1.2.2.1.1" xref="S4.Ex1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S4.Ex1.m1.2.2.1.1.2" xref="S4.Ex1.m1.2.2.1.2.cmml">(</mo><mi id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml">ğš</mi><mo id="S4.Ex1.m1.2.2.1.1.3" xref="S4.Ex1.m1.2.2.1.2.cmml">,</mo><msub id="S4.Ex1.m1.2.2.1.1.1" xref="S4.Ex1.m1.2.2.1.1.1.cmml"><mi id="S4.Ex1.m1.2.2.1.1.1.2" xref="S4.Ex1.m1.2.2.1.1.1.2.cmml">ğš</mi><mtext id="S4.Ex1.m1.2.2.1.1.1.3" xref="S4.Ex1.m1.2.2.1.1.1.3a.cmml">gt</mtext></msub><mo stretchy="false" id="S4.Ex1.m1.2.2.1.1.4" xref="S4.Ex1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.2b"><apply id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2"><times id="S4.Ex1.m1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2"></times><ci id="S4.Ex1.m1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.3">â„“</ci><interval closure="open" id="S4.Ex1.m1.2.2.1.2.cmml" xref="S4.Ex1.m1.2.2.1.1"><ci id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1">ğš</ci><apply id="S4.Ex1.m1.2.2.1.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.1.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.Ex1.m1.2.2.1.1.1.2.cmml" xref="S4.Ex1.m1.2.2.1.1.1.2">ğš</ci><ci id="S4.Ex1.m1.2.2.1.1.1.3a.cmml" xref="S4.Ex1.m1.2.2.1.1.1.3"><mtext mathsize="70%" id="S4.Ex1.m1.2.2.1.1.1.3.cmml" xref="S4.Ex1.m1.2.2.1.1.1.3">gt</mtext></ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.2c">\displaystyle\ell(\mathbf{a},\mathbf{a}_{\text{gt}})</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S4.Ex1.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S4.Ex1.m2.1a"><mo id="S4.Ex1.m2.1.1" xref="S4.Ex1.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.Ex1.m2.1b"><eq id="S4.Ex1.m2.1.1.cmml" xref="S4.Ex1.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.Ex1.m3.4" class="ltx_Math" alttext="\displaystyle\ell\big{(}\left\langle s,a\right\rangle,\,\left\langle s_{\text{gt}},a_{\text{gt}}\right\rangle\big{)}" display="inline"><semantics id="S4.Ex1.m3.4a"><mrow id="S4.Ex1.m3.4.4" xref="S4.Ex1.m3.4.4.cmml"><mi mathvariant="normal" id="S4.Ex1.m3.4.4.4" xref="S4.Ex1.m3.4.4.4.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m3.4.4.3" xref="S4.Ex1.m3.4.4.3.cmml">â€‹</mo><mrow id="S4.Ex1.m3.4.4.2.2" xref="S4.Ex1.m3.4.4.2.3.cmml"><mo maxsize="120%" minsize="120%" id="S4.Ex1.m3.4.4.2.2.3" xref="S4.Ex1.m3.4.4.2.3.cmml">(</mo><mrow id="S4.Ex1.m3.3.3.1.1.1.2" xref="S4.Ex1.m3.3.3.1.1.1.1.cmml"><mo id="S4.Ex1.m3.3.3.1.1.1.2.1" xref="S4.Ex1.m3.3.3.1.1.1.1.cmml">âŸ¨</mo><mi id="S4.Ex1.m3.1.1" xref="S4.Ex1.m3.1.1.cmml">s</mi><mo id="S4.Ex1.m3.3.3.1.1.1.2.2" xref="S4.Ex1.m3.3.3.1.1.1.1.cmml">,</mo><mi id="S4.Ex1.m3.2.2" xref="S4.Ex1.m3.2.2.cmml">a</mi><mo id="S4.Ex1.m3.3.3.1.1.1.2.3" xref="S4.Ex1.m3.3.3.1.1.1.1.cmml">âŸ©</mo></mrow><mo rspace="0.337em" id="S4.Ex1.m3.4.4.2.2.4" xref="S4.Ex1.m3.4.4.2.3.cmml">,</mo><mrow id="S4.Ex1.m3.4.4.2.2.2.2" xref="S4.Ex1.m3.4.4.2.2.2.3.cmml"><mo id="S4.Ex1.m3.4.4.2.2.2.2.3" xref="S4.Ex1.m3.4.4.2.2.2.3.cmml">âŸ¨</mo><msub id="S4.Ex1.m3.4.4.2.2.2.1.1" xref="S4.Ex1.m3.4.4.2.2.2.1.1.cmml"><mi id="S4.Ex1.m3.4.4.2.2.2.1.1.2" xref="S4.Ex1.m3.4.4.2.2.2.1.1.2.cmml">s</mi><mtext id="S4.Ex1.m3.4.4.2.2.2.1.1.3" xref="S4.Ex1.m3.4.4.2.2.2.1.1.3a.cmml">gt</mtext></msub><mo id="S4.Ex1.m3.4.4.2.2.2.2.4" xref="S4.Ex1.m3.4.4.2.2.2.3.cmml">,</mo><msub id="S4.Ex1.m3.4.4.2.2.2.2.2" xref="S4.Ex1.m3.4.4.2.2.2.2.2.cmml"><mi id="S4.Ex1.m3.4.4.2.2.2.2.2.2" xref="S4.Ex1.m3.4.4.2.2.2.2.2.2.cmml">a</mi><mtext id="S4.Ex1.m3.4.4.2.2.2.2.2.3" xref="S4.Ex1.m3.4.4.2.2.2.2.2.3a.cmml">gt</mtext></msub><mo id="S4.Ex1.m3.4.4.2.2.2.2.5" xref="S4.Ex1.m3.4.4.2.2.2.3.cmml">âŸ©</mo></mrow><mo maxsize="120%" minsize="120%" id="S4.Ex1.m3.4.4.2.2.5" xref="S4.Ex1.m3.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m3.4b"><apply id="S4.Ex1.m3.4.4.cmml" xref="S4.Ex1.m3.4.4"><times id="S4.Ex1.m3.4.4.3.cmml" xref="S4.Ex1.m3.4.4.3"></times><ci id="S4.Ex1.m3.4.4.4.cmml" xref="S4.Ex1.m3.4.4.4">â„“</ci><interval closure="open" id="S4.Ex1.m3.4.4.2.3.cmml" xref="S4.Ex1.m3.4.4.2.2"><list id="S4.Ex1.m3.3.3.1.1.1.1.cmml" xref="S4.Ex1.m3.3.3.1.1.1.2"><ci id="S4.Ex1.m3.1.1.cmml" xref="S4.Ex1.m3.1.1">ğ‘ </ci><ci id="S4.Ex1.m3.2.2.cmml" xref="S4.Ex1.m3.2.2">ğ‘</ci></list><list id="S4.Ex1.m3.4.4.2.2.2.3.cmml" xref="S4.Ex1.m3.4.4.2.2.2.2"><apply id="S4.Ex1.m3.4.4.2.2.2.1.1.cmml" xref="S4.Ex1.m3.4.4.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m3.4.4.2.2.2.1.1.1.cmml" xref="S4.Ex1.m3.4.4.2.2.2.1.1">subscript</csymbol><ci id="S4.Ex1.m3.4.4.2.2.2.1.1.2.cmml" xref="S4.Ex1.m3.4.4.2.2.2.1.1.2">ğ‘ </ci><ci id="S4.Ex1.m3.4.4.2.2.2.1.1.3a.cmml" xref="S4.Ex1.m3.4.4.2.2.2.1.1.3"><mtext mathsize="70%" id="S4.Ex1.m3.4.4.2.2.2.1.1.3.cmml" xref="S4.Ex1.m3.4.4.2.2.2.1.1.3">gt</mtext></ci></apply><apply id="S4.Ex1.m3.4.4.2.2.2.2.2.cmml" xref="S4.Ex1.m3.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m3.4.4.2.2.2.2.2.1.cmml" xref="S4.Ex1.m3.4.4.2.2.2.2.2">subscript</csymbol><ci id="S4.Ex1.m3.4.4.2.2.2.2.2.2.cmml" xref="S4.Ex1.m3.4.4.2.2.2.2.2.2">ğ‘</ci><ci id="S4.Ex1.m3.4.4.2.2.2.2.2.3a.cmml" xref="S4.Ex1.m3.4.4.2.2.2.2.2.3"><mtext mathsize="70%" id="S4.Ex1.m3.4.4.2.2.2.2.2.3.cmml" xref="S4.Ex1.m3.4.4.2.2.2.2.2.3">gt</mtext></ci></apply></list></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m3.4c">\displaystyle\ell\big{(}\left\langle s,a\right\rangle,\,\left\langle s_{\text{gt}},a_{\text{gt}}\right\rangle\big{)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr>
<tr class="ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S4.E6.m1.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S4.E6.m1.1a"><mo id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.E6.m1.1b"><eq id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E6.m2.1" class="ltx_Math" alttext="\displaystyle\left\lVert s-s_{\text{gt}}\right\rVert^{2}+\lambda_{a}\left\lVert a-a_{\text{gt}}\right\rVert^{2}." display="inline"><semantics id="S4.E6.m2.1a"><mrow id="S4.E6.m2.1.1.1" xref="S4.E6.m2.1.1.1.1.cmml"><mrow id="S4.E6.m2.1.1.1.1" xref="S4.E6.m2.1.1.1.1.cmml"><msup id="S4.E6.m2.1.1.1.1.1" xref="S4.E6.m2.1.1.1.1.1.cmml"><mrow id="S4.E6.m2.1.1.1.1.1.1.1" xref="S4.E6.m2.1.1.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" stretchy="true" id="S4.E6.m2.1.1.1.1.1.1.1.2" xref="S4.E6.m2.1.1.1.1.1.1.2.1.cmml">âˆ¥</mo><mrow id="S4.E6.m2.1.1.1.1.1.1.1.1" xref="S4.E6.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E6.m2.1.1.1.1.1.1.1.1.2" xref="S4.E6.m2.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo id="S4.E6.m2.1.1.1.1.1.1.1.1.1" xref="S4.E6.m2.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E6.m2.1.1.1.1.1.1.1.1.3" xref="S4.E6.m2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E6.m2.1.1.1.1.1.1.1.1.3.2" xref="S4.E6.m2.1.1.1.1.1.1.1.1.3.2.cmml">s</mi><mtext id="S4.E6.m2.1.1.1.1.1.1.1.1.3.3" xref="S4.E6.m2.1.1.1.1.1.1.1.1.3.3a.cmml">gt</mtext></msub></mrow><mo fence="true" lspace="0em" stretchy="true" id="S4.E6.m2.1.1.1.1.1.1.1.3" xref="S4.E6.m2.1.1.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S4.E6.m2.1.1.1.1.1.3" xref="S4.E6.m2.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S4.E6.m2.1.1.1.1.3" xref="S4.E6.m2.1.1.1.1.3.cmml">+</mo><mrow id="S4.E6.m2.1.1.1.1.2" xref="S4.E6.m2.1.1.1.1.2.cmml"><msub id="S4.E6.m2.1.1.1.1.2.3" xref="S4.E6.m2.1.1.1.1.2.3.cmml"><mi id="S4.E6.m2.1.1.1.1.2.3.2" xref="S4.E6.m2.1.1.1.1.2.3.2.cmml">Î»</mi><mi id="S4.E6.m2.1.1.1.1.2.3.3" xref="S4.E6.m2.1.1.1.1.2.3.3.cmml">a</mi></msub><mo lspace="0em" rspace="0em" id="S4.E6.m2.1.1.1.1.2.2" xref="S4.E6.m2.1.1.1.1.2.2.cmml">â€‹</mo><msup id="S4.E6.m2.1.1.1.1.2.1" xref="S4.E6.m2.1.1.1.1.2.1.cmml"><mrow id="S4.E6.m2.1.1.1.1.2.1.1.1" xref="S4.E6.m2.1.1.1.1.2.1.1.2.cmml"><mo fence="true" rspace="0em" stretchy="true" id="S4.E6.m2.1.1.1.1.2.1.1.1.2" xref="S4.E6.m2.1.1.1.1.2.1.1.2.1.cmml">âˆ¥</mo><mrow id="S4.E6.m2.1.1.1.1.2.1.1.1.1" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.cmml"><mi id="S4.E6.m2.1.1.1.1.2.1.1.1.1.2" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.2.cmml">a</mi><mo id="S4.E6.m2.1.1.1.1.2.1.1.1.1.1" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E6.m2.1.1.1.1.2.1.1.1.1.3" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.cmml"><mi id="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.2" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.2.cmml">a</mi><mtext id="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.3" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.3a.cmml">gt</mtext></msub></mrow><mo fence="true" lspace="0em" rspace="0em" stretchy="true" id="S4.E6.m2.1.1.1.1.2.1.1.1.3" xref="S4.E6.m2.1.1.1.1.2.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S4.E6.m2.1.1.1.1.2.1.3" xref="S4.E6.m2.1.1.1.1.2.1.3.cmml">2</mn></msup></mrow></mrow><mo lspace="0em" id="S4.E6.m2.1.1.1.2" xref="S4.E6.m2.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m2.1b"><apply id="S4.E6.m2.1.1.1.1.cmml" xref="S4.E6.m2.1.1.1"><plus id="S4.E6.m2.1.1.1.1.3.cmml" xref="S4.E6.m2.1.1.1.1.3"></plus><apply id="S4.E6.m2.1.1.1.1.1.cmml" xref="S4.E6.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m2.1.1.1.1.1.2.cmml" xref="S4.E6.m2.1.1.1.1.1">superscript</csymbol><apply id="S4.E6.m2.1.1.1.1.1.1.2.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E6.m2.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S4.E6.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.1"><minus id="S4.E6.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E6.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.1.2">ğ‘ </ci><apply id="S4.E6.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E6.m2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.1.3.2">ğ‘ </ci><ci id="S4.E6.m2.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S4.E6.m2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m2.1.1.1.1.1.1.1.1.3.3">gt</mtext></ci></apply></apply></apply><cn type="integer" id="S4.E6.m2.1.1.1.1.1.3.cmml" xref="S4.E6.m2.1.1.1.1.1.3">2</cn></apply><apply id="S4.E6.m2.1.1.1.1.2.cmml" xref="S4.E6.m2.1.1.1.1.2"><times id="S4.E6.m2.1.1.1.1.2.2.cmml" xref="S4.E6.m2.1.1.1.1.2.2"></times><apply id="S4.E6.m2.1.1.1.1.2.3.cmml" xref="S4.E6.m2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E6.m2.1.1.1.1.2.3.1.cmml" xref="S4.E6.m2.1.1.1.1.2.3">subscript</csymbol><ci id="S4.E6.m2.1.1.1.1.2.3.2.cmml" xref="S4.E6.m2.1.1.1.1.2.3.2">ğœ†</ci><ci id="S4.E6.m2.1.1.1.1.2.3.3.cmml" xref="S4.E6.m2.1.1.1.1.2.3.3">ğ‘</ci></apply><apply id="S4.E6.m2.1.1.1.1.2.1.cmml" xref="S4.E6.m2.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S4.E6.m2.1.1.1.1.2.1.2.cmml" xref="S4.E6.m2.1.1.1.1.2.1">superscript</csymbol><apply id="S4.E6.m2.1.1.1.1.2.1.1.2.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1"><csymbol cd="latexml" id="S4.E6.m2.1.1.1.1.2.1.1.2.1.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S4.E6.m2.1.1.1.1.2.1.1.1.1.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1"><minus id="S4.E6.m2.1.1.1.1.2.1.1.1.1.1.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.1"></minus><ci id="S4.E6.m2.1.1.1.1.2.1.1.1.1.2.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.2">ğ‘</ci><apply id="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.1.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.3">subscript</csymbol><ci id="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.2.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.2">ğ‘</ci><ci id="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.3a.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.3"><mtext mathsize="70%" id="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.3.cmml" xref="S4.E6.m2.1.1.1.1.2.1.1.1.1.3.3">gt</mtext></ci></apply></apply></apply><cn type="integer" id="S4.E6.m2.1.1.1.1.2.1.3.cmml" xref="S4.E6.m2.1.1.1.1.2.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m2.1c">\displaystyle\left\lVert s-s_{\text{gt}}\right\rVert^{2}+\lambda_{a}\left\lVert a-a_{\text{gt}}\right\rVert^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">All models were trained using the Adam solverÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> with minibatches of <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">120</annotation></semantics></math> samples and an initial learning rate of <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="0.0002" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">0.0002</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn type="float" id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">0.0002</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">0.0002</annotation></semantics></math>. For the command-conditional models, minibatches were constructed to contain an equal number of samples with each command.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Training Data Distribution</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">When performing imitation learning, a key decision is how to collect the training data. The simplest solution is to collect trajectories from natural demonstrations of an expert performing the task.
This typically leads to unstable policies, since a model that is only trained on expert trajectories may not learn to recover from disturbance or driftÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">To overcome this problem, training data should include observations of recoveries from perturbations.
In DAggerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, the expert remains in the loop during the training of the controller: the controller is iteratively tested and samples from the obtained trajectories are re-labeled by the expert.
In the system of Bojarski et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, the vehicle is instrumented to record from three cameras simultaneously: one facing forward and the other two shifted to the left and to the right. Recordings from the shifted cameras, as well as intermediate synthetically reprojected views, are added to the training set â€“ with appropriately adjusted control signals â€“ to simulate recovery from drift.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">In this paper we adopt a three-camera setup inspired by Bojarski et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
However, we have found that the policies learned with this setup are not sufficiently robust.
Therefore, to further augment the training dataset, we record some of the data while injecting noise into the expertâ€™s control signal and letting the expert recover from these perturbations.
This is akin to the recent approach of Laskey et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, but instead of i.i.d. noise we inject temporally correlated noise designed to simulate gradual drift away from the desired trajectory. An example is shown in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ IV-C Training Data Distribution â€£ IV Methodology â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For training, we use the driverâ€™s corrective response to the injected noise (not the noise itself). This provides the controller with demonstrations of recovery from drift and unexpected disturbances, but does not contaminate the training set with demonstrations of veering away from desired behavior.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<table id="S4.F4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F4.4.4" class="ltx_tr">
<td id="S4.F4.1.1.1" class="ltx_td ltx_align_center">
<table id="S4.F4.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.F4.1.1.1.1.1" class="ltx_tr">
<td id="S4.F4.1.1.1.1.1.1" class="ltx_td ltx_align_center">
<img src="/html/1710.02410/assets/images_compressed/noise.jpg" id="S4.F4.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="341" height="339" alt="Refer to caption">
</td>
</tr>
</table>
</td>
<td id="S4.F4.4.4.4" class="ltx_td ltx_align_center">

<table id="S4.F4.4.4.4.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.F4.2.2.2.1.1" class="ltx_tr">
<td id="S4.F4.2.2.2.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/drift.jpg" id="S4.F4.2.2.2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="106" alt="Refer to caption"></td>
</tr>
<tr id="S4.F4.3.3.3.2.2" class="ltx_tr">
<td id="S4.F4.3.3.3.2.2.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/middle.jpg" id="S4.F4.3.3.3.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="106" alt="Refer to caption"></td>
</tr>
<tr id="S4.F4.4.4.4.3.3" class="ltx_tr">
<td id="S4.F4.4.4.4.3.3.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/return.jpg" id="S4.F4.4.4.4.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="106" alt="Refer to caption"></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Noise injection during data collection. We show a fragment from an actual driving sequence from the training set. The plot on the left shows steering control [rad] versus time [s]. In the plot, the red curve is an injected triangular noise signal, the green curve is the driverâ€™s steering signal, and the blue curve is the steering signal provided to the car, which is the sum of the driverâ€™s control and the noise. Images on the right show the driverâ€™s view at three points in time (trajectories overlaid post-hoc for visualization). Between times 0 and roughly 1.0, the noise produces a drift to the right, as illustrated in image (a). This triggers a human reaction, from 1.0 to 2.5 seconds, illustrated in (b). Finally, the car recovers from the disturbance, as shown in (c). Only the driver-provided signal (green curve on the left) is used for training.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Data Augmentation</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We found data augmentation to be crucial for good generalization. We perform augmentation online during network training. For each image to be presented to the network, we apply a random subset of a set of transformations with randomly sampled magnitudes. Transformations include change in contrast, brightness, and tone, as well as addition of Gaussian blur, Gaussian noise, salt-and-pepper noise, and region dropout (masking out a random set of rectangles in the image, each rectangle taking roughly 1% of image area).
No geometric augmentations such as translation or rotation were applied, since control commands are not invariant to these transformations.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">System Setup</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We evaluated the presented approach in a simulated urban environment and on a physical systemÂ â€“ a 1/5 scale truck.
In both cases, the observations (images) are recorded by one central camera and two lateral cameras rotated by 30 degrees
with respect to the center. The recorded control signal is two-dimensional: steering angle and acceleration. The steering angle is scaled between -1 and 1, with extreme values corresponding to full left and full right, respectively.
The acceleration is also scaled between -1 and 1, where 1 corresponds to full forward acceleration and -1 to full reverse acceleration.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In addition to the observations (images) and actions (control signals), we record commands provided by the driver.
We use a set of four commands: <span id="S5.p2.1.1" class="ltx_text ltx_font_typewriter">continue</span>Â (follow the road), <span id="S5.p2.1.2" class="ltx_text ltx_font_typewriter">left</span>Â (turn left at the next intersection), <span id="S5.p2.1.3" class="ltx_text ltx_font_typewriter">straight</span>Â (go straight at the next intersection), and <span id="S5.p2.1.4" class="ltx_text ltx_font_typewriter">right</span>Â (turn right at the next intersection). In practice, we represent these as one-hot vectors.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">During training data collection, when approaching an intersection the driver uses buttons on a physical steering wheel (when driving in simulation) or on the remote control (when operating the physical truck) to indicate the command corresponding to the intended course of action.
The driver indicates the command when the intended action becomes clear, akin to turn indicators in cars or navigation instructions provided by mapping applications.
This way we collect realistic data that reflects how a higher level planner or a human could direct the system.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Simulated Environment</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We use CARLAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, an urban driving simulator, to corroborate design decisions and evaluate the proposed approach in a dynamic urban environment with traffic.
CARLA is an open-source simulator implemented using Unreal EngineÂ 4. It contains two professionally designed towns with buildings, vegetation, and traffic signs, as well as vehicular and pedestrian traffic. FigureÂ <a href="#S5.F5" title="Figure 5 â€£ V-A Simulated Environment â€£ V System Setup â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides maps and sample views of Town 1, used for training, and Town 2, used exclusively for testing.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<table id="S5.F5.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F5.2.2" class="ltx_tr">
<td id="S5.F5.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/maptown2.jpg" id="S5.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="293" height="173" alt="Refer to caption"></td>
<td id="S5.F5.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/maptown1.jpg" id="S5.F5.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="293" height="173" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.4.4" class="ltx_tr">
<td id="S5.F5.3.3.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/carla_1_view.jpg" id="S5.F5.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="293" height="141" alt="Refer to caption"></td>
<td id="S5.F5.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/carla_0_view.jpg" id="S5.F5.4.4.2.g1" class="ltx_graphics ltx_img_landscape" width="293" height="141" alt="Refer to caption"></td>
</tr>
<tr id="S5.F5.4.5.1" class="ltx_tr">
<td id="S5.F5.4.5.1.1" class="ltx_td ltx_align_center"><span id="S5.F5.4.5.1.1.1" class="ltx_text" style="font-size:90%;">Town 1 (training)</span></td>
<td id="S5.F5.4.5.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F5.4.5.1.2.1" class="ltx_text" style="font-size:90%;">Town 2 (testing)</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Simulated urban environments. Town 1 is used for training (left), Town 2 is used exclusively for testing (right). Map on top, view from onboard camera below. Note the difference in visual style.</figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.2" class="ltx_p">In order to collect training data, a human driver is presented with a first-person view of the environment (center camera) at a resolution of <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="800\mathbin{\!\times\!}600" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mn id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">800</mn><mo lspace="0.052em" rspace="0.052em" id="S5.SS1.p2.1.m1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><times id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2">800</cn><cn type="integer" id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">800\mathbin{\!\times\!}600</annotation></semantics></math> pixels. The driver controls the simulated vehicle using a physical steering wheel and pedals, and provides command input using buttons on the steering wheel.
The driver keeps the car at a speed below <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mn id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><cn type="integer" id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">60</annotation></semantics></math> km/h and strives to avoid collisions with cars and pedestrians, but ignores traffic lights and stop signs.
We record images from the three simulated cameras, along with other measurements such as speed and the position of the car. The images are cropped to remove part of the sky. CARLA also provides extra information such as distance travelled, collisions, and the occurrence of infractions such as drift onto the opposite lane or the sidewalk. This information is used in evaluating different controllers.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Physical System</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The setup of the physical system is shown in FigureÂ <a href="#S5.F6" title="Figure 6 â€£ V-B Physical System â€£ V System Setup â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We equipped an off-the-shelf 1/5 scale truck (Traxxas Maxx) with an embedded computer (Nvidia TX2), three low-cost webcams, a flight controller (Holybro Pixhawk) running the APMRover firmware, and supporting electronics. The TX2 acquires images from the webcams and shares a bidirectional communication channel with the Pixhawk. The Pixhawk receives controls from either the TX2 or a human driver and converts them to low-level PWM signals for the speed controller and steering servo of the truck.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/1710.02410/assets/images_compressed/real_system.jpg" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="371" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Physical system setup. Red/black indicate +/- power wires, green indicates serial data connections, and blue indicates PWM control signals.</figcaption>
</figure>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS1.4.1.1" class="ltx_text">V-B</span>1 </span>Data collection</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">During data collection the truck is driven by a human. The images from all three cameras are synchronized with the control signals and with GPS and IMU data from the Pixhawk, and recorded to disk. The control signals are passed through the TX2 to support noise injection as described in SectionÂ <a href="#S4.SS3" title="IV-C Training Data Distribution â€£ IV Methodology â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>. In addition, routing the control through the TX2 ensures a similar delay in the training data as during test time. For the physical system we use only three command inputs (<span id="S5.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_typewriter">left</span>, <span id="S5.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_typewriter">straight</span>, <span id="S5.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_typewriter">right</span>), since only a three-way switch is available on the remote control.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS2.4.1.1" class="ltx_text">V-B</span>2 </span>Model evaluation</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">At test time the trained model is evaluated on the TX2 in real time. It receives images from the central webcam and commands (<span id="S5.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_typewriter">left</span>, <span id="S5.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_typewriter">straight</span>, <span id="S5.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_typewriter">right</span>) from the remote control. FigureÂ <a href="#S5.F7" title="Figure 7 â€£ V-B2 Model evaluation â€£ V-B Physical System â€£ V System Setup â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(b) shows an example image from the central camera. The network predicts the appropriate controls in an end-to-end fashion based on only the current image and the provided command. The predicted control is forwarded to the Pixhawk, which controls the car accordingly by sending the appropriate PWM signals to the speed controller and steering servo.</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<table id="S5.F7.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F7.3.3" class="ltx_tr">
<td id="S5.F7.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/rccar_fpv_left.jpg" id="S5.F7.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="192" height="144" alt="Refer to caption"></td>
<td id="S5.F7.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/rccar_fpv_center.jpg" id="S5.F7.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="192" height="144" alt="Refer to caption"></td>
<td id="S5.F7.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/rccar_fpv_right.jpg" id="S5.F7.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="192" height="144" alt="Refer to caption"></td>
</tr>
<tr id="S5.F7.3.4.1" class="ltx_tr">
<td id="S5.F7.3.4.1.1" class="ltx_td ltx_align_center"><span id="S5.F7.3.4.1.1.1" class="ltx_text" style="font-size:90%;">(a) Left camera</span></td>
<td id="S5.F7.3.4.1.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S5.F7.3.4.1.2.1" class="ltx_text" style="font-size:90%;">(b) Central camera</span></td>
<td id="S5.F7.3.4.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span id="S5.F7.3.4.1.3.1" class="ltx_text" style="font-size:90%;">(c) Right camera</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Images from the three cameras on the truck. All three cameras are used for training, with appropriately adjusted steering commands. Only the central camera is used at test time.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S6" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Simulated Environment</span>
</h3>

<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS1.SSS1.4.1.1" class="ltx_text">VI-A</span>1 </span>Experimental setup</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">The use of the CARLA simulator enables running the evaluation in an episodic setup.
In each episode, the agent is initialized at a new location and has to drive to a given destination point, given high-level turn commands from a topological planner.
An episode is considered successful if the agent reaches the goal within a fixed time interval.
In addition to success rate, we measured driving quality by recording the average distance travelled without infractions (collisions or veering outside the lane).</p>
</div>
<div id="S6.SS1.SSS1.p2" class="ltx_para">
<p id="S6.SS1.SSS1.p2.1" class="ltx_p">The two CARLA towns used in our experiments are illustrated in FigureÂ <a href="#S5.F5" title="Figure 5 â€£ V-A Simulated Environment â€£ V System Setup â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and in the supplementary video. Town 1 is used for training, Town 2 is used exclusively for testing. For evaluation, we used <math id="S6.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S6.SS1.SSS1.p2.1.m1.1a"><mn id="S6.SS1.SSS1.p2.1.m1.1.1" xref="S6.SS1.SSS1.p2.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p2.1.m1.1b"><cn type="integer" id="S6.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p2.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p2.1.m1.1c">50</annotation></semantics></math> pairs of start and goal locations set at least 1 km apart, in each town.</p>
</div>
<div id="S6.SS1.SSS1.p3" class="ltx_para">
<p id="S6.SS1.SSS1.p3.3" class="ltx_p">Our training dataset comprises <math id="S6.SS1.SSS1.p3.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S6.SS1.SSS1.p3.1.m1.1a"><mn id="S6.SS1.SSS1.p3.1.m1.1.1" xref="S6.SS1.SSS1.p3.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.1.m1.1b"><cn type="integer" id="S6.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.1.m1.1c">2</annotation></semantics></math> hours of human driving in Town 1 of which only <math id="S6.SS1.SSS1.p3.2.m2.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S6.SS1.SSS1.p3.2.m2.1a"><mrow id="S6.SS1.SSS1.p3.2.m2.1.1" xref="S6.SS1.SSS1.p3.2.m2.1.1.cmml"><mn id="S6.SS1.SSS1.p3.2.m2.1.1.2" xref="S6.SS1.SSS1.p3.2.m2.1.1.2.cmml">10</mn><mo id="S6.SS1.SSS1.p3.2.m2.1.1.1" xref="S6.SS1.SSS1.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.2.m2.1b"><apply id="S6.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S6.SS1.SSS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p3.2.m2.1.1.1.cmml" xref="S6.SS1.SSS1.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS1.p3.2.m2.1.1.2.cmml" xref="S6.SS1.SSS1.p3.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.2.m2.1c">10\%</annotation></semantics></math> (roughly <math id="S6.SS1.SSS1.p3.3.m3.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S6.SS1.SSS1.p3.3.m3.1a"><mn id="S6.SS1.SSS1.p3.3.m3.1.1" xref="S6.SS1.SSS1.p3.3.m3.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.3.m3.1b"><cn type="integer" id="S6.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.3.m3.1c">12</annotation></semantics></math> minutes) contain demonstrations with injected noise. Collecting training data with strong injected noise was quite exhausting for the human driver. However, a relatively small amount of such data proved very effective in stabilizing the learned policy.</p>
</div>
<figure id="S6.T1" class="ltx_table">
<div id="S6.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:263.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(83.6pt,-50.9pt) scale(1.62785321062238,1.62785321062238) ;">
<table id="S6.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T1.1.1.1.1" class="ltx_tr">
<td id="S6.T1.1.1.1.1.1" class="ltx_td ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.1.1.2" class="ltx_td ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;" colspan="2"><span id="S6.T1.1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Success rate</span></td>
<td id="S6.T1.1.1.1.1.4" class="ltx_td ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;" colspan="2"><span id="S6.T1.1.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Km per infraction</span></td>
</tr>
<tr id="S6.T1.1.1.2.2" class="ltx_tr">
<td id="S6.T1.1.1.2.2.1" class="ltx_td ltx_align_left" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S6.T1.1.1.2.2.2" class="ltx_td" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.2.2.3.1" class="ltx_text" style="font-size:90%;">Town 1</span></td>
<td id="S6.T1.1.1.2.2.4" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.2.2.4.1" class="ltx_text" style="font-size:90%;">Town 2</span></td>
<td id="S6.T1.1.1.2.2.5" class="ltx_td" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.2.2.6" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.2.2.6.1" class="ltx_text" style="font-size:90%;">Town 1</span></td>
<td id="S6.T1.1.1.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.2.2.7.1" class="ltx_text" style="font-size:90%;">Town 2</span></td>
</tr>
<tr id="S6.T1.1.1.3.3" class="ltx_tr">
<td id="S6.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.3.3.1.1" class="ltx_text" style="font-size:90%;">Non-conditional</span></td>
<td id="S6.T1.1.1.3.3.2" class="ltx_td ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.3.3.3.1" class="ltx_text" style="font-size:90%;">20%</span></td>
<td id="S6.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.3.3.4.1" class="ltx_text" style="font-size:90%;">26%</span></td>
<td id="S6.T1.1.1.3.3.5" class="ltx_td ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.3.3.6.1" class="ltx_text" style="font-size:90%;">5.76</span></td>
<td id="S6.T1.1.1.3.3.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.3.3.7.1" class="ltx_text" style="font-size:90%;">0.89</span></td>
</tr>
<tr id="S6.T1.1.1.4.4" class="ltx_tr">
<td id="S6.T1.1.1.4.4.1" class="ltx_td ltx_align_left" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.4.4.1.1" class="ltx_text" style="font-size:90%;">Goal-conditional</span></td>
<td id="S6.T1.1.1.4.4.2" class="ltx_td" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.4.4.3.1" class="ltx_text" style="font-size:90%;">24%</span></td>
<td id="S6.T1.1.1.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.4.4.4.1" class="ltx_text" style="font-size:90%;">30%</span></td>
<td id="S6.T1.1.1.4.4.5" class="ltx_td" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.4.4.6" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.4.4.6.1" class="ltx_text" style="font-size:90%;">1.87</span></td>
<td id="S6.T1.1.1.4.4.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.4.4.7.1" class="ltx_text" style="font-size:90%;">1.22</span></td>
</tr>
<tr id="S6.T1.1.1.5.5" class="ltx_tr">
<td id="S6.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.5.5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ours branched</span></td>
<td id="S6.T1.1.1.5.5.2" class="ltx_td ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.5.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">88%</span></td>
<td id="S6.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64%</span></td>
<td id="S6.T1.1.1.5.5.5" class="ltx_td ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.5.5.6.1" class="ltx_text" style="font-size:90%;">2.34</span></td>
<td id="S6.T1.1.1.5.5.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.5.5.7.1" class="ltx_text" style="font-size:90%;">1.18</span></td>
</tr>
<tr id="S6.T1.1.1.6.6" class="ltx_tr">
<td id="S6.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.6.6.1.1" class="ltx_text" style="font-size:90%;">Ours cmd. input</span></td>
<td id="S6.T1.1.1.6.6.2" class="ltx_td ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.6.6.3.1" class="ltx_text" style="font-size:90%;">78%</span></td>
<td id="S6.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.6.6.4.1" class="ltx_text" style="font-size:90%;">52%</span></td>
<td id="S6.T1.1.1.6.6.5" class="ltx_td ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.6.6.6.1" class="ltx_text" style="font-size:90%;">3.97</span></td>
<td id="S6.T1.1.1.6.6.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.6.6.7.1" class="ltx_text" style="font-size:90%;">1.30</span></td>
</tr>
<tr id="S6.T1.1.1.7.7" class="ltx_tr">
<td id="S6.T1.1.1.7.7.1" class="ltx_td ltx_align_left" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.7.7.1.1" class="ltx_text" style="font-size:90%;">Ours no noise</span></td>
<td id="S6.T1.1.1.7.7.2" class="ltx_td" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.7.7.3" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.7.7.3.1" class="ltx_text" style="font-size:90%;">56%</span></td>
<td id="S6.T1.1.1.7.7.4" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.7.7.4.1" class="ltx_text" style="font-size:90%;">22%</span></td>
<td id="S6.T1.1.1.7.7.5" class="ltx_td" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.7.7.6" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.7.7.6.1" class="ltx_text" style="font-size:90%;">1.31</span></td>
<td id="S6.T1.1.1.7.7.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.7.7.7.1" class="ltx_text" style="font-size:90%;">0.54</span></td>
</tr>
<tr id="S6.T1.1.1.8.8" class="ltx_tr">
<td id="S6.T1.1.1.8.8.1" class="ltx_td ltx_align_left" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.8.8.1.1" class="ltx_text" style="font-size:90%;">Ours no aug.</span></td>
<td id="S6.T1.1.1.8.8.2" class="ltx_td" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.8.8.3" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.8.8.3.1" class="ltx_text" style="font-size:90%;">80%</span></td>
<td id="S6.T1.1.1.8.8.4" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.8.8.4.1" class="ltx_text" style="font-size:90%;">0%</span></td>
<td id="S6.T1.1.1.8.8.5" class="ltx_td" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.8.8.6" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.8.8.6.1" class="ltx_text" style="font-size:90%;">4.03</span></td>
<td id="S6.T1.1.1.8.8.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.8.8.7.1" class="ltx_text" style="font-size:90%;">0.36</span></td>
</tr>
<tr id="S6.T1.1.1.9.9" class="ltx_tr">
<td id="S6.T1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.9.9.1.1" class="ltx_text" style="font-size:90%;">Ours shallow net</span></td>
<td id="S6.T1.1.1.9.9.2" class="ltx_td ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.9.9.3.1" class="ltx_text" style="font-size:90%;">46%</span></td>
<td id="S6.T1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.9.9.4.1" class="ltx_text" style="font-size:90%;">14%</span></td>
<td id="S6.T1.1.1.9.9.5" class="ltx_td ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S6.T1.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.9.9.6.1" class="ltx_text" style="font-size:90%;">0.96</span></td>
<td id="S6.T1.1.1.9.9.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T1.1.1.9.9.7.1" class="ltx_text" style="font-size:90%;">0.42</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Results in the simulated urban environment. We compare the presented method to baseline approaches and perform an ablation study.
We measure the percentage of successful episodes and the average distance (in km) driven between infractions.
Higher is better in both cases, but we rank methods based on success.
The proposed <span id="S6.T1.6.1" class="ltx_text ltx_font_typewriter">branched</span> architecture outperforms the baselines and the ablated versions.
</figcaption>
</figure>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS1.SSS2.4.1.1" class="ltx_text">VI-A</span>2 </span>Results</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.3" class="ltx_p">We compare the <span id="S6.SS1.SSS2.p1.3.1" class="ltx_text ltx_font_typewriter">branched</span> command-conditional architecture, as shown in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ IV Methodology â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b), with two baseline approaches, as well as several ablated versions of the full architecture.
The two baselines are standard imitation learning and goal-conditioned imitation learning.
In standard (non-conditional) imitation learning, the action <math id="S6.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{a}" display="inline"><semantics id="S6.SS1.SSS2.p1.1.m1.1a"><mi id="S6.SS1.SSS2.p1.1.m1.1.1" xref="S6.SS1.SSS2.p1.1.m1.1.1.cmml">ğš</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p1.1.m1.1b"><ci id="S6.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p1.1.m1.1.1">ğš</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p1.1.m1.1c">\mathbf{a}</annotation></semantics></math> is predicted from the observation <math id="S6.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{o}" display="inline"><semantics id="S6.SS1.SSS2.p1.2.m2.1a"><mi id="S6.SS1.SSS2.p1.2.m2.1.1" xref="S6.SS1.SSS2.p1.2.m2.1.1.cmml">ğ¨</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p1.2.m2.1b"><ci id="S6.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S6.SS1.SSS2.p1.2.m2.1.1">ğ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p1.2.m2.1c">\mathbf{o}</annotation></semantics></math> and the measurement <math id="S6.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{m}" display="inline"><semantics id="S6.SS1.SSS2.p1.3.m3.1a"><mi id="S6.SS1.SSS2.p1.3.m3.1.1" xref="S6.SS1.SSS2.p1.3.m3.1.1.cmml">ğ¦</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p1.3.m3.1b"><ci id="S6.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S6.SS1.SSS2.p1.3.m3.1.1">ğ¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p1.3.m3.1c">\mathbf{m}</annotation></semantics></math>.
In the goal-conditional variant, the controller is additionally provided with a vector pointing to the goal, in the carâ€™s coordinate system (the architecture follows FigureÂ <a href="#S4.F3" title="Figure 3 â€£ IV Methodology â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a)).
Ablated versions include: a network with the <span id="S6.SS1.SSS2.p1.3.2" class="ltx_text ltx_font_typewriter">command input</span> architecture instead of <span id="S6.SS1.SSS2.p1.3.3" class="ltx_text ltx_font_typewriter">branched</span> (see FigureÂ <a href="#S4.F3" title="Figure 3 â€£ IV Methodology â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), and three variants of the <span id="S6.SS1.SSS2.p1.3.4" class="ltx_text ltx_font_typewriter">branched</span> network: trained without noise-injected data, trained without data augmentation, and implemented with a shallower network.</p>
</div>
<div id="S6.SS1.SSS2.p2" class="ltx_para">
<p id="S6.SS1.SSS2.p2.4" class="ltx_p">The results are summarized in TableÂ <a href="#S6.T1" title="TABLE I â€£ VI-A1 Experimental setup â€£ VI-A Simulated Environment â€£ VI Experiments â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
The controller that is trained using standard imitation learning only completes <math id="S6.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S6.SS1.SSS2.p2.1.m1.1a"><mrow id="S6.SS1.SSS2.p2.1.m1.1.1" xref="S6.SS1.SSS2.p2.1.m1.1.1.cmml"><mn id="S6.SS1.SSS2.p2.1.m1.1.1.2" xref="S6.SS1.SSS2.p2.1.m1.1.1.2.cmml">20</mn><mo id="S6.SS1.SSS2.p2.1.m1.1.1.1" xref="S6.SS1.SSS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.1.m1.1b"><apply id="S6.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p2.1.m1.1.1.1.cmml" xref="S6.SS1.SSS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S6.SS1.SSS2.p2.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.1.m1.1c">20\%</annotation></semantics></math> of the episodes in Town 1 and <math id="S6.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="24\%" display="inline"><semantics id="S6.SS1.SSS2.p2.2.m2.1a"><mrow id="S6.SS1.SSS2.p2.2.m2.1.1" xref="S6.SS1.SSS2.p2.2.m2.1.1.cmml"><mn id="S6.SS1.SSS2.p2.2.m2.1.1.2" xref="S6.SS1.SSS2.p2.2.m2.1.1.2.cmml">24</mn><mo id="S6.SS1.SSS2.p2.2.m2.1.1.1" xref="S6.SS1.SSS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.2.m2.1b"><apply id="S6.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S6.SS1.SSS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p2.2.m2.1.1.1.cmml" xref="S6.SS1.SSS2.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p2.2.m2.1.1.2.cmml" xref="S6.SS1.SSS2.p2.2.m2.1.1.2">24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.2.m2.1c">24\%</annotation></semantics></math> in Town 2, which is not surprising given its ignorance of the goal.
More interestingly, the goal-conditional controller, which is provided with an accurate vector to the goal at every time step during both training and at test time, is performing only slightly better than the non-conditional controller, successfully completing <math id="S6.SS1.SSS2.p2.3.m3.1" class="ltx_Math" alttext="24\%" display="inline"><semantics id="S6.SS1.SSS2.p2.3.m3.1a"><mrow id="S6.SS1.SSS2.p2.3.m3.1.1" xref="S6.SS1.SSS2.p2.3.m3.1.1.cmml"><mn id="S6.SS1.SSS2.p2.3.m3.1.1.2" xref="S6.SS1.SSS2.p2.3.m3.1.1.2.cmml">24</mn><mo id="S6.SS1.SSS2.p2.3.m3.1.1.1" xref="S6.SS1.SSS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.3.m3.1b"><apply id="S6.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S6.SS1.SSS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S6.SS1.SSS2.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S6.SS1.SSS2.p2.3.m3.1.1.2">24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.3.m3.1c">24\%</annotation></semantics></math> of the episodes in Town 1 and <math id="S6.SS1.SSS2.p2.4.m4.1" class="ltx_Math" alttext="30\%" display="inline"><semantics id="S6.SS1.SSS2.p2.4.m4.1a"><mrow id="S6.SS1.SSS2.p2.4.m4.1.1" xref="S6.SS1.SSS2.p2.4.m4.1.1.cmml"><mn id="S6.SS1.SSS2.p2.4.m4.1.1.2" xref="S6.SS1.SSS2.p2.4.m4.1.1.2.cmml">30</mn><mo id="S6.SS1.SSS2.p2.4.m4.1.1.1" xref="S6.SS1.SSS2.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.4.m4.1b"><apply id="S6.SS1.SSS2.p2.4.m4.1.1.cmml" xref="S6.SS1.SSS2.p2.4.m4.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p2.4.m4.1.1.1.cmml" xref="S6.SS1.SSS2.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p2.4.m4.1.1.2.cmml" xref="S6.SS1.SSS2.p2.4.m4.1.1.2">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.4.m4.1c">30\%</annotation></semantics></math> in Town 2. Qualitatively, this controller eventually veers off the road attempting to shortcut to the goal. This also decreases the number of kilometers the controller is able to traverse without infractions. A simple feed-forward network does not automatically learn to convert a vector pointing to the goal into a sequence of turns.</p>
</div>
<div id="S6.SS1.SSS2.p3" class="ltx_para">
<p id="S6.SS1.SSS2.p3.2" class="ltx_p">The proposed <span id="S6.SS1.SSS2.p3.2.1" class="ltx_text ltx_font_typewriter">branched</span> command-conditional controller performs significantly better than the baseline methods in both towns, successfully completing <math id="S6.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="88\%" display="inline"><semantics id="S6.SS1.SSS2.p3.1.m1.1a"><mrow id="S6.SS1.SSS2.p3.1.m1.1.1" xref="S6.SS1.SSS2.p3.1.m1.1.1.cmml"><mn id="S6.SS1.SSS2.p3.1.m1.1.1.2" xref="S6.SS1.SSS2.p3.1.m1.1.1.2.cmml">88</mn><mo id="S6.SS1.SSS2.p3.1.m1.1.1.1" xref="S6.SS1.SSS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p3.1.m1.1b"><apply id="S6.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S6.SS1.SSS2.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S6.SS1.SSS2.p3.1.m1.1.1.2">88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p3.1.m1.1c">88\%</annotation></semantics></math> of the episodes in Town 1 and <math id="S6.SS1.SSS2.p3.2.m2.1" class="ltx_Math" alttext="64\%" display="inline"><semantics id="S6.SS1.SSS2.p3.2.m2.1a"><mrow id="S6.SS1.SSS2.p3.2.m2.1.1" xref="S6.SS1.SSS2.p3.2.m2.1.1.cmml"><mn id="S6.SS1.SSS2.p3.2.m2.1.1.2" xref="S6.SS1.SSS2.p3.2.m2.1.1.2.cmml">64</mn><mo id="S6.SS1.SSS2.p3.2.m2.1.1.1" xref="S6.SS1.SSS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p3.2.m2.1b"><apply id="S6.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S6.SS1.SSS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S6.SS1.SSS2.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S6.SS1.SSS2.p3.2.m2.1.1.2">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p3.2.m2.1c">64\%</annotation></semantics></math> in Town 2.
In terms of distance travelled without infractions, in Town 2 the method is on par with baselines, while in Town 1 it is outperformed by the non-conditional model. This difference is misleading: the non-conditional model drives more cleanly because it is not constrained to travel towards the goal and therefore typically takes a simpler route at each intersection.</p>
</div>
<div id="S6.SS1.SSS2.p4" class="ltx_para">
<p id="S6.SS1.SSS2.p4.2" class="ltx_p">The ablation study shown in the bottom part of TableÂ <a href="#S6.T1" title="TABLE I â€£ VI-A1 Experimental setup â€£ VI-A Simulated Environment â€£ VI Experiments â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> reveals that all components of the proposed system are important for good performance.
The <span id="S6.SS1.SSS2.p4.2.1" class="ltx_text ltx_font_typewriter">branched</span> architecture reaches the goal more reliably than the <span id="S6.SS1.SSS2.p4.2.2" class="ltx_text ltx_font_typewriter">command input</span> one.
The addition of even a small amount of training data with noise in the steering dramatically improves the performance. (Recall that we have only <math id="S6.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S6.SS1.SSS2.p4.1.m1.1a"><mn id="S6.SS1.SSS2.p4.1.m1.1.1" xref="S6.SS1.SSS2.p4.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p4.1.m1.1b"><cn type="integer" id="S6.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p4.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p4.1.m1.1c">12</annotation></semantics></math> minutes of noisy data out of the total of <math id="S6.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S6.SS1.SSS2.p4.2.m2.1a"><mn id="S6.SS1.SSS2.p4.2.m2.1.1" xref="S6.SS1.SSS2.p4.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p4.2.m2.1b"><cn type="integer" id="S6.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S6.SS1.SSS2.p4.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p4.2.m2.1c">2</annotation></semantics></math> hours.)
Careful data augmentation is crucial for generalization, even within Town 1, but much more so in the previously unseen Town 2: the model without data augmentation was not able to complete a single episode there.
Finally, a sufficiently deep network is needed to learn the perceptuomotor mapping in the visually rich and complex simulated urban environment.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic">Physical System</span>
</h3>

<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS2.SSS1.4.1.1" class="ltx_text">VI-B</span>1 </span>Experimental setup</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p">The training dataset consists of <math id="S6.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S6.SS2.SSS1.p1.1.m1.1a"><mn id="S6.SS2.SSS1.p1.1.m1.1.1" xref="S6.SS2.SSS1.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.1.m1.1b"><cn type="integer" id="S6.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.1.m1.1c">2</annotation></semantics></math> hours of driving the truck via remote control in a residential area.
FigureÂ <a href="#S6.F8" title="Figure 8 â€£ VI-B1 Experimental setup â€£ VI-B Physical System â€£ VI Experiments â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows a map with the route on which the vehicle was evaluated. The route includes a total of 14 intersections with roughly the same number of <span id="S6.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_typewriter">left</span>, <span id="S6.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_typewriter">straight</span>, and <span id="S6.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_typewriter">right</span>.</p>
</div>
<div id="S6.SS2.SSS1.p2" class="ltx_para">
<p id="S6.SS2.SSS1.p2.1" class="ltx_p">We measure the performance in terms of missed intersections, interventions, and time to complete the course. If the robotic vehicle misses an intersection for the first time, it is rerouted to get a second chance to do the turn. If it manages to do the turn the second time, this is not counted as a missed intersection but increases the time taken to complete the route. However, if the vehicle misses the intersection for the second time, this is counted as missed and we intervene to drive the vehicle through the turn manually. Besides missed intersections, we also intervene if the vehicle goes off the road for more than five seconds or if it collides with an obstacle. The models were all evaluated in overcast weather conditions. The majority of training data was collected in sunny weather.</p>
</div>
<figure id="S6.F8" class="ltx_figure"><img src="/html/1710.02410/assets/images_compressed/Map2D_PhysicalSystem.jpg" id="S6.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="449" height="455" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>A map of the primary route used for testing the physical system. Intersections traversed by the truck are numbered according to their order along the route. Colors indicate commands provided to the vehicle when it approaches the intersection: blue = <span id="S6.F8.4.1" class="ltx_text ltx_font_typewriter">left</span>, green = <span id="S6.F8.5.2" class="ltx_text ltx_font_typewriter">straight</span>, orange = <span id="S6.F8.6.3" class="ltx_text ltx_font_typewriter">right</span>.</figcaption>
</figure>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS2.SSS2.4.1.1" class="ltx_text">VI-B</span>2 </span>Main results</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.4" class="ltx_p">We select the most important comparisons from the extensive evaluation performed in simulation (SectionÂ <a href="#S6.SS1" title="VI-A Simulated Environment â€£ VI Experiments â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-A</span></span></a>) and perform them on the physical system. Table <a href="#S6.T2" title="TABLE II â€£ VI-B2 Main results â€£ VI-B Physical System â€£ VI Experiments â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows the results of several variants of command-conditional imitation learning: <span id="S6.SS2.SSS2.p1.4.1" class="ltx_text ltx_font_typewriter">branched</span> and <span id="S6.SS2.SSS2.p1.4.2" class="ltx_text ltx_font_typewriter">command input</span> architectures, as well as two ablated models, trained without data augmentation or without noise-injected data. It is evident that the <span id="S6.SS2.SSS2.p1.4.3" class="ltx_text ltx_font_typewriter">branched</span> architecture achieves the best performance. The ablation experiments show the impact of our noise injection method and augmentation strategy. The model trained without noise injection is very unstable, as indicated by the average number of interventions rising from <math id="S6.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="0.67" display="inline"><semantics id="S6.SS2.SSS2.p1.1.m1.1a"><mn id="S6.SS2.SSS2.p1.1.m1.1.1" xref="S6.SS2.SSS2.p1.1.m1.1.1.cmml">0.67</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p1.1.m1.1b"><cn type="float" id="S6.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS2.p1.1.m1.1.1">0.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p1.1.m1.1c">0.67</annotation></semantics></math> to <math id="S6.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="8.67" display="inline"><semantics id="S6.SS2.SSS2.p1.2.m2.1a"><mn id="S6.SS2.SSS2.p1.2.m2.1.1" xref="S6.SS2.SSS2.p1.2.m2.1.1.cmml">8.67</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p1.2.m2.1b"><cn type="float" id="S6.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S6.SS2.SSS2.p1.2.m2.1.1">8.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p1.2.m2.1c">8.67</annotation></semantics></math>. Moreover, it misses almost <math id="S6.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S6.SS2.SSS2.p1.3.m3.1a"><mrow id="S6.SS2.SSS2.p1.3.m3.1.1" xref="S6.SS2.SSS2.p1.3.m3.1.1.cmml"><mn id="S6.SS2.SSS2.p1.3.m3.1.1.2" xref="S6.SS2.SSS2.p1.3.m3.1.1.2.cmml">25</mn><mo id="S6.SS2.SSS2.p1.3.m3.1.1.1" xref="S6.SS2.SSS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p1.3.m3.1b"><apply id="S6.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S6.SS2.SSS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S6.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S6.SS2.SSS2.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S6.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S6.SS2.SSS2.p1.3.m3.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p1.3.m3.1c">25\%</annotation></semantics></math> of the intersections and takes double the time to complete the course. The model trained without data augmentation fails completely. The truck misses most intersections and very frequently leaves the lane resulting in almost <math id="S6.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S6.SS2.SSS2.p1.4.m4.1a"><mn id="S6.SS2.SSS2.p1.4.m4.1.1" xref="S6.SS2.SSS2.p1.4.m4.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p1.4.m4.1b"><cn type="integer" id="S6.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S6.SS2.SSS2.p1.4.m4.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p1.4.m4.1c">40</annotation></semantics></math> interventions. It takes more than four times longer to complete the course. This extreme degradation highlights the importance of generalization in real world settings with constantly changing environmental conditions such as weather and lighting. Proper data augmentation dramatically improves performance given limited training data.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T2.1.1.1" class="ltx_tr">
<th id="S6.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S6.T2.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Missed turns</span></th>
<th id="S6.T2.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Interventions</span></th>
<th id="S6.T2.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Time</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T2.1.2.1" class="ltx_tr">
<td id="S6.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ours branched</span></td>
<td id="S6.T2.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0%</span></td>
<td id="S6.T2.1.2.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.67</span></td>
<td id="S6.T2.1.2.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2:19</span></td>
</tr>
<tr id="S6.T2.1.3.2" class="ltx_tr">
<td id="S6.T2.1.3.2.1" class="ltx_td ltx_align_left" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Ours cmd. input</span></td>
<td id="S6.T2.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.3.2.2.1" class="ltx_text" style="font-size:90%;">11.1%</span></td>
<td id="S6.T2.1.3.2.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.3.2.3.1" class="ltx_text" style="font-size:90%;">2.33</span></td>
<td id="S6.T2.1.3.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.3.2.4.1" class="ltx_text" style="font-size:90%;">4:13</span></td>
</tr>
<tr id="S6.T2.1.4.3" class="ltx_tr">
<td id="S6.T2.1.4.3.1" class="ltx_td ltx_align_left" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.4.3.1.1" class="ltx_text" style="font-size:90%;">Ours no noise</span></td>
<td id="S6.T2.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.4.3.2.1" class="ltx_text" style="font-size:90%;">24.4%</span></td>
<td id="S6.T2.1.4.3.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.4.3.3.1" class="ltx_text" style="font-size:90%;">8.67</span></td>
<td id="S6.T2.1.4.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.4.3.4.1" class="ltx_text" style="font-size:90%;">4:39</span></td>
</tr>
<tr id="S6.T2.1.5.4" class="ltx_tr">
<td id="S6.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Ours no aug.</span></td>
<td id="S6.T2.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.5.4.2.1" class="ltx_text" style="font-size:90%;">73%</span></td>
<td id="S6.T2.1.5.4.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.5.4.3.1" class="ltx_text" style="font-size:90%;">39</span></td>
<td id="S6.T2.1.5.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;"><span id="S6.T2.1.5.4.4.1" class="ltx_text" style="font-size:90%;">10:41</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Results on the physical system. Lower is better. We compare the <span id="S6.T2.4.1" class="ltx_text ltx_font_typewriter">branched</span> model to the simpler <span id="S6.T2.5.2" class="ltx_text ltx_font_typewriter">command input</span> architecture and to ablated versions (without noise injection and without data augmentation). Average performance across 3 runs is reported for all models except for â€œOurs no aug.â€, for which we only performed 1 run to avoid breaking the truck.</figcaption>
</figure>
</section>
<section id="S6.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS2.SSS3.4.1.1" class="ltx_text">VI-B</span>3 </span>Generalization to new environments</h4>

<div id="S6.SS2.SSS3.p1" class="ltx_para">
<p id="S6.SS2.SSS3.p1.1" class="ltx_p">Beyond the implicit generalization to varying weather conditions that occur naturally in the physical world, we also evaluate qualitatively how well the model generalizes to previously unseen environments with very different appearance. To this end, we run the truck in three environments shown in FigureÂ <a href="#S6.F9" title="Figure 9 â€£ VI-B3 Generalization to new environments â€£ VI-B Physical System â€£ VI Experiments â€£ End-to-end Driving via Conditional Imitation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The truck is able to consistently follow the lane in all tested environments and is responsive to commands. These and other experiments are shown in the supplementary video.</p>
</div>
<figure id="S6.F9" class="ltx_figure">
<table id="S6.F9.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.F9.3.3" class="ltx_tr">
<td id="S6.F9.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/Generalization_PhysicalSystem1.jpg" id="S6.F9.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="192" height="107" alt="Refer to caption"></td>
<td id="S6.F9.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/Generalization_PhysicalSystem2.jpg" id="S6.F9.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="192" height="107" alt="Refer to caption"></td>
<td id="S6.F9.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><img src="/html/1710.02410/assets/images_compressed/Generalization_PhysicalSystem3.jpg" id="S6.F9.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="192" height="107" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Testing in new environments with very different appearance.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S7" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We proposed command-conditional imitation learning: an approach to learning from expert demonstrations of low-level controls and high-level commands. At training time, the commands resolve ambiguities in the perceptuomotor mapping, thus facilitating learning. At test time, the commands serve as a communication channel that can be used to direct the controller.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We applied the presented approach to vision-based driving of a physical robotic vehicle and in realistic simulations of dynamic urban environments. Our results show that the command-conditional formulation significantly improves performance in both scenarios.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">While the presented results are encouraging, they also reveal that significant room for progress remains. In particular, more sophisticated and higher-capacity architectures along with larger datasets will be necessary to support autonomous urban driving on a large scale. We hope that the presented approach to making driving policies more controllable will prove useful in such deployment.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">Our work has not addressed human guidance of autonomous vehicles using natural language: a mode of human-robot communication that has been explored in the literatureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. We leave unstructured natural language communication with autonomous vehicles as an important direction for future work.</p>
</div>
</section>
<section id="S8" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Acknowledgements</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Antonio M. LÃ³pez and Felipe Codevilla acknowledge the Spanish project TIN2017-88709-R (Ministerio de Economia, Industria y Competitividad) and the Spanish DGT project SPIP2017-02237, the Generalitat de Catalunya CERCA Program and its ACCIO agency. Felipe Codevilla was supported in part by FI grant 2017FI-B1-00162. Antonio and Felipe also thank GermÃ¡n Ros who proposed to investigate the benefits of introducing route commands into the end-to-end driving paradigm during his time at CVC.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography ltx_centering">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
P.Â Abbeel, A.Â Coates, and A.Â Y. Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Autonomous helicopter aerobatics through apprenticeship learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Robotics Research</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 29(13), 2010.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
B.Â Argall, S.Â Chernova, M.Â M. Veloso, and B.Â Browning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">A survey of robot learning from demonstration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Robotics and Autonomous Systems</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 57(5), 2009.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
A.Â G. Barto and S.Â Mahadevan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Recent advances in hierarchical reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Discrete Event Dynamic Systems</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 13(1-2), 2003.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
M.Â Bojarski, D.Â D. Testa, D.Â Dworakowski, B.Â Firner, B.Â Flepp, P.Â Goyal, L.Â D.
Jackel, M.Â Monfort, U.Â Muller, J.Â Zhang, X.Â Zhang, J.Â Zhao, and K.Â Zieba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">End to end learning for self-driving cars.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1604.07316</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
A.Â Broad, J.Â Arkin, N.Â Ratliff, T.Â Howard, and B.Â Argall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Real-Â­time natural language corrections for assistive robotic
manipulators.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Robotics Research</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
C.Â Chen, A.Â Seff, A.Â L. Kornhauser, and J.Â Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">DeepDriving: Learning affordance for direct perception in
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
B.Â C. daÂ Silva, G.Â Konidaris, and A.Â G. Barto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Learning parameterized skills.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
M.Â P. Deisenroth, P.Â Englert, J.Â Peters, and D.Â Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Multi-task policy search for robotics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
A.Â Dosovitskiy and V.Â Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Learning to act by predicting the future.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
A.Â Dosovitskiy, G.Â Ros, F.Â Codevilla, A.Â LÃ³pez, and V.Â Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">CARLA: An open urban driving simulator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Robot Learning (CoRL)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
P.Â Englert, A.Â Paraschos, J.Â Peters, and M.Â P. Deisenroth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Model-based imitation learning by probabilistic trajectory matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
U.Â Franke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision in Vehicle Technology</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">. 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
A.Â Giusti, J.Â Guzzi, D.Â Ciresan, F.-L. He, J.Â P. Rodriguez, F.Â Fontana,
M.Â Faessler, C.Â Forster, J.Â Schmidhuber, G.Â DiÂ Caro, D.Â Scaramuzza, and
L.Â Gambardella.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">A machine learning approach to visual perception of forest trails for
mobile robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
S.Â Hemachandra, F.Â Duvallet, T.Â M. Howard, N.Â Roy, A.Â Stentz, and M.Â R. Walter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Learning models for following natural language directions in unknown
environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
S.Â Javdani, S.Â S. Srinivasa, and J.Â A. Bagnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Shared autonomy via hindsight optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">RSS</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
D.Â P. Kingma and J.Â Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
J.Â Kober, J.Â A. Bagnell, and J.Â Peters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Reinforcement learning in robotics: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Robotics Research</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 32(11), 2013.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
J.Â Kober, A.Â Wilhelm, E.Â Oztop, and J.Â Peters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Reinforcement learning to adjust parametrized motor primitives to new
situations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Autonomous Robots</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 33(4), 2012.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
G.Â Konidaris, S.Â Kuindersma, R.Â A. Grupen, and A.Â G. Barto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Robot learning from demonstration by constructing skill trees.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Robotics Research</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 31(3), 2012.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
T.Â D. Kulkarni, K.Â Narasimhan, A.Â Saeedi, and J.Â B. Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Hierarchical deep reinforcement learning: Integrating temporal
abstraction and intrinsic motivation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
M.Â Laskey, A.Â Dragan, J.Â Lee, K.Â Goldberg, and R.Â Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Dart: Optimizing noise injection in imitation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Robot Learning (CoRL)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Y.Â LeCun, U.Â Muller, J.Â Ben, E.Â Cosatto, and B.Â Flepp.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Off-road obstacle avoidance through end-to-end learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2005.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
S.Â Levine and V.Â Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Guided policy search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
C.Â Matuszek, L.Â Bo, L.Â Zettlemoyer, and D.Â Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Learning from unscripted deictic gesture and language for human-robot
interactions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
B.Â Paden, M.Â CÃ¡p, S.Â Z. Yong, D.Â S. Yershov, and E.Â Frazzoli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">A survey of motion planning and control techniques for self-driving
urban vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Vehicles</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 1(1), 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
P.Â Pastor, H.Â Hoffmann, T.Â Asfour, and S.Â Schaal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Learning and generalization of motor skills by learning from
demonstration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
D.Â Pomerleau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">ALVINN: An autonomous land vehicle in a neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 1988.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
N.Â D. Ratliff, J.Â A. Bagnell, and S.Â S. Srinivasa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Imitation learning for locomotion and manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Humanoid Robots</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2007.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
S.Â Ross, G.Â J. Gordon, and J.Â A. Bagnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">A reduction of imitation learning and structured prediction to
no-regret online learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AISTATS</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
S.Â Ross, N.Â Melik-Barkhudarov, K.Â S. Shankar, A.Â Wendel, D.Â Dey, J.Â A.
Bagnell, and M.Â Hebert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Learning monocular reactive UAV control in cluttered natural
environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
T.Â Schaul, D.Â Horgan, K.Â Gregor, and D.Â Silver.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Universal value function approximators.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
D.Â Silver, J.Â A. Bagnell, and A.Â Stentz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Learning from demonstration for autonomous navigation in complex
unstructured terrain.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Robotics Research</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 29(12), 2010.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
R.Â S. Sutton, D.Â Precup, and S.Â P. Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Between MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Artificial Intelligence</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 112(1-2), 1999.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
S.Â Tellex, T.Â Kollar, S.Â Dickerson, M.Â R. Walter, A.Â G. Banerjee, S.Â J. Teller,
and N.Â Roy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Understanding natural language commands for robotic navigation and
mobile manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
M.Â R. Walter, S.Â Hemachandra, B.Â Homberg, S.Â Tellex, and S.Â J. Teller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Learning semantic maps from natural language descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">RSS</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
J.Â Zhang and K.Â Cho.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Query-efficient imitation learning for end-to-end simulated driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
B.Â D. Ziebart, A.Â L. Maas, J.Â A. Bagnell, and A.Â K. Dey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Maximum entropy inverse reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
B.Â D. Ziebart, A.Â L. Maas, A.Â K. Dey, and J.Â A. Bagnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Navigate like a cabbie: Probabilistic reasoning from observed
context-aware behavior.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">UbiComp</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2008.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1710.02409" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/land_of_honey_and_milk" rel="nofollow" aria-hidden="true" tabindex="-1"></a>
    <a href="/log/1710.02410" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1710.02410">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1710.02410" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1710.02411" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar  3 03:26:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
