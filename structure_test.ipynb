{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/yuuki/Documents/Code/zhihuishu/chunk_experiment', '/Users/yuuki/Documents/Code/zhihuishu/chunk_experiment', '/opt/anaconda3/envs/d2l/lib/python38.zip', '/opt/anaconda3/envs/d2l/lib/python3.8', '/opt/anaconda3/envs/d2l/lib/python3.8/lib-dynload', '', '/Users/yuuki/Documents/Code/zhihuishu/intern/lib/python3.8/site-packages', '/Users/yuuki/Documents/Code/zhihuishu/chunk_experiment']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "# from langchain_text_splitters import HTMLSectionSplitter\n",
    "# from langchain.text_splitter import (\n",
    "#     HTMLHeaderTextSplitter,\n",
    "#     PythonCodeTextSplitter,\n",
    "#     MarkdownTextSplitter,\n",
    "#     LatexTextSplitter,\n",
    "# )\n",
    "from src.html_chunk import HTMLSectionSplitter\n",
    "from src.pythoncode_chunk import PythonCodeTextSplitter\n",
    "from src.markdown_chunk import MarkdownTextSplitter\n",
    "from src.latex_chunk import LatexTextSplitter\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..','..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, notebook_dir)\n",
    "    \n",
    "from util.chunk_highlight import TextHighlighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_files_path=project_root+\"/data/html/arxiv\"\n",
    "latex_files_path=notebook_dir+\"/data/latex/arxiv\"\n",
    "md_files_path=notebook_dir+\"/data/markdown/markdown-documentation-transformers\"\n",
    "python_files_path=notebook_dir+\"/data/python\"\n",
    "\n",
    "html_splitter = HTMLSectionSplitter(headers_to_split_on=[(\"h1\", \"Header 1\"),(\"h2\", \"Header 2\"),(\"h3\", \"Header 3\"),(\"h4\", \"Header 4\"),(\"h5\", \"Header 5\"),(\"h6\", \"Header 6\")])\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "md_splitter = MarkdownTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "tex_splitter = LatexTextSplitter(chunk_size=50, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_type(type):\n",
    "    if type==\".html\":\n",
    "        return html_files_path,html_splitter\n",
    "    elif type==\".tex\":\n",
    "        return latex_files_path,tex_splitter\n",
    "    elif type==\".md\":\n",
    "        return md_files_path,md_splitter\n",
    "    elif type==\".py\":\n",
    "        return python_files_path,python_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/340.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/205.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/74.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/439.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/317.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/252.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/481.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/23.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/194.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/301.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/244.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/35.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/497.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/182.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/478.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/356.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/213.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/62.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/9.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/268.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/287.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/454.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/141.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/19.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/395.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/403.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/116.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/383.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/229.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/415.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/100.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/58.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/291.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/442.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/157.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/120.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/78.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/97.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/435.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/209.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/177.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/198.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/462.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/161.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/39.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/474.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/248.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/136.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/423.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/81.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/5.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/458.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/15.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/264.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/321.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/42.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/399.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/233.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/376.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/419.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/54.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/225.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/360.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/272.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/337.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/273.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/336.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/224.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/361.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/418.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/55.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/398.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/232.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/377.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/43.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/265.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/320.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/459.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/14.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/137.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/422.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/4.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/80.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/249.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/160.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/38.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/475.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/176.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/199.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/463.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/208.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/121.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/79.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/96.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/434.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/443.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/156.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/290.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/414.html\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/html/arxiv/101.html\n",
      "100个文档平均运行时间:0.5774秒\n"
     ]
    }
   ],
   "source": [
    "type=\".html\"\n",
    "paths,splitter = choose_type(type)\n",
    "\n",
    "\n",
    "file_paths = [os.path.join(paths, f) for f in os.listdir(paths) if f.endswith(type)]\n",
    "file_paths = file_paths[:100]\n",
    "total_time = 0\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sample_text = f.read()\n",
    "\n",
    "    # html需要移除xml声明\n",
    "    sample_text = sample_text.replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', '')\n",
    "    print(f\"\\n测试文件: {file_path}\")\n",
    "    start_time = time.time()\n",
    "    splitter.split_text(sample_text)\n",
    "    end_time = time.time()\n",
    "    total_time += end_time - start_time\n",
    "    \n",
    "average_time = total_time / len(file_paths)\n",
    "print(f\"100个文档平均运行时间:{average_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高亮展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机选取的文本片段起始索引: 89944, 长度: 200000\n",
      "选取的文本片段:\n",
      "\"></times><cn type=\"float\" id=\"S2.p2.14.m14.1.1.3.2.2.cmml\" xref=\"S2.p2.14.m14.1.1.3.2.2\">5.24</cn><ci id=\"S2.p2.14.m14.1.1.3.2.3.cmml\" xref=\"S2.p2.14.m14.1.1.3.2.3\">GeV</ci></apply><apply id=\"S2.p2.14.m14.1.1.3.3.cmml\" xref=\"S2.p2.14.m14.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p2.14.m14.1.1.3.3.1.cmml\" xref=\"S2.p2.14.m14.1.1.3.3\">superscript</csymbol><ci id=\"S2.p2.14.m14.1.1.3.3.2.cmml\" xref=\"S2.p2.14.m14.1.1.3.3.2\">𝑐</ci><cn type=\"integer\" id=\"S2.p2.14.m14.1.1.3.3.3.cmml\" xref=\"S2.p2.14.m14.1.1.3.3.3\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p2.14.m14.1c\">M_{bc}&gt;5.24~{}\\mathrm{GeV/}c^{2}</annotation></semantics></math> and <math id=\"S2.p2.15.m15.1\" class=\"ltx_Math\" alttext=\"-0.3&lt;\\Delta E&lt;0.3~{}\\rm{GeV}\" display=\"inline\"><semantics id=\"S2.p2.15.m15.1a\"><mrow id=\"S2.p2.15.m15.1.1\" xref=\"S2.p2.15.m15.1.1.cmml\"><mrow id=\"S2.p2.15.m15.1.1.2\" xref=\"S2.p2.15.m15.1.1.2.cmml\"><mo id=\"S2.p2.15.m15.1.1.2a\" xref=\"S2.p2.15.m15.1.1.2.cmml\">−</mo><mn id=\"S2.p2.15.m15.1.1.2.2\" xref=\"S2.p2.15.m15.1.1.2.2.cmml\">0.3</mn></mrow><mo id=\"S2.p2.15.m15.1.1.3\" xref=\"S2.p2.15.m15.1.1.3.cmml\">&lt;</mo><mrow id=\"S2.p2.15.m15.1.1.4\" xref=\"S2.p2.15.m15.1.1.4.cmml\"><mi mathvariant=\"normal\" id=\"S2.p2.15.m15.1.1.4.2\" xref=\"S2.p2.15.m15.1.1.4.2.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p2.15.m15.1.1.4.1\" xref=\"S2.p2.15.m15.1.1.4.1.cmml\">​</mo><mi id=\"S2.p2.15.m15.1.1.4.3\" xref=\"S2.p2.15.m15.1.1.4.3.cmml\">E</mi></mrow><mo id=\"S2.p2.15.m15.1.1.5\" xref=\"S2.p2.15.m15.1.1.5.cmml\">&lt;</mo><mrow id=\"S2.p2.15.m15.1.1.6\" xref=\"S2.p2.15.m15.1.1.6.cmml\"><mn id=\"S2.p2.15.m15.1.1.6.2\" xref=\"S2.p2.15.m15.1.1.6.2.cmml\">0.3</mn><mo lspace=\"0.330em\" rspace=\"0em\" id=\"S2.p2.15.m15.1.1.6.1\" xref=\"S2.p2.15.m15.1.1.6.1.cmml\">​</mo><mi id=\"S2.p2.15.m15.1.1.6.3\" xref=\"S2.p2.15.m15.1.1.6.3.cmml\">GeV</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p2.15.m15.1b\"><apply id=\"S2.p2.15.m15.1.1.cmml\" xref=\"S2.p2.15.m15.1.1\"><and id=\"S2.p2.15.m15.1.1a.cmml\" xref=\"S2.p2.15.m15.1.1\"></and><apply id=\"S2.p2.15.m15.1.1b.cmml\" xref=\"S2.p2.15.m15.1.1\"><lt id=\"S2.p2.15.m15.1.1.3.cmml\" xref=\"S2.p2.15.m15.1.1.3\"></lt><apply id=\"S2.p2.15.m15.1.1.2.cmml\" xref=\"S2.p2.15.m15.1.1.2\"><minus id=\"S2.p2.15.m15.1.1.2.1.cmml\" xref=\"S2.p2.15.m15.1.1.2\"></minus><cn type=\"float\" id=\"S2.p2.15.m15.1.1.2.2.cmml\" xref=\"S2.p2.15.m15.1.1.2.2\">0.3</cn></apply><apply id=\"S2.p2.15.m15.1.1.4.cmml\" xref=\"S2.p2.15.m15.1.1.4\"><times id=\"S2.p2.15.m15.1.1.4.1.cmml\" xref=\"S2.p2.15.m15.1.1.4.1\"></times><ci id=\"S2.p2.15.m15.1.1.4.2.cmml\" xref=\"S2.p2.15.m15.1.1.4.2\">Δ</ci><ci id=\"S2.p2.15.m15.1.1.4.3.cmml\" xref=\"S2.p2.15.m15.1.1.4.3\">𝐸</ci></apply></apply><apply id=\"S2.p2.15.m15.1.1c.cmml\" xref=\"S2.p2.15.m15.1.1\"><lt id=\"S2.p2.15.m15.1.1.5.cmml\" xref=\"S2.p2.15.m15.1.1.5\"></lt><share href=\"#S2.p2.15.m15.1.1.4.cmml\" id=\"S2.p2.15.m15.1.1d.cmml\" xref=\"S2.p2.15.m15.1.1\"></share><apply id=\"S2.p2.15.m15.1.1.6.cmml\" xref=\"S2.p2.15.m15.1.1.6\"><times id=\"S2.p2.15.m15.1.1.6.1.cmml\" xref=\"S2.p2.15.m15.1.1.6.1\"></times><cn type=\"float\" id=\"S2.p2.15.m15.1.1.6.2.cmml\" xref=\"S2.p2.15.m15.1.1.6.2\">0.3</cn><ci id=\"S2.p2.15.m15.1.1.6.3.cmml\" xref=\"S2.p2.15.m15.1.1.6.3\">GeV</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p2.15.m15.1c\">-0.3&lt;\\Delta E&lt;0.3~{}\\rm{GeV}</annotation></semantics></math>, while the signal-enhanced region is defined as <math id=\"S2.p2.16.m16.1\" class=\"ltx_Math\" alttext=\"5.27&lt;M_{bc}&lt;5.29~{}\\mathrm{GeV/}c^{2}\" display=\"inline\"><semantics id=\"S2.p2.16.m16.1a\"><mrow id=\"S2.p2.16.m16.1.1\" xref=\"S2.p2.16.m16.1.1.cmml\"><mn id=\"S2.p2.16.m16.1.1.2\" xref=\"S2.p2.16.m16.1.1.2.cmml\">5.27</mn><mo id=\"S2.p2.16.m16.1.1.3\" xref=\"S2.p2.16.m16.1.1.3.cmml\">&lt;</mo><msub id=\"S2.p2.16.m16.1.1.4\" xref=\"S2.p2.16.m16.1.1.4.cmml\"><mi id=\"S2.p2.16.m16.1.1.4.2\" xref=\"S2.p2.16.m16.1.1.4.2.cmml\">M</mi><mrow id=\"S2.p2.16.m16.1.1.4.3\" xref=\"S2.p2.16.m16.1.1.4.3.cmml\"><mi id=\"S2.p2.16.m16.1.1.4.3.2\" xref=\"S2.p2.16.m16.1.1.4.3.2.cmml\">b</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p2.16.m16.1.1.4.3.1\" xref=\"S2.p2.16.m16.1.1.4.3.1.cmml\">​</mo><mi id=\"S2.p2.16.m16.1.1.4.3.3\" xref=\"S2.p2.16.m16.1.1.4.3.3.cmml\">c</mi></mrow></msub><mo id=\"S2.p2.16.m16.1.1.5\" xref=\"S2.p2.16.m16.1.1.5.cmml\">&lt;</mo><mrow id=\"S2.p2.16.m16.1.1.6\" xref=\"S2.p2.16.m16.1.1.6.cmml\"><mrow id=\"S2.p2.16.m16.1.1.6.2\" xref=\"S2.p2.16.m16.1.1.6.2.cmml\"><mn id=\"S2.p2.16.m16.1.1.6.2.2\" xref=\"S2.p2.16.m16.1.1.6.2.2.cmml\">5.29</mn><mo lspace=\"0.330em\" rspace=\"0em\" id=\"S2.p2.16.m16.1.1.6.2.1\" xref=\"S2.p2.16.m16.1.1.6.2.1.cmml\">​</mo><mi id=\"S2.p2.16.m16.1.1.6.2.3\" xref=\"S2.p2.16.m16.1.1.6.2.3.cmml\">GeV</mi></mrow><mo id=\"S2.p2.16.m16.1.1.6.1\" xref=\"S2.p2.16.m16.1.1.6.1.cmml\">/</mo><msup id=\"S2.p2.16.m16.1.1.6.3\" xref=\"S2.p2.16.m16.1.1.6.3.cmml\"><mi id=\"S2.p2.16.m16.1.1.6.3.2\" xref=\"S2.p2.16.m16.1.1.6.3.2.cmml\">c</mi><mn id=\"S2.p2.16.m16.1.1.6.3.3\" xref=\"S2.p2.16.m16.1.1.6.3.3.cmml\">2</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p2.16.m16.1b\"><apply id=\"S2.p2.16.m16.1.1.cmml\" xref=\"S2.p2.16.m16.1.1\"><and id=\"S2.p2.16.m16.1.1a.cmml\" xref=\"S2.p2.16.m16.1.1\"></and><apply id=\"S2.p2.16.m16.1.1b.cmml\" xref=\"S2.p2.16.m16.1.1\"><lt id=\"S2.p2.16.m16.1.1.3.cmml\" xref=\"S2.p2.16.m16.1.1.3\"></lt><cn type=\"float\" id=\"S2.p2.16.m16.1.1.2.cmml\" xref=\"S2.p2.16.m16.1.1.2\">5.27</cn><apply id=\"S2.p2.16.m16.1.1.4.cmml\" xref=\"S2.p2.16.m16.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S2.p2.16.m16.1.1.4.1.cmml\" xref=\"S2.p2.16.m16.1.1.4\">subscript</csymbol><ci id=\"S2.p2.16.m16.1.1.4.2.cmml\" xref=\"S2.p2.16.m16.1.1.4.2\">𝑀</ci><apply id=\"S2.p2.16.m16.1.1.4.3.cmml\" xref=\"S2.p2.16.m16.1.1.4.3\"><times id=\"S2.p2.16.m16.1.1.4.3.1.cmml\" xref=\"S2.p2.16.m16.1.1.4.3.1\"></times><ci id=\"S2.p2.16.m16.1.1.4.3.2.cmml\" xref=\"S2.p2.16.m16.1.1.4.3.2\">𝑏</ci><ci id=\"S2.p2.16.m16.1.1.4.3.3.cmml\" xref=\"S2.p2.16.m16.1.1.4.3.3\">𝑐</ci></apply></apply></apply><apply id=\"S2.p2.16.m16.1.1c.cmml\" xref=\"S2.p2.16.m16.1.1\"><lt id=\"S2.p2.16.m16.1.1.5.cmml\" xref=\"S2.p2.16.m16.1.1.5\"></lt><share href=\"#S2.p2.16.m16.1.1.4.cmml\" id=\"S2.p2.16.m16.1.1d.cmml\" xref=\"S2.p2.16.m16.1.1\"></share><apply id=\"S2.p2.16.m16.1.1.6.cmml\" xref=\"S2.p2.16.m16.1.1.6\"><divide id=\"S2.p2.16.m16.1.1.6.1.cmml\" xref=\"S2.p2.16.m16.1.1.6.1\"></divide><apply id=\"S2.p2.16.m16.1.1.6.2.cmml\" xref=\"S2.p2.16.m16.1.1.6.2\"><times id=\"S2.p2.16.m16.1.1.6.2.1.cmml\" xref=\"S2.p2.16.m16.1.1.6.2.1\"></times><cn type=\"float\" id=\"S2.p2.16.m16.1.1.6.2.2.cmml\" xref=\"S2.p2.16.m16.1.1.6.2.2\">5.29</cn><ci id=\"S2.p2.16.m16.1.1.6.2.3.cmml\" xref=\"S2.p2.16.m16.1.1.6.2.3\">GeV</ci></apply><apply id=\"S2.p2.16.m16.1.1.6.3.cmml\" xref=\"S2.p2.16.m16.1.1.6.3\"><csymbol cd=\"ambiguous\" id=\"S2.p2.16.m16.1.1.6.3.1.cmml\" xref=\"S2.p2.16.m16.1.1.6.3\">superscript</csymbol><ci id=\"S2.p2.16.m16.1.1.6.3.2.cmml\" xref=\"S2.p2.16.m16.1.1.6.3.2\">𝑐</ci><cn type=\"integer\" id=\"S2.p2.16.m16.1.1.6.3.3.cmml\" xref=\"S2.p2.16.m16.1.1.6.3.3\">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p2.16.m16.1c\">5.27&lt;M_{bc}&lt;5.29~{}\\mathrm{GeV/}c^{2}</annotation></semantics></math> and <math id=\"S2.p2.17.m17.1\" class=\"ltx_Math\" alttext=\"-0.05&lt;\\Delta E&lt;0.05~{}\\rm{GeV}\" display=\"inline\"><semantics id=\"S2.p2.17.m17.1a\"><mrow id=\"S2.p2.17.m17.1.1\" xref=\"S2.p2.17.m17.1.1.cmml\"><mrow id=\"S2.p2.17.m17.1.1.2\" xref=\"S2.p2.17.m17.1.1.2.cmml\"><mo id=\"S2.p2.17.m17.1.1.2a\" xref=\"S2.p2.17.m17.1.1.2.cmml\">−</mo><mn id=\"S2.p2.17.m17.1.1.2.2\" xref=\"S2.p2.17.m17.1.1.2.2.cmml\">0.05</mn></mrow><mo id=\"S2.p2.17.m17.1.1.3\" xref=\"S2.p2.17.m17.1.1.3.cmml\">&lt;</mo><mrow id=\"S2.p2.17.m17.1.1.4\" xref=\"S2.p2.17.m17.1.1.4.cmml\"><mi mathvariant=\"normal\" id=\"S2.p2.17.m17.1.1.4.2\" xref=\"S2.p2.17.m17.1.1.4.2.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p2.17.m17.1.1.4.1\" xref=\"S2.p2.17.m17.1.1.4.1.cmml\">​</mo><mi id=\"S2.p2.17.m17.1.1.4.3\" xref=\"S2.p2.17.m17.1.1.4.3.cmml\">E</mi></mrow><mo id=\"S2.p2.17.m17.1.1.5\" xref=\"S2.p2.17.m17.1.1.5.cmml\">&lt;</mo><mrow id=\"S2.p2.17.m17.1.1.6\" xref=\"S2.p2.17.m17.1.1.6.cmml\"><mn id=\"S2.p2.17.m17.1.1.6.2\" xref=\"S2.p2.17.m17.1.1.6.2.cmml\">0.05</mn><mo lspace=\"0.330em\" rspace=\"0em\" id=\"S2.p2.17.m17.1.1.6.1\" xref=\"S2.p2.17.m17.1.1.6.1.cmml\">​</mo><mi id=\"S2.p2.17.m17.1.1.6.3\" xref=\"S2.p2.17.m17.1.1.6.3.cmml\">GeV</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p2.17.m17.1b\"><apply id=\"S2.p2.17.m17.1.1.cmml\" xref=\"S2.p2.17.m17.1.1\"><and id=\"S2.p2.17.m17.1.1a.cmml\" xref=\"S2.p2.17.m17.1.1\"></and><apply id=\"S2.p2.17.m17.1.1b.cmml\" xref=\"S2.p2.17.m17.1.1\"><lt id=\"S2.p2.17.m17.1.1.3.cmml\" xref=\"S2.p2.17.m17.1.1.3\"></lt><apply id=\"S2.p2.17.m17.1.1.2.cmml\" xref=\"S2.p2.17.m17.1.1.2\"><minus id=\"S2.p2.17.m17.1.1.2.1.cmml\" xref=\"S2.p2.17.m17.1.1.2\"></minus><cn type=\"float\" id=\"S2.p2.17.m17.1.1.2.2.cmml\" xref=\"S2.p2.17.m17.1.1.2.2\">0.05</cn></apply><apply id=\"S2.p2.17.m17.1.1.4.cmml\" xref=\"S2.p2.17.m17.1.1.4\"><times id=\"S2.p2.17.m17.1.1.4.1.cmml\" xref=\"S2.p2.17.m17.1.1.4.1\"></times><ci id=\"S2.p2.17.m17.1.1.4.2.cmml\" xref=\"S2.p2.17.m17.1.1.4.2\">Δ</ci><ci id=\"S2.p2.17.m17.1.1.4.3.cmml\" xref=\"S2.p2.17.m17.1.1.4.3\">𝐸</ci></apply></apply><apply id=\"S2.p2.17.m17.1.1c.cmml\" xref=\"S2.p2.17.m17.1.1\"><lt id=\"S2.p2.17.m17.1.1.5.cmml\" xref=\"S2.p2.17.m17.1.1.5\"></lt><share href=\"#S2.p2.17.m17.1.1.4.cmml\" id=\"S2.p2.17.m17.1.1d.cmml\" xref=\"S2.p2.17.m17.1.1\"></share><apply id=\"S2.p2.17.m17.1.1.6.cmml\" xref=\"S2.p2.17.m17.1.1.6\"><times id=\"S2.p2.17.m17.1.1.6.1.cmml\" xref=\"S2.p2.17.m17.1.1.6.1\"></times><cn type=\"float\" id=\"S2.p2.17.m17.1.1.6.2.cmml\" xref=\"S2.p2.17.m17.1.1.6.2\">0.05</cn><ci id=\"S2.p2.17.m17.1.1.6.3.cmml\" xref=\"S2.p2.17.m17.1.1.6.3\">GeV</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p2.17.m17.1c\">-0.05&lt;\\Delta E&lt;0.05~{}\\rm{GeV}</annotation></semantics></math>. When multiple <math id=\"S2.p2.18.m18.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S2.p2.18.m18.1a\"><mi id=\"S2.p2.18.m18.1.1\" xref=\"S2.p2.18.m18.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p2.18.m18.1b\"><ci id=\"S2.p2.18.m18.1.1.cmml\" xref=\"S2.p2.18.m18.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p2.18.m18.1c\">B</annotation></semantics></math> candidates are present in an event, we choose the candidate with the best fit quality from the <math id=\"S2.p2.19.m19.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S2.p2.19.m19.1a\"><mi id=\"S2.p2.19.m19.1.1\" xref=\"S2.p2.19.m19.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p2.19.m19.1b\"><ci id=\"S2.p2.19.m19.1.1.cmml\" xref=\"S2.p2.19.m19.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p2.19.m19.1c\">B</annotation></semantics></math> vertex fit. This is done for 19% of events and the selection efficiency is 92%.</p>\n",
      "</div>\n",
      "<div id=\"S2.p3\" class=\"ltx_para\">\n",
      "<p id=\"S2.p3.24\" class=\"ltx_p\">The dominant backgrounds are from <math id=\"S2.p3.1.m1.5\" class=\"ltx_Math\" alttext=\"e^{+}e^{-}\\rightarrow q\\bar{q}~{}(q=u,d,s,c)\" display=\"inline\"><semantics id=\"S2.p3.1.m1.5a\"><mrow id=\"S2.p3.1.m1.5.5\" xref=\"S2.p3.1.m1.5.5.cmml\"><mrow id=\"S2.p3.1.m1.5.5.3\" xref=\"S2.p3.1.m1.5.5.3.cmml\"><msup id=\"S2.p3.1.m1.5.5.3.2\" xref=\"S2.p3.1.m1.5.5.3.2.cmml\"><mi id=\"S2.p3.1.m1.5.5.3.2.2\" xref=\"S2.p3.1.m1.5.5.3.2.2.cmml\">e</mi><mo id=\"S2.p3.1.m1.5.5.3.2.3\" xref=\"S2.p3.1.m1.5.5.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.1.m1.5.5.3.1\" xref=\"S2.p3.1.m1.5.5.3.1.cmml\">​</mo><msup id=\"S2.p3.1.m1.5.5.3.3\" xref=\"S2.p3.1.m1.5.5.3.3.cmml\"><mi id=\"S2.p3.1.m1.5.5.3.3.2\" xref=\"S2.p3.1.m1.5.5.3.3.2.cmml\">e</mi><mo id=\"S2.p3.1.m1.5.5.3.3.3\" xref=\"S2.p3.1.m1.5.5.3.3.3.cmml\">−</mo></msup></mrow><mo stretchy=\"false\" id=\"S2.p3.1.m1.5.5.2\" xref=\"S2.p3.1.m1.5.5.2.cmml\">→</mo><mrow id=\"S2.p3.1.m1.5.5.1\" xref=\"S2.p3.1.m1.5.5.1.cmml\"><mi id=\"S2.p3.1.m1.5.5.1.3\" xref=\"S2.p3.1.m1.5.5.1.3.cmml\">q</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.1.m1.5.5.1.2\" xref=\"S2.p3.1.m1.5.5.1.2.cmml\">​</mo><mover accent=\"true\" id=\"S2.p3.1.m1.5.5.1.4\" xref=\"S2.p3.1.m1.5.5.1.4.cmml\"><mi id=\"S2.p3.1.m1.5.5.1.4.2\" xref=\"S2.p3.1.m1.5.5.1.4.2.cmml\">q</mi><mo id=\"S2.p3.1.m1.5.5.1.4.1\" xref=\"S2.p3.1.m1.5.5.1.4.1.cmml\">¯</mo></mover><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.1.m1.5.5.1.2a\" xref=\"S2.p3.1.m1.5.5.1.2.cmml\">​</mo><mrow id=\"S2.p3.1.m1.5.5.1.1.1\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S2.p3.1.m1.5.5.1.1.1.2\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.cmml\">(</mo><mrow id=\"S2.p3.1.m1.5.5.1.1.1.1\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.cmml\"><mi id=\"S2.p3.1.m1.5.5.1.1.1.1.2\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.2.cmml\">q</mi><mo id=\"S2.p3.1.m1.5.5.1.1.1.1.1\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.1.cmml\">=</mo><mrow id=\"S2.p3.1.m1.5.5.1.1.1.1.3.2\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.3.1.cmml\"><mi id=\"S2.p3.1.m1.1.1\" xref=\"S2.p3.1.m1.1.1.cmml\">u</mi><mo id=\"S2.p3.1.m1.5.5.1.1.1.1.3.2.1\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.3.1.cmml\">,</mo><mi id=\"S2.p3.1.m1.2.2\" xref=\"S2.p3.1.m1.2.2.cmml\">d</mi><mo id=\"S2.p3.1.m1.5.5.1.1.1.1.3.2.2\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.3.1.cmml\">,</mo><mi id=\"S2.p3.1.m1.3.3\" xref=\"S2.p3.1.m1.3.3.cmml\">s</mi><mo id=\"S2.p3.1.m1.5.5.1.1.1.1.3.2.3\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.3.1.cmml\">,</mo><mi id=\"S2.p3.1.m1.4.4\" xref=\"S2.p3.1.m1.4.4.cmml\">c</mi></mrow></mrow><mo stretchy=\"false\" id=\"S2.p3.1.m1.5.5.1.1.1.3\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.1.m1.5b\"><apply id=\"S2.p3.1.m1.5.5.cmml\" xref=\"S2.p3.1.m1.5.5\"><ci id=\"S2.p3.1.m1.5.5.2.cmml\" xref=\"S2.p3.1.m1.5.5.2\">→</ci><apply id=\"S2.p3.1.m1.5.5.3.cmml\" xref=\"S2.p3.1.m1.5.5.3\"><times id=\"S2.p3.1.m1.5.5.3.1.cmml\" xref=\"S2.p3.1.m1.5.5.3.1\"></times><apply id=\"S2.p3.1.m1.5.5.3.2.cmml\" xref=\"S2.p3.1.m1.5.5.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.1.m1.5.5.3.2.1.cmml\" xref=\"S2.p3.1.m1.5.5.3.2\">superscript</csymbol><ci id=\"S2.p3.1.m1.5.5.3.2.2.cmml\" xref=\"S2.p3.1.m1.5.5.3.2.2\">𝑒</ci><plus id=\"S2.p3.1.m1.5.5.3.2.3.cmml\" xref=\"S2.p3.1.m1.5.5.3.2.3\"></plus></apply><apply id=\"S2.p3.1.m1.5.5.3.3.cmml\" xref=\"S2.p3.1.m1.5.5.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p3.1.m1.5.5.3.3.1.cmml\" xref=\"S2.p3.1.m1.5.5.3.3\">superscript</csymbol><ci id=\"S2.p3.1.m1.5.5.3.3.2.cmml\" xref=\"S2.p3.1.m1.5.5.3.3.2\">𝑒</ci><minus id=\"S2.p3.1.m1.5.5.3.3.3.cmml\" xref=\"S2.p3.1.m1.5.5.3.3.3\"></minus></apply></apply><apply id=\"S2.p3.1.m1.5.5.1.cmml\" xref=\"S2.p3.1.m1.5.5.1\"><times id=\"S2.p3.1.m1.5.5.1.2.cmml\" xref=\"S2.p3.1.m1.5.5.1.2\"></times><ci id=\"S2.p3.1.m1.5.5.1.3.cmml\" xref=\"S2.p3.1.m1.5.5.1.3\">𝑞</ci><apply id=\"S2.p3.1.m1.5.5.1.4.cmml\" xref=\"S2.p3.1.m1.5.5.1.4\"><ci id=\"S2.p3.1.m1.5.5.1.4.1.cmml\" xref=\"S2.p3.1.m1.5.5.1.4.1\">¯</ci><ci id=\"S2.p3.1.m1.5.5.1.4.2.cmml\" xref=\"S2.p3.1.m1.5.5.1.4.2\">𝑞</ci></apply><apply id=\"S2.p3.1.m1.5.5.1.1.1.1.cmml\" xref=\"S2.p3.1.m1.5.5.1.1.1\"><eq id=\"S2.p3.1.m1.5.5.1.1.1.1.1.cmml\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.1\"></eq><ci id=\"S2.p3.1.m1.5.5.1.1.1.1.2.cmml\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.2\">𝑞</ci><list id=\"S2.p3.1.m1.5.5.1.1.1.1.3.1.cmml\" xref=\"S2.p3.1.m1.5.5.1.1.1.1.3.2\"><ci id=\"S2.p3.1.m1.1.1.cmml\" xref=\"S2.p3.1.m1.1.1\">𝑢</ci><ci id=\"S2.p3.1.m1.2.2.cmml\" xref=\"S2.p3.1.m1.2.2\">𝑑</ci><ci id=\"S2.p3.1.m1.3.3.cmml\" xref=\"S2.p3.1.m1.3.3\">𝑠</ci><ci id=\"S2.p3.1.m1.4.4.cmml\" xref=\"S2.p3.1.m1.4.4\">𝑐</ci></list></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.1.m1.5c\">e^{+}e^{-}\\rightarrow q\\bar{q}~{}(q=u,d,s,c)</annotation></semantics></math> continuum process. The <math id=\"S2.p3.2.m2.1\" class=\"ltx_Math\" alttext=\"B\\bar{B}\" display=\"inline\"><semantics id=\"S2.p3.2.m2.1a\"><mrow id=\"S2.p3.2.m2.1.1\" xref=\"S2.p3.2.m2.1.1.cmml\"><mi id=\"S2.p3.2.m2.1.1.2\" xref=\"S2.p3.2.m2.1.1.2.cmml\">B</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.2.m2.1.1.1\" xref=\"S2.p3.2.m2.1.1.1.cmml\">​</mo><mover accent=\"true\" id=\"S2.p3.2.m2.1.1.3\" xref=\"S2.p3.2.m2.1.1.3.cmml\"><mi id=\"S2.p3.2.m2.1.1.3.2\" xref=\"S2.p3.2.m2.1.1.3.2.cmml\">B</mi><mo id=\"S2.p3.2.m2.1.1.3.1\" xref=\"S2.p3.2.m2.1.1.3.1.cmml\">¯</mo></mover></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.2.m2.1b\"><apply id=\"S2.p3.2.m2.1.1.cmml\" xref=\"S2.p3.2.m2.1.1\"><times id=\"S2.p3.2.m2.1.1.1.cmml\" xref=\"S2.p3.2.m2.1.1.1\"></times><ci id=\"S2.p3.2.m2.1.1.2.cmml\" xref=\"S2.p3.2.m2.1.1.2\">𝐵</ci><apply id=\"S2.p3.2.m2.1.1.3.cmml\" xref=\"S2.p3.2.m2.1.1.3\"><ci id=\"S2.p3.2.m2.1.1.3.1.cmml\" xref=\"S2.p3.2.m2.1.1.3.1\">¯</ci><ci id=\"S2.p3.2.m2.1.1.3.2.cmml\" xref=\"S2.p3.2.m2.1.1.3.2\">𝐵</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.2.m2.1c\">B\\bar{B}</annotation></semantics></math> events are spherical in shape whereas the particles from continuum events are collimated into two back-to-back jets. We make use of this difference in event topology by using a neural network [8] to combine several shape variables along with other properties of the event that distinguish <math id=\"S2.p3.3.m3.1\" class=\"ltx_Math\" alttext=\"q\\bar{q}\" display=\"inline\"><semantics id=\"S2.p3.3.m3.1a\"><mrow id=\"S2.p3.3.m3.1.1\" xref=\"S2.p3.3.m3.1.1.cmml\"><mi id=\"S2.p3.3.m3.1.1.2\" xref=\"S2.p3.3.m3.1.1.2.cmml\">q</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.3.m3.1.1.1\" xref=\"S2.p3.3.m3.1.1.1.cmml\">​</mo><mover accent=\"true\" id=\"S2.p3.3.m3.1.1.3\" xref=\"S2.p3.3.m3.1.1.3.cmml\"><mi id=\"S2.p3.3.m3.1.1.3.2\" xref=\"S2.p3.3.m3.1.1.3.2.cmml\">q</mi><mo id=\"S2.p3.3.m3.1.1.3.1\" xref=\"S2.p3.3.m3.1.1.3.1.cmml\">¯</mo></mover></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.3.m3.1b\"><apply id=\"S2.p3.3.m3.1.1.cmml\" xref=\"S2.p3.3.m3.1.1\"><times id=\"S2.p3.3.m3.1.1.1.cmml\" xref=\"S2.p3.3.m3.1.1.1\"></times><ci id=\"S2.p3.3.m3.1.1.2.cmml\" xref=\"S2.p3.3.m3.1.1.2\">𝑞</ci><apply id=\"S2.p3.3.m3.1.1.3.cmml\" xref=\"S2.p3.3.m3.1.1.3\"><ci id=\"S2.p3.3.m3.1.1.3.1.cmml\" xref=\"S2.p3.3.m3.1.1.3.1\">¯</ci><ci id=\"S2.p3.3.m3.1.1.3.2.cmml\" xref=\"S2.p3.3.m3.1.1.3.2\">𝑞</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.3.m3.1c\">q\\bar{q}</annotation></semantics></math> from <math id=\"S2.p3.4.m4.1\" class=\"ltx_Math\" alttext=\"B\\bar{B}\" display=\"inline\"><semantics id=\"S2.p3.4.m4.1a\"><mrow id=\"S2.p3.4.m4.1.1\" xref=\"S2.p3.4.m4.1.1.cmml\"><mi id=\"S2.p3.4.m4.1.1.2\" xref=\"S2.p3.4.m4.1.1.2.cmml\">B</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.4.m4.1.1.1\" xref=\"S2.p3.4.m4.1.1.1.cmml\">​</mo><mover accent=\"true\" id=\"S2.p3.4.m4.1.1.3\" xref=\"S2.p3.4.m4.1.1.3.cmml\"><mi id=\"S2.p3.4.m4.1.1.3.2\" xref=\"S2.p3.4.m4.1.1.3.2.cmml\">B</mi><mo id=\"S2.p3.4.m4.1.1.3.1\" xref=\"S2.p3.4.m4.1.1.3.1.cmml\">¯</mo></mover></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.4.m4.1b\"><apply id=\"S2.p3.4.m4.1.1.cmml\" xref=\"S2.p3.4.m4.1.1\"><times id=\"S2.p3.4.m4.1.1.1.cmml\" xref=\"S2.p3.4.m4.1.1.1\"></times><ci id=\"S2.p3.4.m4.1.1.2.cmml\" xref=\"S2.p3.4.m4.1.1.2\">𝐵</ci><apply id=\"S2.p3.4.m4.1.1.3.cmml\" xref=\"S2.p3.4.m4.1.1.3\"><ci id=\"S2.p3.4.m4.1.1.3.1.cmml\" xref=\"S2.p3.4.m4.1.1.3.1\">¯</ci><ci id=\"S2.p3.4.m4.1.1.3.2.cmml\" xref=\"S2.p3.4.m4.1.1.3.2\">𝐵</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.4.m4.1c\">B\\bar{B}</annotation></semantics></math> events. A requirement on the neural network output (<math id=\"S2.p3.5.m5.1\" class=\"ltx_Math\" alttext=\"C_{NN}\" display=\"inline\"><semantics id=\"S2.p3.5.m5.1a\"><msub id=\"S2.p3.5.m5.1.1\" xref=\"S2.p3.5.m5.1.1.cmml\"><mi id=\"S2.p3.5.m5.1.1.2\" xref=\"S2.p3.5.m5.1.1.2.cmml\">C</mi><mrow id=\"S2.p3.5.m5.1.1.3\" xref=\"S2.p3.5.m5.1.1.3.cmml\"><mi id=\"S2.p3.5.m5.1.1.3.2\" xref=\"S2.p3.5.m5.1.1.3.2.cmml\">N</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.5.m5.1.1.3.1\" xref=\"S2.p3.5.m5.1.1.3.1.cmml\">​</mo><mi id=\"S2.p3.5.m5.1.1.3.3\" xref=\"S2.p3.5.m5.1.1.3.3.cmml\">N</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.5.m5.1b\"><apply id=\"S2.p3.5.m5.1.1.cmml\" xref=\"S2.p3.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p3.5.m5.1.1.1.cmml\" xref=\"S2.p3.5.m5.1.1\">subscript</csymbol><ci id=\"S2.p3.5.m5.1.1.2.cmml\" xref=\"S2.p3.5.m5.1.1.2\">𝐶</ci><apply id=\"S2.p3.5.m5.1.1.3.cmml\" xref=\"S2.p3.5.m5.1.1.3\"><times id=\"S2.p3.5.m5.1.1.3.1.cmml\" xref=\"S2.p3.5.m5.1.1.3.1\"></times><ci id=\"S2.p3.5.m5.1.1.3.2.cmml\" xref=\"S2.p3.5.m5.1.1.3.2\">𝑁</ci><ci id=\"S2.p3.5.m5.1.1.3.3.cmml\" xref=\"S2.p3.5.m5.1.1.3.3\">𝑁</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.5.m5.1c\">C_{NN}</annotation></semantics></math> <math id=\"S2.p3.6.m6.1\" class=\"ltx_Math\" alttext=\"&gt;0.88\" display=\"inline\"><semantics id=\"S2.p3.6.m6.1a\"><mrow id=\"S2.p3.6.m6.1.1\" xref=\"S2.p3.6.m6.1.1.cmml\"><mi id=\"S2.p3.6.m6.1.1.2\" xref=\"S2.p3.6.m6.1.1.2.cmml\"></mi><mo id=\"S2.p3.6.m6.1.1.1\" xref=\"S2.p3.6.m6.1.1.1.cmml\">&gt;</mo><mn id=\"S2.p3.6.m6.1.1.3\" xref=\"S2.p3.6.m6.1.1.3.cmml\">0.88</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.6.m6.1b\"><apply id=\"S2.p3.6.m6.1.1.cmml\" xref=\"S2.p3.6.m6.1.1\"><gt id=\"S2.p3.6.m6.1.1.1.cmml\" xref=\"S2.p3.6.m6.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S2.p3.6.m6.1.1.2.cmml\" xref=\"S2.p3.6.m6.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S2.p3.6.m6.1.1.3.cmml\" xref=\"S2.p3.6.m6.1.1.3\">0.88</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.6.m6.1c\">&gt;0.88</annotation></semantics></math>) is applied to suppress continuum background. This selection requirement is optimized by maximizing a figure of merit defined as <math id=\"S2.p3.7.m7.1\" class=\"ltx_Math\" alttext=\"\\frac{N_{S}}{N_{S}+N_{B}}\" display=\"inline\"><semantics id=\"S2.p3.7.m7.1a\"><mfrac id=\"S2.p3.7.m7.1.1\" xref=\"S2.p3.7.m7.1.1.cmml\"><msub id=\"S2.p3.7.m7.1.1.2\" xref=\"S2.p3.7.m7.1.1.2.cmml\"><mi id=\"S2.p3.7.m7.1.1.2.2\" xref=\"S2.p3.7.m7.1.1.2.2.cmml\">N</mi><mi id=\"S2.p3.7.m7.1.1.2.3\" xref=\"S2.p3.7.m7.1.1.2.3.cmml\">S</mi></msub><mrow id=\"S2.p3.7.m7.1.1.3\" xref=\"S2.p3.7.m7.1.1.3.cmml\"><msub id=\"S2.p3.7.m7.1.1.3.2\" xref=\"S2.p3.7.m7.1.1.3.2.cmml\"><mi id=\"S2.p3.7.m7.1.1.3.2.2\" xref=\"S2.p3.7.m7.1.1.3.2.2.cmml\">N</mi><mi id=\"S2.p3.7.m7.1.1.3.2.3\" xref=\"S2.p3.7.m7.1.1.3.2.3.cmml\">S</mi></msub><mo id=\"S2.p3.7.m7.1.1.3.1\" xref=\"S2.p3.7.m7.1.1.3.1.cmml\">+</mo><msub id=\"S2.p3.7.m7.1.1.3.3\" xref=\"S2.p3.7.m7.1.1.3.3.cmml\"><mi id=\"S2.p3.7.m7.1.1.3.3.2\" xref=\"S2.p3.7.m7.1.1.3.3.2.cmml\">N</mi><mi id=\"S2.p3.7.m7.1.1.3.3.3\" xref=\"S2.p3.7.m7.1.1.3.3.3.cmml\">B</mi></msub></mrow></mfrac><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.7.m7.1b\"><apply id=\"S2.p3.7.m7.1.1.cmml\" xref=\"S2.p3.7.m7.1.1\"><divide id=\"S2.p3.7.m7.1.1.1.cmml\" xref=\"S2.p3.7.m7.1.1\"></divide><apply id=\"S2.p3.7.m7.1.1.2.cmml\" xref=\"S2.p3.7.m7.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.7.m7.1.1.2.1.cmml\" xref=\"S2.p3.7.m7.1.1.2\">subscript</csymbol><ci id=\"S2.p3.7.m7.1.1.2.2.cmml\" xref=\"S2.p3.7.m7.1.1.2.2\">𝑁</ci><ci id=\"S2.p3.7.m7.1.1.2.3.cmml\" xref=\"S2.p3.7.m7.1.1.2.3\">𝑆</ci></apply><apply id=\"S2.p3.7.m7.1.1.3.cmml\" xref=\"S2.p3.7.m7.1.1.3\"><plus id=\"S2.p3.7.m7.1.1.3.1.cmml\" xref=\"S2.p3.7.m7.1.1.3.1\"></plus><apply id=\"S2.p3.7.m7.1.1.3.2.cmml\" xref=\"S2.p3.7.m7.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.7.m7.1.1.3.2.1.cmml\" xref=\"S2.p3.7.m7.1.1.3.2\">subscript</csymbol><ci id=\"S2.p3.7.m7.1.1.3.2.2.cmml\" xref=\"S2.p3.7.m7.1.1.3.2.2\">𝑁</ci><ci id=\"S2.p3.7.m7.1.1.3.2.3.cmml\" xref=\"S2.p3.7.m7.1.1.3.2.3\">𝑆</ci></apply><apply id=\"S2.p3.7.m7.1.1.3.3.cmml\" xref=\"S2.p3.7.m7.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p3.7.m7.1.1.3.3.1.cmml\" xref=\"S2.p3.7.m7.1.1.3.3\">subscript</csymbol><ci id=\"S2.p3.7.m7.1.1.3.3.2.cmml\" xref=\"S2.p3.7.m7.1.1.3.3.2\">𝑁</ci><ci id=\"S2.p3.7.m7.1.1.3.3.3.cmml\" xref=\"S2.p3.7.m7.1.1.3.3.3\">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.7.m7.1c\">\\frac{N_{S}}{N_{S}+N_{B}}</annotation></semantics></math>, where <math id=\"S2.p3.8.m8.1\" class=\"ltx_Math\" alttext=\"N_{S}\" display=\"inline\"><semantics id=\"S2.p3.8.m8.1a\"><msub id=\"S2.p3.8.m8.1.1\" xref=\"S2.p3.8.m8.1.1.cmml\"><mi id=\"S2.p3.8.m8.1.1.2\" xref=\"S2.p3.8.m8.1.1.2.cmml\">N</mi><mi id=\"S2.p3.8.m8.1.1.3\" xref=\"S2.p3.8.m8.1.1.3.cmml\">S</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.8.m8.1b\"><apply id=\"S2.p3.8.m8.1.1.cmml\" xref=\"S2.p3.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p3.8.m8.1.1.1.cmml\" xref=\"S2.p3.8.m8.1.1\">subscript</csymbol><ci id=\"S2.p3.8.m8.1.1.2.cmml\" xref=\"S2.p3.8.m8.1.1.2\">𝑁</ci><ci id=\"S2.p3.8.m8.1.1.3.cmml\" xref=\"S2.p3.8.m8.1.1.3\">𝑆</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.8.m8.1c\">N_{S}</annotation></semantics></math> (<math id=\"S2.p3.9.m9.1\" class=\"ltx_Math\" alttext=\"N_{B}\" display=\"inline\"><semantics id=\"S2.p3.9.m9.1a\"><msub id=\"S2.p3.9.m9.1.1\" xref=\"S2.p3.9.m9.1.1.cmml\"><mi id=\"S2.p3.9.m9.1.1.2\" xref=\"S2.p3.9.m9.1.1.2.cmml\">N</mi><mi id=\"S2.p3.9.m9.1.1.3\" xref=\"S2.p3.9.m9.1.1.3.cmml\">B</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.9.m9.1b\"><apply id=\"S2.p3.9.m9.1.1.cmml\" xref=\"S2.p3.9.m9.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p3.9.m9.1.1.1.cmml\" xref=\"S2.p3.9.m9.1.1\">subscript</csymbol><ci id=\"S2.p3.9.m9.1.1.2.cmml\" xref=\"S2.p3.9.m9.1.1.2\">𝑁</ci><ci id=\"S2.p3.9.m9.1.1.3.cmml\" xref=\"S2.p3.9.m9.1.1.3\">𝐵</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.9.m9.1c\">N_{B}</annotation></semantics></math>) denotes the expected number of signal (background) events in the signal-enhanced region. Background contributions from <math id=\"S2.p3.10.m10.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S2.p3.10.m10.1a\"><mi id=\"S2.p3.10.m10.1.1\" xref=\"S2.p3.10.m10.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.10.m10.1b\"><ci id=\"S2.p3.10.m10.1.1.cmml\" xref=\"S2.p3.10.m10.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.10.m10.1c\">B</annotation></semantics></math> decays mediated by the dominant <math id=\"S2.p3.11.m11.1\" class=\"ltx_Math\" alttext=\"b\\rightarrow c\" display=\"inline\"><semantics id=\"S2.p3.11.m11.1a\"><mrow id=\"S2.p3.11.m11.1.1\" xref=\"S2.p3.11.m11.1.1.cmml\"><mi id=\"S2.p3.11.m11.1.1.2\" xref=\"S2.p3.11.m11.1.1.2.cmml\">b</mi><mo stretchy=\"false\" id=\"S2.p3.11.m11.1.1.1\" xref=\"S2.p3.11.m11.1.1.1.cmml\">→</mo><mi id=\"S2.p3.11.m11.1.1.3\" xref=\"S2.p3.11.m11.1.1.3.cmml\">c</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.11.m11.1b\"><apply id=\"S2.p3.11.m11.1.1.cmml\" xref=\"S2.p3.11.m11.1.1\"><ci id=\"S2.p3.11.m11.1.1.1.cmml\" xref=\"S2.p3.11.m11.1.1.1\">→</ci><ci id=\"S2.p3.11.m11.1.1.2.cmml\" xref=\"S2.p3.11.m11.1.1.2\">𝑏</ci><ci id=\"S2.p3.11.m11.1.1.3.cmml\" xref=\"S2.p3.11.m11.1.1.3\">𝑐</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.11.m11.1c\">b\\rightarrow c</annotation></semantics></math> transition are investigated with an MC sample of such decays. To suppress these backgrounds, candidates for which the invariant mass of the <math id=\"S2.p3.12.m12.1\" class=\"ltx_Math\" alttext=\"K^{+}K^{-}\" display=\"inline\"><semantics id=\"S2.p3.12.m12.1a\"><mrow id=\"S2.p3.12.m12.1.1\" xref=\"S2.p3.12.m12.1.1.cmml\"><msup id=\"S2.p3.12.m12.1.1.2\" xref=\"S2.p3.12.m12.1.1.2.cmml\"><mi id=\"S2.p3.12.m12.1.1.2.2\" xref=\"S2.p3.12.m12.1.1.2.2.cmml\">K</mi><mo id=\"S2.p3.12.m12.1.1.2.3\" xref=\"S2.p3.12.m12.1.1.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.12.m12.1.1.1\" xref=\"S2.p3.12.m12.1.1.1.cmml\">​</mo><msup id=\"S2.p3.12.m12.1.1.3\" xref=\"S2.p3.12.m12.1.1.3.cmml\"><mi id=\"S2.p3.12.m12.1.1.3.2\" xref=\"S2.p3.12.m12.1.1.3.2.cmml\">K</mi><mo id=\"S2.p3.12.m12.1.1.3.3\" xref=\"S2.p3.12.m12.1.1.3.3.cmml\">−</mo></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.12.m12.1b\"><apply id=\"S2.p3.12.m12.1.1.cmml\" xref=\"S2.p3.12.m12.1.1\"><times id=\"S2.p3.12.m12.1.1.1.cmml\" xref=\"S2.p3.12.m12.1.1.1\"></times><apply id=\"S2.p3.12.m12.1.1.2.cmml\" xref=\"S2.p3.12.m12.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.12.m12.1.1.2.1.cmml\" xref=\"S2.p3.12.m12.1.1.2\">superscript</csymbol><ci id=\"S2.p3.12.m12.1.1.2.2.cmml\" xref=\"S2.p3.12.m12.1.1.2.2\">𝐾</ci><plus id=\"S2.p3.12.m12.1.1.2.3.cmml\" xref=\"S2.p3.12.m12.1.1.2.3\"></plus></apply><apply id=\"S2.p3.12.m12.1.1.3.cmml\" xref=\"S2.p3.12.m12.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S2.p3.12.m12.1.1.3.1.cmml\" xref=\"S2.p3.12.m12.1.1.3\">superscript</csymbol><ci id=\"S2.p3.12.m12.1.1.3.2.cmml\" xref=\"S2.p3.12.m12.1.1.3.2\">𝐾</ci><minus id=\"S2.p3.12.m12.1.1.3.3.cmml\" xref=\"S2.p3.12.m12.1.1.3.3\"></minus></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.12.m12.1c\">K^{+}K^{-}</annotation></semantics></math> or <math id=\"S2.p3.13.m13.1\" class=\"ltx_Math\" alttext=\"K^{+}\\pi^{-}\" display=\"inline\"><semantics id=\"S2.p3.13.m13.1a\"><mrow id=\"S2.p3.13.m13.1.1\" xref=\"S2.p3.13.m13.1.1.cmml\"><msup id=\"S2.p3.13.m13.1.1.2\" xref=\"S2.p3.13.m13.1.1.2.cmml\"><mi id=\"S2.p3.13.m13.1.1.2.2\" xref=\"S2.p3.13.m13.1.1.2.2.cmml\">K</mi><mo id=\"S2.p3.13.m13.1.1.2.3\" xref=\"S2.p3.13.m13.1.1.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.13.m13.1.1.1\" xref=\"S2.p3.13.m13.1.1.1.cmml\">​</mo><msup id=\"S2.p3.13.m13.1.1.3\" xref=\"S2.p3.13.m13.1.1.3.cmml\"><mi id=\"S2.p3.13.m13.1.1.3.2\" xref=\"S2.p3.13.m13.1.1.3.2.cmml\">π</mi><mo id=\"S2.p3.13.m13.1.1.3.3\" xref=\"S2.p3.13.m13.1.1.3.3.cmml\">−</mo></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.13.m13.1b\"><apply id=\"S2.p3.13.m13.1.1.cmml\" xref=\"S2.p3.13.m13.1.1\"><times id=\"S2.p3.13.m13.1.1.1.cmml\" xref=\"S2.p3.13.m13.1.1.1\"></times><apply id=\"S2.p3.13.m13.1.1.2.cmml\" xref=\"S2.p3.13.m13.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.13.m13.1.1.2.1.cmml\" xref=\"S2.p3.13.m13.1.1.2\">superscript</csymbol><ci id=\"S2.p3.13.m13.1.1.2.2.cmml\" xref=\"S2.p3.13.m13.1.1.2.2\">𝐾</ci><plus id=\"S2.p3.13.m13.1.1.2.3.cmml\" xref=\"S2.p3.13.m13.1.1.2.3\"></plus></apply><apply id=\"S2.p3.13.m13.1.1.3.cmml\" xref=\"S2.p3.13.m13.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S2.p3.13.m13.1.1.3.1.cmml\" xref=\"S2.p3.13.m13.1.1.3\">superscript</csymbol><ci id=\"S2.p3.13.m13.1.1.3.2.cmml\" xref=\"S2.p3.13.m13.1.1.3.2\">𝜋</ci><minus id=\"S2.p3.13.m13.1.1.3.3.cmml\" xref=\"S2.p3.13.m13.1.1.3.3\"></minus></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.13.m13.1c\">K^{+}\\pi^{-}</annotation></semantics></math> system lies in the range 185–188 MeV/<math id=\"S2.p3.14.m14.1\" class=\"ltx_Math\" alttext=\"c^{2}\" display=\"inline\"><semantics id=\"S2.p3.14.m14.1a\"><msup id=\"S2.p3.14.m14.1.1\" xref=\"S2.p3.14.m14.1.1.cmml\"><mi id=\"S2.p3.14.m14.1.1.2\" xref=\"S2.p3.14.m14.1.1.2.cmml\">c</mi><mn id=\"S2.p3.14.m14.1.1.3\" xref=\"S2.p3.14.m14.1.1.3.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.14.m14.1b\"><apply id=\"S2.p3.14.m14.1.1.cmml\" xref=\"S2.p3.14.m14.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p3.14.m14.1.1.1.cmml\" xref=\"S2.p3.14.m14.1.1\">superscript</csymbol><ci id=\"S2.p3.14.m14.1.1.2.cmml\" xref=\"S2.p3.14.m14.1.1.2\">𝑐</ci><cn type=\"integer\" id=\"S2.p3.14.m14.1.1.3.cmml\" xref=\"S2.p3.14.m14.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.14.m14.1c\">c^{2}</annotation></semantics></math> are removed. This selection window corresponds to <math id=\"S2.p3.15.m15.1\" class=\"ltx_Math\" alttext=\"\\pm 3.75\\sigma\" display=\"inline\"><semantics id=\"S2.p3.15.m15.1a\"><mrow id=\"S2.p3.15.m15.1.1\" xref=\"S2.p3.15.m15.1.1.cmml\"><mo id=\"S2.p3.15.m15.1.1a\" xref=\"S2.p3.15.m15.1.1.cmml\">±</mo><mrow id=\"S2.p3.15.m15.1.1.2\" xref=\"S2.p3.15.m15.1.1.2.cmml\"><mn id=\"S2.p3.15.m15.1.1.2.2\" xref=\"S2.p3.15.m15.1.1.2.2.cmml\">3.75</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.15.m15.1.1.2.1\" xref=\"S2.p3.15.m15.1.1.2.1.cmml\">​</mo><mi id=\"S2.p3.15.m15.1.1.2.3\" xref=\"S2.p3.15.m15.1.1.2.3.cmml\">σ</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.15.m15.1b\"><apply id=\"S2.p3.15.m15.1.1.cmml\" xref=\"S2.p3.15.m15.1.1\"><csymbol cd=\"latexml\" id=\"S2.p3.15.m15.1.1.1.cmml\" xref=\"S2.p3.15.m15.1.1\">plus-or-minus</csymbol><apply id=\"S2.p3.15.m15.1.1.2.cmml\" xref=\"S2.p3.15.m15.1.1.2\"><times id=\"S2.p3.15.m15.1.1.2.1.cmml\" xref=\"S2.p3.15.m15.1.1.2.1\"></times><cn type=\"float\" id=\"S2.p3.15.m15.1.1.2.2.cmml\" xref=\"S2.p3.15.m15.1.1.2.2\">3.75</cn><ci id=\"S2.p3.15.m15.1.1.2.3.cmml\" xref=\"S2.p3.15.m15.1.1.2.3\">𝜎</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.15.m15.1c\">\\pm 3.75\\sigma</annotation></semantics></math> around the nominal <math id=\"S2.p3.16.m16.1\" class=\"ltx_Math\" alttext=\"D^{0}\" display=\"inline\"><semantics id=\"S2.p3.16.m16.1a\"><msup id=\"S2.p3.16.m16.1.1\" xref=\"S2.p3.16.m16.1.1.cmml\"><mi id=\"S2.p3.16.m16.1.1.2\" xref=\"S2.p3.16.m16.1.1.2.cmml\">D</mi><mn id=\"S2.p3.16.m16.1.1.3\" xref=\"S2.p3.16.m16.1.1.3.cmml\">0</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.16.m16.1b\"><apply id=\"S2.p3.16.m16.1.1.cmml\" xref=\"S2.p3.16.m16.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p3.16.m16.1.1.1.cmml\" xref=\"S2.p3.16.m16.1.1\">superscript</csymbol><ci id=\"S2.p3.16.m16.1.1.2.cmml\" xref=\"S2.p3.16.m16.1.1.2\">𝐷</ci><cn type=\"integer\" id=\"S2.p3.16.m16.1.1.3.cmml\" xref=\"S2.p3.16.m16.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.16.m16.1c\">D^{0}</annotation></semantics></math> mass [9], where <math id=\"S2.p3.17.m17.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S2.p3.17.m17.1a\"><mi id=\"S2.p3.17.m17.1.1\" xref=\"S2.p3.17.m17.1.1.cmml\">σ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.17.m17.1b\"><ci id=\"S2.p3.17.m17.1.1.cmml\" xref=\"S2.p3.17.m17.1.1\">𝜎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.17.m17.1c\">\\sigma</annotation></semantics></math> is the mass resolution. Backgrounds from charmless <math id=\"S2.p3.18.m18.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S2.p3.18.m18.1a\"><mi id=\"S2.p3.18.m18.1.1\" xref=\"S2.p3.18.m18.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.18.m18.1b\"><ci id=\"S2.p3.18.m18.1.1.cmml\" xref=\"S2.p3.18.m18.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.18.m18.1c\">B</annotation></semantics></math> decays are studied with a large MC sample, where one of the <math id=\"S2.p3.19.m19.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S2.p3.19.m19.1a\"><mi id=\"S2.p3.19.m19.1.1\" xref=\"S2.p3.19.m19.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.19.m19.1b\"><ci id=\"S2.p3.19.m19.1.1.cmml\" xref=\"S2.p3.19.m19.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.19.m19.1c\">B</annotation></semantics></math> mesons decays via a process with a known branching fraction. The study reveals that a few modes contribute in the <math id=\"S2.p3.20.m20.1\" class=\"ltx_Math\" alttext=\"M_{bc}\" display=\"inline\"><semantics id=\"S2.p3.20.m20.1a\"><msub id=\"S2.p3.20.m20.1.1\" xref=\"S2.p3.20.m20.1.1.cmml\"><mi id=\"S2.p3.20.m20.1.1.2\" xref=\"S2.p3.20.m20.1.1.2.cmml\">M</mi><mrow id=\"S2.p3.20.m20.1.1.3\" xref=\"S2.p3.20.m20.1.1.3.cmml\"><mi id=\"S2.p3.20.m20.1.1.3.2\" xref=\"S2.p3.20.m20.1.1.3.2.cmml\">b</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.20.m20.1.1.3.1\" xref=\"S2.p3.20.m20.1.1.3.1.cmml\">​</mo><mi id=\"S2.p3.20.m20.1.1.3.3\" xref=\"S2.p3.20.m20.1.1.3.3.cmml\">c</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.20.m20.1b\"><apply id=\"S2.p3.20.m20.1.1.cmml\" xref=\"S2.p3.20.m20.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p3.20.m20.1.1.1.cmml\" xref=\"S2.p3.20.m20.1.1\">subscript</csymbol><ci id=\"S2.p3.20.m20.1.1.2.cmml\" xref=\"S2.p3.20.m20.1.1.2\">𝑀</ci><apply id=\"S2.p3.20.m20.1.1.3.cmml\" xref=\"S2.p3.20.m20.1.1.3\"><times id=\"S2.p3.20.m20.1.1.3.1.cmml\" xref=\"S2.p3.20.m20.1.1.3.1\"></times><ci id=\"S2.p3.20.m20.1.1.3.2.cmml\" xref=\"S2.p3.20.m20.1.1.3.2\">𝑏</ci><ci id=\"S2.p3.20.m20.1.1.3.3.cmml\" xref=\"S2.p3.20.m20.1.1.3.3\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.20.m20.1c\">M_{bc}</annotation></semantics></math> signal region with a corresponding <math id=\"S2.p3.21.m21.1\" class=\"ltx_Math\" alttext=\"\\Delta E\" display=\"inline\"><semantics id=\"S2.p3.21.m21.1a\"><mrow id=\"S2.p3.21.m21.1.1\" xref=\"S2.p3.21.m21.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.p3.21.m21.1.1.2\" xref=\"S2.p3.21.m21.1.1.2.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.21.m21.1.1.1\" xref=\"S2.p3.21.m21.1.1.1.cmml\">​</mo><mi id=\"S2.p3.21.m21.1.1.3\" xref=\"S2.p3.21.m21.1.1.3.cmml\">E</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.21.m21.1b\"><apply id=\"S2.p3.21.m21.1.1.cmml\" xref=\"S2.p3.21.m21.1.1\"><times id=\"S2.p3.21.m21.1.1.1.cmml\" xref=\"S2.p3.21.m21.1.1.1\"></times><ci id=\"S2.p3.21.m21.1.1.2.cmml\" xref=\"S2.p3.21.m21.1.1.2\">Δ</ci><ci id=\"S2.p3.21.m21.1.1.3.cmml\" xref=\"S2.p3.21.m21.1.1.3\">𝐸</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.21.m21.1c\">\\Delta E</annotation></semantics></math> peak, denoted collectively as the <span id=\"S2.p3.24.1\" class=\"ltx_text ltx_inline-quote ltx_outerquote\">“rare peaking”</span> background. These peaking backgrounds are due to <math id=\"S2.p3.22.m22.1\" class=\"ltx_Math\" alttext=\"K-\\pi\" display=\"inline\"><semantics id=\"S2.p3.22.m22.1a\"><mrow id=\"S2.p3.22.m22.1.1\" xref=\"S2.p3.22.m22.1.1.cmml\"><mi id=\"S2.p3.22.m22.1.1.2\" xref=\"S2.p3.22.m22.1.1.2.cmml\">K</mi><mo id=\"S2.p3.22.m22.1.1.1\" xref=\"S2.p3.22.m22.1.1.1.cmml\">−</mo><mi id=\"S2.p3.22.m22.1.1.3\" xref=\"S2.p3.22.m22.1.1.3.cmml\">π</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.22.m22.1b\"><apply id=\"S2.p3.22.m22.1.1.cmml\" xref=\"S2.p3.22.m22.1.1\"><minus id=\"S2.p3.22.m22.1.1.1.cmml\" xref=\"S2.p3.22.m22.1.1.1\"></minus><ci id=\"S2.p3.22.m22.1.1.2.cmml\" xref=\"S2.p3.22.m22.1.1.2\">𝐾</ci><ci id=\"S2.p3.22.m22.1.1.3.cmml\" xref=\"S2.p3.22.m22.1.1.3\">𝜋</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.22.m22.1c\">K-\\pi</annotation></semantics></math> misidentification, which consist of <math id=\"S2.p3.23.m23.1\" class=\"ltx_Math\" alttext=\"B^{+}\\rightarrow K^{+}K^{-}K^{+}\" display=\"inline\"><semantics id=\"S2.p3.23.m23.1a\"><mrow id=\"S2.p3.23.m23.1.1\" xref=\"S2.p3.23.m23.1.1.cmml\"><msup id=\"S2.p3.23.m23.1.1.2\" xref=\"S2.p3.23.m23.1.1.2.cmml\"><mi id=\"S2.p3.23.m23.1.1.2.2\" xref=\"S2.p3.23.m23.1.1.2.2.cmml\">B</mi><mo id=\"S2.p3.23.m23.1.1.2.3\" xref=\"S2.p3.23.m23.1.1.2.3.cmml\">+</mo></msup><mo stretchy=\"false\" id=\"S2.p3.23.m23.1.1.1\" xref=\"S2.p3.23.m23.1.1.1.cmml\">→</mo><mrow id=\"S2.p3.23.m23.1.1.3\" xref=\"S2.p3.23.m23.1.1.3.cmml\"><msup id=\"S2.p3.23.m23.1.1.3.2\" xref=\"S2.p3.23.m23.1.1.3.2.cmml\"><mi id=\"S2.p3.23.m23.1.1.3.2.2\" xref=\"S2.p3.23.m23.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.p3.23.m23.1.1.3.2.3\" xref=\"S2.p3.23.m23.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.23.m23.1.1.3.1\" xref=\"S2.p3.23.m23.1.1.3.1.cmml\">​</mo><msup id=\"S2.p3.23.m23.1.1.3.3\" xref=\"S2.p3.23.m23.1.1.3.3.cmml\"><mi id=\"S2.p3.23.m23.1.1.3.3.2\" xref=\"S2.p3.23.m23.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.p3.23.m23.1.1.3.3.3\" xref=\"S2.p3.23.m23.1.1.3.3.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.23.m23.1.1.3.1a\" xref=\"S2.p3.23.m23.1.1.3.1.cmml\">​</mo><msup id=\"S2.p3.23.m23.1.1.3.4\" xref=\"S2.p3.23.m23.1.1.3.4.cmml\"><mi id=\"S2.p3.23.m23.1.1.3.4.2\" xref=\"S2.p3.23.m23.1.1.3.4.2.cmml\">K</mi><mo id=\"S2.p3.23.m23.1.1.3.4.3\" xref=\"S2.p3.23.m23.1.1.3.4.3.cmml\">+</mo></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.23.m23.1b\"><apply id=\"S2.p3.23.m23.1.1.cmml\" xref=\"S2.p3.23.m23.1.1\"><ci id=\"S2.p3.23.m23.1.1.1.cmml\" xref=\"S2.p3.23.m23.1.1.1\">→</ci><apply id=\"S2.p3.23.m23.1.1.2.cmml\" xref=\"S2.p3.23.m23.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.23.m23.1.1.2.1.cmml\" xref=\"S2.p3.23.m23.1.1.2\">superscript</csymbol><ci id=\"S2.p3.23.m23.1.1.2.2.cmml\" xref=\"S2.p3.23.m23.1.1.2.2\">𝐵</ci><plus id=\"S2.p3.23.m23.1.1.2.3.cmml\" xref=\"S2.p3.23.m23.1.1.2.3\"></plus></apply><apply id=\"S2.p3.23.m23.1.1.3.cmml\" xref=\"S2.p3.23.m23.1.1.3\"><times id=\"S2.p3.23.m23.1.1.3.1.cmml\" xref=\"S2.p3.23.m23.1.1.3.1\"></times><apply id=\"S2.p3.23.m23.1.1.3.2.cmml\" xref=\"S2.p3.23.m23.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.23.m23.1.1.3.2.1.cmml\" xref=\"S2.p3.23.m23.1.1.3.2\">superscript</csymbol><ci id=\"S2.p3.23.m23.1.1.3.2.2.cmml\" xref=\"S2.p3.23.m23.1.1.3.2.2\">𝐾</ci><plus id=\"S2.p3.23.m23.1.1.3.2.3.cmml\" xref=\"S2.p3.23.m23.1.1.3.2.3\"></plus></apply><apply id=\"S2.p3.23.m23.1.1.3.3.cmml\" xref=\"S2.p3.23.m23.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p3.23.m23.1.1.3.3.1.cmml\" xref=\"S2.p3.23.m23.1.1.3.3\">superscript</csymbol><ci id=\"S2.p3.23.m23.1.1.3.3.2.cmml\" xref=\"S2.p3.23.m23.1.1.3.3.2\">𝐾</ci><minus id=\"S2.p3.23.m23.1.1.3.3.3.cmml\" xref=\"S2.p3.23.m23.1.1.3.3.3\"></minus></apply><apply id=\"S2.p3.23.m23.1.1.3.4.cmml\" xref=\"S2.p3.23.m23.1.1.3.4\"><csymbol cd=\"ambiguous\" id=\"S2.p3.23.m23.1.1.3.4.1.cmml\" xref=\"S2.p3.23.m23.1.1.3.4\">superscript</csymbol><ci id=\"S2.p3.23.m23.1.1.3.4.2.cmml\" xref=\"S2.p3.23.m23.1.1.3.4.2\">𝐾</ci><plus id=\"S2.p3.23.m23.1.1.3.4.3.cmml\" xref=\"S2.p3.23.m23.1.1.3.4.3\"></plus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.23.m23.1c\">B^{+}\\rightarrow K^{+}K^{-}K^{+}</annotation></semantics></math>, <math id=\"S2.p3.24.m24.1\" class=\"ltx_Math\" alttext=\"B^{+}\\rightarrow K^{+}\\pi^{-}\\pi^{+}\" display=\"inline\"><semantics id=\"S2.p3.24.m24.1a\"><mrow id=\"S2.p3.24.m24.1.1\" xref=\"S2.p3.24.m24.1.1.cmml\"><msup id=\"S2.p3.24.m24.1.1.2\" xref=\"S2.p3.24.m24.1.1.2.cmml\"><mi id=\"S2.p3.24.m24.1.1.2.2\" xref=\"S2.p3.24.m24.1.1.2.2.cmml\">B</mi><mo id=\"S2.p3.24.m24.1.1.2.3\" xref=\"S2.p3.24.m24.1.1.2.3.cmml\">+</mo></msup><mo stretchy=\"false\" id=\"S2.p3.24.m24.1.1.1\" xref=\"S2.p3.24.m24.1.1.1.cmml\">→</mo><mrow id=\"S2.p3.24.m24.1.1.3\" xref=\"S2.p3.24.m24.1.1.3.cmml\"><msup id=\"S2.p3.24.m24.1.1.3.2\" xref=\"S2.p3.24.m24.1.1.3.2.cmml\"><mi id=\"S2.p3.24.m24.1.1.3.2.2\" xref=\"S2.p3.24.m24.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.p3.24.m24.1.1.3.2.3\" xref=\"S2.p3.24.m24.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.24.m24.1.1.3.1\" xref=\"S2.p3.24.m24.1.1.3.1.cmml\">​</mo><msup id=\"S2.p3.24.m24.1.1.3.3\" xref=\"S2.p3.24.m24.1.1.3.3.cmml\"><mi id=\"S2.p3.24.m24.1.1.3.3.2\" xref=\"S2.p3.24.m24.1.1.3.3.2.cmml\">π</mi><mo id=\"S2.p3.24.m24.1.1.3.3.3\" xref=\"S2.p3.24.m24.1.1.3.3.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p3.24.m24.1.1.3.1a\" xref=\"S2.p3.24.m24.1.1.3.1.cmml\">​</mo><msup id=\"S2.p3.24.m24.1.1.3.4\" xref=\"S2.p3.24.m24.1.1.3.4.cmml\"><mi id=\"S2.p3.24.m24.1.1.3.4.2\" xref=\"S2.p3.24.m24.1.1.3.4.2.cmml\">π</mi><mo id=\"S2.p3.24.m24.1.1.3.4.3\" xref=\"S2.p3.24.m24.1.1.3.4.3.cmml\">+</mo></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p3.24.m24.1b\"><apply id=\"S2.p3.24.m24.1.1.cmml\" xref=\"S2.p3.24.m24.1.1\"><ci id=\"S2.p3.24.m24.1.1.1.cmml\" xref=\"S2.p3.24.m24.1.1.1\">→</ci><apply id=\"S2.p3.24.m24.1.1.2.cmml\" xref=\"S2.p3.24.m24.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.24.m24.1.1.2.1.cmml\" xref=\"S2.p3.24.m24.1.1.2\">superscript</csymbol><ci id=\"S2.p3.24.m24.1.1.2.2.cmml\" xref=\"S2.p3.24.m24.1.1.2.2\">𝐵</ci><plus id=\"S2.p3.24.m24.1.1.2.3.cmml\" xref=\"S2.p3.24.m24.1.1.2.3\"></plus></apply><apply id=\"S2.p3.24.m24.1.1.3.cmml\" xref=\"S2.p3.24.m24.1.1.3\"><times id=\"S2.p3.24.m24.1.1.3.1.cmml\" xref=\"S2.p3.24.m24.1.1.3.1\"></times><apply id=\"S2.p3.24.m24.1.1.3.2.cmml\" xref=\"S2.p3.24.m24.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p3.24.m24.1.1.3.2.1.cmml\" xref=\"S2.p3.24.m24.1.1.3.2\">superscript</csymbol><ci id=\"S2.p3.24.m24.1.1.3.2.2.cmml\" xref=\"S2.p3.24.m24.1.1.3.2.2\">𝐾</ci><plus id=\"S2.p3.24.m24.1.1.3.2.3.cmml\" xref=\"S2.p3.24.m24.1.1.3.2.3\"></plus></apply><apply id=\"S2.p3.24.m24.1.1.3.3.cmml\" xref=\"S2.p3.24.m24.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p3.24.m24.1.1.3.3.1.cmml\" xref=\"S2.p3.24.m24.1.1.3.3\">superscript</csymbol><ci id=\"S2.p3.24.m24.1.1.3.3.2.cmml\" xref=\"S2.p3.24.m24.1.1.3.3.2\">𝜋</ci><minus id=\"S2.p3.24.m24.1.1.3.3.3.cmml\" xref=\"S2.p3.24.m24.1.1.3.3.3\"></minus></apply><apply id=\"S2.p3.24.m24.1.1.3.4.cmml\" xref=\"S2.p3.24.m24.1.1.3.4\"><csymbol cd=\"ambiguous\" id=\"S2.p3.24.m24.1.1.3.4.1.cmml\" xref=\"S2.p3.24.m24.1.1.3.4\">superscript</csymbol><ci id=\"S2.p3.24.m24.1.1.3.4.2.cmml\" xref=\"S2.p3.24.m24.1.1.3.4.2\">𝜋</ci><plus id=\"S2.p3.24.m24.1.1.3.4.3.cmml\" xref=\"S2.p3.24.m24.1.1.3.4.3\"></plus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p3.24.m24.1c\">B^{+}\\rightarrow K^{+}\\pi^{-}\\pi^{+}</annotation></semantics></math>, and their intermediate resonant modes. Events that remain after removing the peaking components are called the <span id=\"S2.p3.24.2\" class=\"ltx_text ltx_inline-quote ltx_outerquote\">“rare combinatorial”</span> background.\n",
      "<br class=\"ltx_break\"></p>\n",
      "</div>\n",
      "<div id=\"S2.p4\" class=\"ltx_para\">\n",
      "<p id=\"S2.p4.2\" class=\"ltx_p\">The signal yield is extracted by performing a two-dimensional unbinned extended maximum likelihood fit in <math id=\"S2.p4.1.m1.1\" class=\"ltx_Math\" alttext=\"M_{bc}\" display=\"inline\"><semantics id=\"S2.p4.1.m1.1a\"><msub id=\"S2.p4.1.m1.1.1\" xref=\"S2.p4.1.m1.1.1.cmml\"><mi id=\"S2.p4.1.m1.1.1.2\" xref=\"S2.p4.1.m1.1.1.2.cmml\">M</mi><mrow id=\"S2.p4.1.m1.1.1.3\" xref=\"S2.p4.1.m1.1.1.3.cmml\"><mi id=\"S2.p4.1.m1.1.1.3.2\" xref=\"S2.p4.1.m1.1.1.3.2.cmml\">b</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p4.1.m1.1.1.3.1\" xref=\"S2.p4.1.m1.1.1.3.1.cmml\">​</mo><mi id=\"S2.p4.1.m1.1.1.3.3\" xref=\"S2.p4.1.m1.1.1.3.3.cmml\">c</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.1.m1.1b\"><apply id=\"S2.p4.1.m1.1.1.cmml\" xref=\"S2.p4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p4.1.m1.1.1.1.cmml\" xref=\"S2.p4.1.m1.1.1\">subscript</csymbol><ci id=\"S2.p4.1.m1.1.1.2.cmml\" xref=\"S2.p4.1.m1.1.1.2\">𝑀</ci><apply id=\"S2.p4.1.m1.1.1.3.cmml\" xref=\"S2.p4.1.m1.1.1.3\"><times id=\"S2.p4.1.m1.1.1.3.1.cmml\" xref=\"S2.p4.1.m1.1.1.3.1\"></times><ci id=\"S2.p4.1.m1.1.1.3.2.cmml\" xref=\"S2.p4.1.m1.1.1.3.2\">𝑏</ci><ci id=\"S2.p4.1.m1.1.1.3.3.cmml\" xref=\"S2.p4.1.m1.1.1.3.3\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.1.m1.1c\">M_{bc}</annotation></semantics></math> and <math id=\"S2.p4.2.m2.1\" class=\"ltx_Math\" alttext=\"\\Delta E\" display=\"inline\"><semantics id=\"S2.p4.2.m2.1a\"><mrow id=\"S2.p4.2.m2.1.1\" xref=\"S2.p4.2.m2.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.p4.2.m2.1.1.2\" xref=\"S2.p4.2.m2.1.1.2.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p4.2.m2.1.1.1\" xref=\"S2.p4.2.m2.1.1.1.cmml\">​</mo><mi id=\"S2.p4.2.m2.1.1.3\" xref=\"S2.p4.2.m2.1.1.3.cmml\">E</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.2.m2.1b\"><apply id=\"S2.p4.2.m2.1.1.cmml\" xref=\"S2.p4.2.m2.1.1\"><times id=\"S2.p4.2.m2.1.1.1.cmml\" xref=\"S2.p4.2.m2.1.1.1\"></times><ci id=\"S2.p4.2.m2.1.1.2.cmml\" xref=\"S2.p4.2.m2.1.1.2\">Δ</ci><ci id=\"S2.p4.2.m2.1.1.3.cmml\" xref=\"S2.p4.2.m2.1.1.3\">𝐸</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.2.m2.1c\">\\Delta E</annotation></semantics></math> with the likelihood defined as</p>\n",
      "<table id=\"S2.E1\" class=\"ltx_equation ltx_eqn_table\">\n",
      "\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S2.E1.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\mathcal{L}=\\dfrac{e^{-\\sum_{j}N_{j}}}{N!}\\prod_{i}\\Big{[}\\sum_{j}N_{j}\\mathcal{P}_{j}^{i}\\Big{]},\\ \\ \\mathrm{where}\\ \\ \\ \\mathcal{P}_{j}^{i}=\\dfrac{1}{2}(1-q_{i}.A_{CP})\\times\\mathcal{P}_{j}(M_{bc}^{i},\\Delta E^{i}),\" display=\"block\"><semantics id=\"S2.E1.m1.1a\"><mrow id=\"S2.E1.m1.1b\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.E1.m1.1.2\">ℒ</mi><mo id=\"S2.E1.m1.1.3\">=</mo><mfrac id=\"S2.E1.m1.1.4\"><msup id=\"S2.E1.m1.1.4.2\"><mi id=\"S2.E1.m1.1.4.2.2\">e</mi><mrow id=\"S2.E1.m1.1.4.2.3\"><mo id=\"S2.E1.m1.1.4.2.3a\">−</mo><mrow id=\"S2.E1.m1.1.4.2.3.2\"><mstyle displaystyle=\"false\" id=\"S2.E1.m1.1.4.2.3.2.1\"><msub id=\"S2.E1.m1.1.4.2.3.2.1a\"><mo id=\"S2.E1.m1.1.4.2.3.2.1.2\">∑</mo><mi id=\"S2.E1.m1.1.4.2.3.2.1.3\">j</mi></msub></mstyle><msub id=\"S2.E1.m1.1.4.2.3.2.2\"><mi id=\"S2.E1.m1.1.4.2.3.2.2.2\">N</mi><mi id=\"S2.E1.m1.1.4.2.3.2.2.3\">j</mi></msub></mrow></mrow></msup><mrow id=\"S2.E1.m1.1.4.3\"><mi id=\"S2.E1.m1.1.4.3.2\">N</mi><mo id=\"S2.E1.m1.1.4.3.1\">!</mo></mrow></mfrac><munder id=\"S2.E1.m1.1.5\"><mo movablelimits=\"false\" rspace=\"0em\" id=\"S2.E1.m1.1.5.2\">∏</mo><mi id=\"S2.E1.m1.1.5.3\">i</mi></munder><mrow id=\"S2.E1.m1.1.6\"><mo maxsize=\"160%\" minsize=\"160%\" id=\"S2.E1.m1.1.6.1\">[</mo><munder id=\"S2.E1.m1.1.6.2\"><mo lspace=\"0em\" movablelimits=\"false\" id=\"S2.E1.m1.1.6.2.2\">∑</mo><mi id=\"S2.E1.m1.1.6.2.3\">j</mi></munder><msub id=\"S2.E1.m1.1.6.3\"><mi id=\"S2.E1.m1.1.6.3.2\">N</mi><mi id=\"S2.E1.m1.1.6.3.3\">j</mi></msub><msubsup id=\"S2.E1.m1.1.6.4\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.E1.m1.1.6.4.2.2\">𝒫</mi><mi id=\"S2.E1.m1.1.6.4.2.3\">j</mi><mi id=\"S2.E1.m1.1.6.4.3\">i</mi></msubsup><mo maxsize=\"160%\" minsize=\"160%\" id=\"S2.E1.m1.1.6.5\">]</mo></mrow><mo rspace=\"1.167em\" id=\"S2.E1.m1.1.7\">,</mo><mi id=\"S2.E1.m1.1.1\">where</mi><mspace width=\"1.5em\" id=\"S2.E1.m1.1.8\"></mspace><msubsup id=\"S2.E1.m1.1.9\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.E1.m1.1.9.2.2\">𝒫</mi><mi id=\"S2.E1.m1.1.9.2.3\">j</mi><mi id=\"S2.E1.m1.1.9.3\">i</mi></msubsup><mo id=\"S2.E1.m1.1.10\">=</mo><mfrac id=\"S2.E1.m1.1.11\"><mn id=\"S2.E1.m1.1.11.2\">1</mn><mn id=\"S2.E1.m1.1.11.3\">2</mn></mfrac><mrow id=\"S2.E1.m1.1.12\"><mo stretchy=\"false\" id=\"S2.E1.m1.1.12.1\">(</mo><mn id=\"S2.E1.m1.1.12.2\">1</mn><mo id=\"S2.E1.m1.1.12.3\">−</mo><msub id=\"S2.E1.m1.1.12.4\"><mi id=\"S2.E1.m1.1.12.4.2\">q</mi><mi id=\"S2.E1.m1.1.12.4.3\">i</mi></msub><mo lspace=\"0em\" rspace=\"0.167em\" id=\"S2.E1.m1.1.12.5\">.</mo><msub id=\"S2.E1.m1.1.12.6\"><mi id=\"S2.E1.m1.1.12.6.2\">A</mi><mrow id=\"S2.E1.m1.1.12.6.3\"><mi id=\"S2.E1.m1.1.12.6.3.2\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.E1.m1.1.12.6.3.1\">​</mo><mi id=\"S2.E1.m1.1.12.6.3.3\">P</mi></mrow></msub><mo rspace=\"0.055em\" stretchy=\"false\" id=\"S2.E1.m1.1.12.7\">)</mo></mrow><mo rspace=\"0.222em\" id=\"S2.E1.m1.1.13\">×</mo><msub id=\"S2.E1.m1.1.14\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.E1.m1.1.14.2\">𝒫</mi><mi id=\"S2.E1.m1.1.14.3\">j</mi></msub><mrow id=\"S2.E1.m1.1.15\"><mo stretchy=\"false\" id=\"S2.E1.m1.1.15.1\">(</mo><msubsup id=\"S2.E1.m1.1.15.2\"><mi id=\"S2.E1.m1.1.15.2.2.2\">M</mi><mrow id=\"S2.E1.m1.1.15.2.2.3\"><mi id=\"S2.E1.m1.1.15.2.2.3.2\">b</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.E1.m1.1.15.2.2.3.1\">​</mo><mi id=\"S2.E1.m1.1.15.2.2.3.3\">c</mi></mrow><mi id=\"S2.E1.m1.1.15.2.3\">i</mi></msubsup><mo id=\"S2.E1.m1.1.15.3\">,</mo><mi mathvariant=\"normal\" id=\"S2.E1.m1.1.15.4\">Δ</mi><msup id=\"S2.E1.m1.1.15.5\"><mi id=\"S2.E1.m1.1.15.5.2\">E</mi><mi id=\"S2.E1.m1.1.15.5.3\">i</mi></msup><mo stretchy=\"false\" id=\"S2.E1.m1.1.15.6\">)</mo></mrow><mo id=\"S2.E1.m1.1.16\">,</mo></mrow><annotation encoding=\"application/x-tex\" id=\"S2.E1.m1.1c\">\\mathcal{L}=\\dfrac{e^{-\\sum_{j}N_{j}}}{N!}\\prod_{i}\\Big{[}\\sum_{j}N_{j}\\mathcal{P}_{j}^{i}\\Big{]},\\ \\ \\mathrm{where}\\ \\ \\ \\mathcal{P}_{j}^{i}=\\dfrac{1}{2}(1-q_{i}.A_{CP})\\times\\mathcal{P}_{j}(M_{bc}^{i},\\Delta E^{i}),</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p id=\"S2.p4.13\" class=\"ltx_p\">where <math id=\"S2.p4.3.m1.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S2.p4.3.m1.1a\"><mi id=\"S2.p4.3.m1.1.1\" xref=\"S2.p4.3.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.3.m1.1b\"><ci id=\"S2.p4.3.m1.1.1.cmml\" xref=\"S2.p4.3.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.3.m1.1c\">i</annotation></semantics></math> denotes the event index, <math id=\"S2.p4.4.m2.1\" class=\"ltx_Math\" alttext=\"N_{j}\" display=\"inline\"><semantics id=\"S2.p4.4.m2.1a\"><msub id=\"S2.p4.4.m2.1.1\" xref=\"S2.p4.4.m2.1.1.cmml\"><mi id=\"S2.p4.4.m2.1.1.2\" xref=\"S2.p4.4.m2.1.1.2.cmml\">N</mi><mi id=\"S2.p4.4.m2.1.1.3\" xref=\"S2.p4.4.m2.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.4.m2.1b\"><apply id=\"S2.p4.4.m2.1.1.cmml\" xref=\"S2.p4.4.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p4.4.m2.1.1.1.cmml\" xref=\"S2.p4.4.m2.1.1\">subscript</csymbol><ci id=\"S2.p4.4.m2.1.1.2.cmml\" xref=\"S2.p4.4.m2.1.1.2\">𝑁</ci><ci id=\"S2.p4.4.m2.1.1.3.cmml\" xref=\"S2.p4.4.m2.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.4.m2.1c\">N_{j}</annotation></semantics></math> is the yield for the component <math id=\"S2.p4.5.m3.1\" class=\"ltx_Math\" alttext=\"j\" display=\"inline\"><semantics id=\"S2.p4.5.m3.1a\"><mi id=\"S2.p4.5.m3.1.1\" xref=\"S2.p4.5.m3.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.5.m3.1b\"><ci id=\"S2.p4.5.m3.1.1.cmml\" xref=\"S2.p4.5.m3.1.1\">𝑗</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.5.m3.1c\">j</annotation></semantics></math>, <math id=\"S2.p4.6.m4.1\" class=\"ltx_Math\" alttext=\"q_{i}\" display=\"inline\"><semantics id=\"S2.p4.6.m4.1a\"><msub id=\"S2.p4.6.m4.1.1\" xref=\"S2.p4.6.m4.1.1.cmml\"><mi id=\"S2.p4.6.m4.1.1.2\" xref=\"S2.p4.6.m4.1.1.2.cmml\">q</mi><mi id=\"S2.p4.6.m4.1.1.3\" xref=\"S2.p4.6.m4.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.6.m4.1b\"><apply id=\"S2.p4.6.m4.1.1.cmml\" xref=\"S2.p4.6.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p4.6.m4.1.1.1.cmml\" xref=\"S2.p4.6.m4.1.1\">subscript</csymbol><ci id=\"S2.p4.6.m4.1.1.2.cmml\" xref=\"S2.p4.6.m4.1.1.2\">𝑞</ci><ci id=\"S2.p4.6.m4.1.1.3.cmml\" xref=\"S2.p4.6.m4.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.6.m4.1c\">q_{i}</annotation></semantics></math> is the charge of <math id=\"S2.p4.7.m5.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S2.p4.7.m5.1a\"><mi id=\"S2.p4.7.m5.1.1\" xref=\"S2.p4.7.m5.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.7.m5.1b\"><ci id=\"S2.p4.7.m5.1.1.cmml\" xref=\"S2.p4.7.m5.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.7.m5.1c\">B</annotation></semantics></math> candidates (<math id=\"S2.p4.8.m6.1\" class=\"ltx_Math\" alttext=\"q_{i}=\\pm 1\" display=\"inline\"><semantics id=\"S2.p4.8.m6.1a\"><mrow id=\"S2.p4.8.m6.1.1\" xref=\"S2.p4.8.m6.1.1.cmml\"><msub id=\"S2.p4.8.m6.1.1.2\" xref=\"S2.p4.8.m6.1.1.2.cmml\"><mi id=\"S2.p4.8.m6.1.1.2.2\" xref=\"S2.p4.8.m6.1.1.2.2.cmml\">q</mi><mi id=\"S2.p4.8.m6.1.1.2.3\" xref=\"S2.p4.8.m6.1.1.2.3.cmml\">i</mi></msub><mo id=\"S2.p4.8.m6.1.1.1\" xref=\"S2.p4.8.m6.1.1.1.cmml\">=</mo><mrow id=\"S2.p4.8.m6.1.1.3\" xref=\"S2.p4.8.m6.1.1.3.cmml\"><mo id=\"S2.p4.8.m6.1.1.3a\" xref=\"S2.p4.8.m6.1.1.3.cmml\">±</mo><mn id=\"S2.p4.8.m6.1.1.3.2\" xref=\"S2.p4.8.m6.1.1.3.2.cmml\">1</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.8.m6.1b\"><apply id=\"S2.p4.8.m6.1.1.cmml\" xref=\"S2.p4.8.m6.1.1\"><eq id=\"S2.p4.8.m6.1.1.1.cmml\" xref=\"S2.p4.8.m6.1.1.1\"></eq><apply id=\"S2.p4.8.m6.1.1.2.cmml\" xref=\"S2.p4.8.m6.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.p4.8.m6.1.1.2.1.cmml\" xref=\"S2.p4.8.m6.1.1.2\">subscript</csymbol><ci id=\"S2.p4.8.m6.1.1.2.2.cmml\" xref=\"S2.p4.8.m6.1.1.2.2\">𝑞</ci><ci id=\"S2.p4.8.m6.1.1.2.3.cmml\" xref=\"S2.p4.8.m6.1.1.2.3\">𝑖</ci></apply><apply id=\"S2.p4.8.m6.1.1.3.cmml\" xref=\"S2.p4.8.m6.1.1.3\"><csymbol cd=\"latexml\" id=\"S2.p4.8.m6.1.1.3.1.cmml\" xref=\"S2.p4.8.m6.1.1.3\">plus-or-minus</csymbol><cn type=\"integer\" id=\"S2.p4.8.m6.1.1.3.2.cmml\" xref=\"S2.p4.8.m6.1.1.3.2\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.8.m6.1c\">q_{i}=\\pm 1</annotation></semantics></math> for <math id=\"S2.p4.9.m7.1\" class=\"ltx_Math\" alttext=\"B^{\\pm}\" display=\"inline\"><semantics id=\"S2.p4.9.m7.1a\"><msup id=\"S2.p4.9.m7.1.1\" xref=\"S2.p4.9.m7.1.1.cmml\"><mi id=\"S2.p4.9.m7.1.1.2\" xref=\"S2.p4.9.m7.1.1.2.cmml\">B</mi><mo id=\"S2.p4.9.m7.1.1.3\" xref=\"S2.p4.9.m7.1.1.3.cmml\">±</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.9.m7.1b\"><apply id=\"S2.p4.9.m7.1.1.cmml\" xref=\"S2.p4.9.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p4.9.m7.1.1.1.cmml\" xref=\"S2.p4.9.m7.1.1\">superscript</csymbol><ci id=\"S2.p4.9.m7.1.1.2.cmml\" xref=\"S2.p4.9.m7.1.1.2\">𝐵</ci><csymbol cd=\"latexml\" id=\"S2.p4.9.m7.1.1.3.cmml\" xref=\"S2.p4.9.m7.1.1.3\">plus-or-minus</csymbol></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.9.m7.1c\">B^{\\pm}</annotation></semantics></math>), and <math id=\"S2.p4.10.m8.1\" class=\"ltx_Math\" alttext=\"\\mathcal{P}_{j}\" display=\"inline\"><semantics id=\"S2.p4.10.m8.1a\"><msub id=\"S2.p4.10.m8.1.1\" xref=\"S2.p4.10.m8.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.p4.10.m8.1.1.2\" xref=\"S2.p4.10.m8.1.1.2.cmml\">𝒫</mi><mi id=\"S2.p4.10.m8.1.1.3\" xref=\"S2.p4.10.m8.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.10.m8.1b\"><apply id=\"S2.p4.10.m8.1.1.cmml\" xref=\"S2.p4.10.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p4.10.m8.1.1.1.cmml\" xref=\"S2.p4.10.m8.1.1\">subscript</csymbol><ci id=\"S2.p4.10.m8.1.1.2.cmml\" xref=\"S2.p4.10.m8.1.1.2\">𝒫</ci><ci id=\"S2.p4.10.m8.1.1.3.cmml\" xref=\"S2.p4.10.m8.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.10.m8.1c\">\\mathcal{P}_{j}</annotation></semantics></math> is the probability density function (PDF) corresponding to the component <math id=\"S2.p4.11.m9.1\" class=\"ltx_Math\" alttext=\"j\" display=\"inline\"><semantics id=\"S2.p4.11.m9.1a\"><mi id=\"S2.p4.11.m9.1.1\" xref=\"S2.p4.11.m9.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.11.m9.1b\"><ci id=\"S2.p4.11.m9.1.1.cmml\" xref=\"S2.p4.11.m9.1.1\">𝑗</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.11.m9.1c\">j</annotation></semantics></math>. Figure 1 shows the fit results of first two <math id=\"S2.p4.12.m10.1\" class=\"ltx_Math\" alttext=\"M_{K^{+}K^{-}}\" display=\"inline\"><semantics id=\"S2.p4.12.m10.1a\"><msub id=\"S2.p4.12.m10.1.1\" xref=\"S2.p4.12.m10.1.1.cmml\"><mi id=\"S2.p4.12.m10.1.1.2\" xref=\"S2.p4.12.m10.1.1.2.cmml\">M</mi><mrow id=\"S2.p4.12.m10.1.1.3\" xref=\"S2.p4.12.m10.1.1.3.cmml\"><msup id=\"S2.p4.12.m10.1.1.3.2\" xref=\"S2.p4.12.m10.1.1.3.2.cmml\"><mi id=\"S2.p4.12.m10.1.1.3.2.2\" xref=\"S2.p4.12.m10.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.p4.12.m10.1.1.3.2.3\" xref=\"S2.p4.12.m10.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p4.12.m10.1.1.3.1\" xref=\"S2.p4.12.m10.1.1.3.1.cmml\">​</mo><msup id=\"S2.p4.12.m10.1.1.3.3\" xref=\"S2.p4.12.m10.1.1.3.3.cmml\"><mi id=\"S2.p4.12.m10.1.1.3.3.2\" xref=\"S2.p4.12.m10.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.p4.12.m10.1.1.3.3.3\" xref=\"S2.p4.12.m10.1.1.3.3.3.cmml\">−</mo></msup></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.12.m10.1b\"><apply id=\"S2.p4.12.m10.1.1.cmml\" xref=\"S2.p4.12.m10.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p4.12.m10.1.1.1.cmml\" xref=\"S2.p4.12.m10.1.1\">subscript</csymbol><ci id=\"S2.p4.12.m10.1.1.2.cmml\" xref=\"S2.p4.12.m10.1.1.2\">𝑀</ci><apply id=\"S2.p4.12.m10.1.1.3.cmml\" xref=\"S2.p4.12.m10.1.1.3\"><times id=\"S2.p4.12.m10.1.1.3.1.cmml\" xref=\"S2.p4.12.m10.1.1.3.1\"></times><apply id=\"S2.p4.12.m10.1.1.3.2.cmml\" xref=\"S2.p4.12.m10.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p4.12.m10.1.1.3.2.1.cmml\" xref=\"S2.p4.12.m10.1.1.3.2\">superscript</csymbol><ci id=\"S2.p4.12.m10.1.1.3.2.2.cmml\" xref=\"S2.p4.12.m10.1.1.3.2.2\">𝐾</ci><plus id=\"S2.p4.12.m10.1.1.3.2.3.cmml\" xref=\"S2.p4.12.m10.1.1.3.2.3\"></plus></apply><apply id=\"S2.p4.12.m10.1.1.3.3.cmml\" xref=\"S2.p4.12.m10.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p4.12.m10.1.1.3.3.1.cmml\" xref=\"S2.p4.12.m10.1.1.3.3\">superscript</csymbol><ci id=\"S2.p4.12.m10.1.1.3.3.2.cmml\" xref=\"S2.p4.12.m10.1.1.3.3.2\">𝐾</ci><minus id=\"S2.p4.12.m10.1.1.3.3.3.cmml\" xref=\"S2.p4.12.m10.1.1.3.3.3\"></minus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.12.m10.1c\">M_{K^{+}K^{-}}</annotation></semantics></math> bins in the signal-enhanced region. The resulting branching fraction and <math id=\"S2.p4.13.m11.1\" class=\"ltx_Math\" alttext=\"CP\" display=\"inline\"><semantics id=\"S2.p4.13.m11.1a\"><mrow id=\"S2.p4.13.m11.1.1\" xref=\"S2.p4.13.m11.1.1.cmml\"><mi id=\"S2.p4.13.m11.1.1.2\" xref=\"S2.p4.13.m11.1.1.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p4.13.m11.1.1.1\" xref=\"S2.p4.13.m11.1.1.1.cmml\">​</mo><mi id=\"S2.p4.13.m11.1.1.3\" xref=\"S2.p4.13.m11.1.1.3.cmml\">P</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p4.13.m11.1b\"><apply id=\"S2.p4.13.m11.1.1.cmml\" xref=\"S2.p4.13.m11.1.1\"><times id=\"S2.p4.13.m11.1.1.1.cmml\" xref=\"S2.p4.13.m11.1.1.1\"></times><ci id=\"S2.p4.13.m11.1.1.2.cmml\" xref=\"S2.p4.13.m11.1.1.2\">𝐶</ci><ci id=\"S2.p4.13.m11.1.1.3.cmml\" xref=\"S2.p4.13.m11.1.1.3\">𝑃</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p4.13.m11.1c\">CP</annotation></semantics></math> asymmetry are [10]</p>\n",
      "<table id=\"S2.E2\" class=\"ltx_equation ltx_eqn_table\">\n",
      "\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S2.E2.m1.2\" class=\"ltx_Math\" alttext=\"\\mathcal{B}(B^{+}\\rightarrow K^{+}K^{-}\\pi^{+})=(5.38\\pm 0.40\\pm 0.35)\\times 10^{-6}\" display=\"block\"><semantics id=\"S2.E2.m1.2a\"><mrow id=\"S2.E2.m1.2.2\" xref=\"S2.E2.m1.2.2.cmml\"><mrow id=\"S2.E2.m1.1.1.1\" xref=\"S2.E2.m1.1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.E2.m1.1.1.1.3\" xref=\"S2.E2.m1.1.1.1.3.cmml\">ℬ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.E2.m1.1.1.1.2\" xref=\"S2.E2.m1.1.1.1.2.cmml\">​</mo><mrow id=\"S2.E2.m1.1.1.1.1.1\" xref=\"S2.E2.m1.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S2.E2.m1.1.1.1.1.1.2\" xref=\"S2.E2.m1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S2.E2.m1.1.1.1.1.1.1\" xref=\"S2.E2.m1.1.1.1.1.1.1.cmml\"><msup id=\"S2.E2.m1.1.1.1.1.1.1.2\" xref=\"S2.E2.m1.1.1.1.1.1.1.2.cmml\"><mi id=\"S2.E2.m1.1.1.1.1.1.1.2.2\" xref=\"S2.E2.m1.1.1.1.1.1.1.2.2.cmml\">B</mi><mo id=\"S2.E2.m1.1.1.1.1.1.1.2.3\" xref=\"S2.E2.m1.1.1.1.1.1.1.2.3.cmml\">+</mo></msup><mo stretchy=\"false\" id=\"S2.E2.m1.1.1.1.1.1.1.1\" xref=\"S2.E2.m1.1.1.1.1.1.1.1.cmml\">→</mo><mrow id=\"S2.E2.m1.1.1.1.1.1.1.3\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.cmml\"><msup id=\"S2.E2.m1.1.1.1.1.1.1.3.2\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.2.cmml\"><mi id=\"S2.E2.m1.1.1.1.1.1.1.3.2.2\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.E2.m1.1.1.1.1.1.1.3.2.3\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.E2.m1.1.1.1.1.1.1.3.1\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.1.cmml\">​</mo><msup id=\"S2.E2.m1.1.1.1.1.1.1.3.3\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.3.cmml\"><mi id=\"S2.E2.m1.1.1.1.1.1.1.3.3.2\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.E2.m1.1.1.1.1.1.1.3.3.3\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.3.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.E2.m1.1.1.1.1.1.1.3.1a\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.1.cmml\">​</mo><msup id=\"S2.E2.m1.1.1.1.1.1.1.3.4\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.4.cmml\"><mi id=\"S2.E2.m1.1.1.1.1.1.1.3.4.2\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.4.2.cmml\">π</mi><mo id=\"S2.E2.m1.1.1.1.1.1.1.3.4.3\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.4.3.cmml\">+</mo></msup></mrow></mrow><mo stretchy=\"false\" id=\"S2.E2.m1.1.1.1.1.1.3\" xref=\"S2.E2.m1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S2.E2.m1.2.2.3\" xref=\"S2.E2.m1.2.2.3.cmml\">=</mo><mrow id=\"S2.E2.m1.2.2.2\" xref=\"S2.E2.m1.2.2.2.cmml\"><mrow id=\"S2.E2.m1.2.2.2.1.1\" xref=\"S2.E2.m1.2.2.2.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S2.E2.m1.2.2.2.1.1.2\" xref=\"S2.E2.m1.2.2.2.1.1.1.cmml\">(</mo><mrow id=\"S2.E2.m1.2.2.2.1.1.1\" xref=\"S2.E2.m1.2.2.2.1.1.1.cmml\"><mn id=\"S2.E2.m1.2.2.2.1.1.1.2\" xref=\"S2.E2.m1.2.2.2.1.1.1.2.cmml\">5.38</mn><mo id=\"S2.E2.m1.2.2.2.1.1.1.1\" xref=\"S2.E2.m1.2.2.2.1.1.1.1.cmml\">±</mo><mn id=\"S2.E2.m1.2.2.2.1.1.1.3\" xref=\"S2.E2.m1.2.2.2.1.1.1.3.cmml\">0.40</mn><mo id=\"S2.E2.m1.2.2.2.1.1.1.1a\" xref=\"S2.E2.m1.2.2.2.1.1.1.1.cmml\">±</mo><mn id=\"S2.E2.m1.2.2.2.1.1.1.4\" xref=\"S2.E2.m1.2.2.2.1.1.1.4.cmml\">0.35</mn></mrow><mo rspace=\"0.055em\" stretchy=\"false\" id=\"S2.E2.m1.2.2.2.1.1.3\" xref=\"S2.E2.m1.2.2.2.1.1.1.cmml\">)</mo></mrow><mo rspace=\"0.222em\" id=\"S2.E2.m1.2.2.2.2\" xref=\"S2.E2.m1.2.2.2.2.cmml\">×</mo><msup id=\"S2.E2.m1.2.2.2.3\" xref=\"S2.E2.m1.2.2.2.3.cmml\"><mn id=\"S2.E2.m1.2.2.2.3.2\" xref=\"S2.E2.m1.2.2.2.3.2.cmml\">10</mn><mrow id=\"S2.E2.m1.2.2.2.3.3\" xref=\"S2.E2.m1.2.2.2.3.3.cmml\"><mo id=\"S2.E2.m1.2.2.2.3.3a\" xref=\"S2.E2.m1.2.2.2.3.3.cmml\">−</mo><mn id=\"S2.E2.m1.2.2.2.3.3.2\" xref=\"S2.E2.m1.2.2.2.3.3.2.cmml\">6</mn></mrow></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.E2.m1.2b\"><apply id=\"S2.E2.m1.2.2.cmml\" xref=\"S2.E2.m1.2.2\"><eq id=\"S2.E2.m1.2.2.3.cmml\" xref=\"S2.E2.m1.2.2.3\"></eq><apply id=\"S2.E2.m1.1.1.1.cmml\" xref=\"S2.E2.m1.1.1.1\"><times id=\"S2.E2.m1.1.1.1.2.cmml\" xref=\"S2.E2.m1.1.1.1.2\"></times><ci id=\"S2.E2.m1.1.1.1.3.cmml\" xref=\"S2.E2.m1.1.1.1.3\">ℬ</ci><apply id=\"S2.E2.m1.1.1.1.1.1.1.cmml\" xref=\"S2.E2.m1.1.1.1.1.1\"><ci id=\"S2.E2.m1.1.1.1.1.1.1.1.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.1\">→</ci><apply id=\"S2.E2.m1.1.1.1.1.1.1.2.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m1.1.1.1.1.1.1.2.1.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.2\">superscript</csymbol><ci id=\"S2.E2.m1.1.1.1.1.1.1.2.2.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.2.2\">𝐵</ci><plus id=\"S2.E2.m1.1.1.1.1.1.1.2.3.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.2.3\"></plus></apply><apply id=\"S2.E2.m1.1.1.1.1.1.1.3.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3\"><times id=\"S2.E2.m1.1.1.1.1.1.1.3.1.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.1\"></times><apply id=\"S2.E2.m1.1.1.1.1.1.1.3.2.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m1.1.1.1.1.1.1.3.2.1.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.2\">superscript</csymbol><ci id=\"S2.E2.m1.1.1.1.1.1.1.3.2.2.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.2.2\">𝐾</ci><plus id=\"S2.E2.m1.1.1.1.1.1.1.3.2.3.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.2.3\"></plus></apply><apply id=\"S2.E2.m1.1.1.1.1.1.1.3.3.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m1.1.1.1.1.1.1.3.3.1.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.3\">superscript</csymbol><ci id=\"S2.E2.m1.1.1.1.1.1.1.3.3.2.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.3.2\">𝐾</ci><minus id=\"S2.E2.m1.1.1.1.1.1.1.3.3.3.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.3.3\"></minus></apply><apply id=\"S2.E2.m1.1.1.1.1.1.1.3.4.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.4\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m1.1.1.1.1.1.1.3.4.1.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.4\">superscript</csymbol><ci id=\"S2.E2.m1.1.1.1.1.1.1.3.4.2.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.4.2\">𝜋</ci><plus id=\"S2.E2.m1.1.1.1.1.1.1.3.4.3.cmml\" xref=\"S2.E2.m1.1.1.1.1.1.1.3.4.3\"></plus></apply></apply></apply></apply><apply id=\"S2.E2.m1.2.2.2.cmml\" xref=\"S2.E2.m1.2.2.2\"><times id=\"S2.E2.m1.2.2.2.2.cmml\" xref=\"S2.E2.m1.2.2.2.2\"></times><apply id=\"S2.E2.m1.2.2.2.1.1.1.cmml\" xref=\"S2.E2.m1.2.2.2.1.1\"><csymbol cd=\"latexml\" id=\"S2.E2.m1.2.2.2.1.1.1.1.cmml\" xref=\"S2.E2.m1.2.2.2.1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.E2.m1.2.2.2.1.1.1.2.cmml\" xref=\"S2.E2.m1.2.2.2.1.1.1.2\">5.38</cn><cn type=\"float\" id=\"S2.E2.m1.2.2.2.1.1.1.3.cmml\" xref=\"S2.E2.m1.2.2.2.1.1.1.3\">0.40</cn><cn type=\"float\" id=\"S2.E2.m1.2.2.2.1.1.1.4.cmml\" xref=\"S2.E2.m1.2.2.2.1.1.1.4\">0.35</cn></apply><apply id=\"S2.E2.m1.2.2.2.3.cmml\" xref=\"S2.E2.m1.2.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m1.2.2.2.3.1.cmml\" xref=\"S2.E2.m1.2.2.2.3\">superscript</csymbol><cn type=\"integer\" id=\"S2.E2.m1.2.2.2.3.2.cmml\" xref=\"S2.E2.m1.2.2.2.3.2\">10</cn><apply id=\"S2.E2.m1.2.2.2.3.3.cmml\" xref=\"S2.E2.m1.2.2.2.3.3\"><minus id=\"S2.E2.m1.2.2.2.3.3.1.cmml\" xref=\"S2.E2.m1.2.2.2.3.3\"></minus><cn type=\"integer\" id=\"S2.E2.m1.2.2.2.3.3.2.cmml\" xref=\"S2.E2.m1.2.2.2.3.3.2\">6</cn></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.E2.m1.2c\">\\mathcal{B}(B^{+}\\rightarrow K^{+}K^{-}\\pi^{+})=(5.38\\pm 0.40\\pm 0.35)\\times 10^{-6}</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p id=\"S2.p4.14\" class=\"ltx_p\">and</p>\n",
      "<table id=\"S2.E3\" class=\"ltx_equation ltx_eqn_table\">\n",
      "\n",
      "<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n",
      "<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S2.E3.m1.1\" class=\"ltx_Math\" alttext=\"A_{CP}=-0.182\\pm 0.071\\pm 0.016,\" display=\"block\"><semantics id=\"S2.E3.m1.1a\"><mrow id=\"S2.E3.m1.1.1.1\" xref=\"S2.E3.m1.1.1.1.1.cmml\"><mrow id=\"S2.E3.m1.1.1.1.1\" xref=\"S2.E3.m1.1.1.1.1.cmml\"><msub id=\"S2.E3.m1.1.1.1.1.2\" xref=\"S2.E3.m1.1.1.1.1.2.cmml\"><mi id=\"S2.E3.m1.1.1.1.1.2.2\" xref=\"S2.E3.m1.1.1.1.1.2.2.cmml\">A</mi><mrow id=\"S2.E3.m1.1.1.1.1.2.3\" xref=\"S2.E3.m1.1.1.1.1.2.3.cmml\"><mi id=\"S2.E3.m1.1.1.1.1.2.3.2\" xref=\"S2.E3.m1.1.1.1.1.2.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.E3.m1.1.1.1.1.2.3.1\" xref=\"S2.E3.m1.1.1.1.1.2.3.1.cmml\">​</mo><mi id=\"S2.E3.m1.1.1.1.1.2.3.3\" xref=\"S2.E3.m1.1.1.1.1.2.3.3.cmml\">P</mi></mrow></msub><mo id=\"S2.E3.m1.1.1.1.1.1\" xref=\"S2.E3.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S2.E3.m1.1.1.1.1.3\" xref=\"S2.E3.m1.1.1.1.1.3.cmml\"><mrow id=\"S2.E3.m1.1.1.1.1.3.2\" xref=\"S2.E3.m1.1.1.1.1.3.2.cmml\"><mo id=\"S2.E3.m1.1.1.1.1.3.2a\" xref=\"S2.E3.m1.1.1.1.1.3.2.cmml\">−</mo><mn id=\"S2.E3.m1.1.1.1.1.3.2.2\" xref=\"S2.E3.m1.1.1.1.1.3.2.2.cmml\">0.182</mn></mrow><mo id=\"S2.E3.m1.1.1.1.1.3.1\" xref=\"S2.E3.m1.1.1.1.1.3.1.cmml\">±</mo><mn id=\"S2.E3.m1.1.1.1.1.3.3\" xref=\"S2.E3.m1.1.1.1.1.3.3.cmml\">0.071</mn><mo id=\"S2.E3.m1.1.1.1.1.3.1a\" xref=\"S2.E3.m1.1.1.1.1.3.1.cmml\">±</mo><mn id=\"S2.E3.m1.1.1.1.1.3.4\" xref=\"S2.E3.m1.1.1.1.1.3.4.cmml\">0.016</mn></mrow></mrow><mo id=\"S2.E3.m1.1.1.1.2\" xref=\"S2.E3.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.E3.m1.1b\"><apply id=\"S2.E3.m1.1.1.1.1.cmml\" xref=\"S2.E3.m1.1.1.1\"><eq id=\"S2.E3.m1.1.1.1.1.1.cmml\" xref=\"S2.E3.m1.1.1.1.1.1\"></eq><apply id=\"S2.E3.m1.1.1.1.1.2.cmml\" xref=\"S2.E3.m1.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.E3.m1.1.1.1.1.2.1.cmml\" xref=\"S2.E3.m1.1.1.1.1.2\">subscript</csymbol><ci id=\"S2.E3.m1.1.1.1.1.2.2.cmml\" xref=\"S2.E3.m1.1.1.1.1.2.2\">𝐴</ci><apply id=\"S2.E3.m1.1.1.1.1.2.3.cmml\" xref=\"S2.E3.m1.1.1.1.1.2.3\"><times id=\"S2.E3.m1.1.1.1.1.2.3.1.cmml\" xref=\"S2.E3.m1.1.1.1.1.2.3.1\"></times><ci id=\"S2.E3.m1.1.1.1.1.2.3.2.cmml\" xref=\"S2.E3.m1.1.1.1.1.2.3.2\">𝐶</ci><ci id=\"S2.E3.m1.1.1.1.1.2.3.3.cmml\" xref=\"S2.E3.m1.1.1.1.1.2.3.3\">𝑃</ci></apply></apply><apply id=\"S2.E3.m1.1.1.1.1.3.cmml\" xref=\"S2.E3.m1.1.1.1.1.3\"><csymbol cd=\"latexml\" id=\"S2.E3.m1.1.1.1.1.3.1.cmml\" xref=\"S2.E3.m1.1.1.1.1.3.1\">plus-or-minus</csymbol><apply id=\"S2.E3.m1.1.1.1.1.3.2.cmml\" xref=\"S2.E3.m1.1.1.1.1.3.2\"><minus id=\"S2.E3.m1.1.1.1.1.3.2.1.cmml\" xref=\"S2.E3.m1.1.1.1.1.3.2\"></minus><cn type=\"float\" id=\"S2.E3.m1.1.1.1.1.3.2.2.cmml\" xref=\"S2.E3.m1.1.1.1.1.3.2.2\">0.182</cn></apply><cn type=\"float\" id=\"S2.E3.m1.1.1.1.1.3.3.cmml\" xref=\"S2.E3.m1.1.1.1.1.3.3\">0.071</cn><cn type=\"float\" id=\"S2.E3.m1.1.1.1.1.3.4.cmml\" xref=\"S2.E3.m1.1.1.1.1.3.4\">0.016</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.E3.m1.1c\">A_{CP}=-0.182\\pm 0.071\\pm 0.016,</annotation></semantics></math></td>\n",
      "<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n",
      "<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "<p id=\"S2.p4.15\" class=\"ltx_p\">where the quoted uncertainties are statistical and systematic, respectively.</p>\n",
      "</div>\n",
      "<div id=\"S2.p5\" class=\"ltx_para\">\n",
      "<p id=\"S2.p5.10\" class=\"ltx_p\">To investigate the localized <math id=\"S2.p5.1.m1.1\" class=\"ltx_Math\" alttext=\"CP\" display=\"inline\"><semantics id=\"S2.p5.1.m1.1a\"><mrow id=\"S2.p5.1.m1.1.1\" xref=\"S2.p5.1.m1.1.1.cmml\"><mi id=\"S2.p5.1.m1.1.1.2\" xref=\"S2.p5.1.m1.1.1.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.1.m1.1.1.1\" xref=\"S2.p5.1.m1.1.1.1.cmml\">​</mo><mi id=\"S2.p5.1.m1.1.1.3\" xref=\"S2.p5.1.m1.1.1.3.cmml\">P</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.1.m1.1b\"><apply id=\"S2.p5.1.m1.1.1.cmml\" xref=\"S2.p5.1.m1.1.1\"><times id=\"S2.p5.1.m1.1.1.1.cmml\" xref=\"S2.p5.1.m1.1.1.1\"></times><ci id=\"S2.p5.1.m1.1.1.2.cmml\" xref=\"S2.p5.1.m1.1.1.2\">𝐶</ci><ci id=\"S2.p5.1.m1.1.1.3.cmml\" xref=\"S2.p5.1.m1.1.1.3\">𝑃</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.1.m1.1c\">CP</annotation></semantics></math> asymmetry in the low <math id=\"S2.p5.2.m2.1\" class=\"ltx_Math\" alttext=\"M_{K^{+}K^{-}}\" display=\"inline\"><semantics id=\"S2.p5.2.m2.1a\"><msub id=\"S2.p5.2.m2.1.1\" xref=\"S2.p5.2.m2.1.1.cmml\"><mi id=\"S2.p5.2.m2.1.1.2\" xref=\"S2.p5.2.m2.1.1.2.cmml\">M</mi><mrow id=\"S2.p5.2.m2.1.1.3\" xref=\"S2.p5.2.m2.1.1.3.cmml\"><msup id=\"S2.p5.2.m2.1.1.3.2\" xref=\"S2.p5.2.m2.1.1.3.2.cmml\"><mi id=\"S2.p5.2.m2.1.1.3.2.2\" xref=\"S2.p5.2.m2.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.p5.2.m2.1.1.3.2.3\" xref=\"S2.p5.2.m2.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.2.m2.1.1.3.1\" xref=\"S2.p5.2.m2.1.1.3.1.cmml\">​</mo><msup id=\"S2.p5.2.m2.1.1.3.3\" xref=\"S2.p5.2.m2.1.1.3.3.cmml\"><mi id=\"S2.p5.2.m2.1.1.3.3.2\" xref=\"S2.p5.2.m2.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.p5.2.m2.1.1.3.3.3\" xref=\"S2.p5.2.m2.1.1.3.3.3.cmml\">−</mo></msup></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.2.m2.1b\"><apply id=\"S2.p5.2.m2.1.1.cmml\" xref=\"S2.p5.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p5.2.m2.1.1.1.cmml\" xref=\"S2.p5.2.m2.1.1\">subscript</csymbol><ci id=\"S2.p5.2.m2.1.1.2.cmml\" xref=\"S2.p5.2.m2.1.1.2\">𝑀</ci><apply id=\"S2.p5.2.m2.1.1.3.cmml\" xref=\"S2.p5.2.m2.1.1.3\"><times id=\"S2.p5.2.m2.1.1.3.1.cmml\" xref=\"S2.p5.2.m2.1.1.3.1\"></times><apply id=\"S2.p5.2.m2.1.1.3.2.cmml\" xref=\"S2.p5.2.m2.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p5.2.m2.1.1.3.2.1.cmml\" xref=\"S2.p5.2.m2.1.1.3.2\">superscript</csymbol><ci id=\"S2.p5.2.m2.1.1.3.2.2.cmml\" xref=\"S2.p5.2.m2.1.1.3.2.2\">𝐾</ci><plus id=\"S2.p5.2.m2.1.1.3.2.3.cmml\" xref=\"S2.p5.2.m2.1.1.3.2.3\"></plus></apply><apply id=\"S2.p5.2.m2.1.1.3.3.cmml\" xref=\"S2.p5.2.m2.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p5.2.m2.1.1.3.3.1.cmml\" xref=\"S2.p5.2.m2.1.1.3.3\">superscript</csymbol><ci id=\"S2.p5.2.m2.1.1.3.3.2.cmml\" xref=\"S2.p5.2.m2.1.1.3.3.2\">𝐾</ci><minus id=\"S2.p5.2.m2.1.1.3.3.3.cmml\" xref=\"S2.p5.2.m2.1.1.3.3.3\"></minus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.2.m2.1c\">M_{K^{+}K^{-}}</annotation></semantics></math> region, we determine the signal yield and <math id=\"S2.p5.3.m3.1\" class=\"ltx_Math\" alttext=\"A_{CP}\" display=\"inline\"><semantics id=\"S2.p5.3.m3.1a\"><msub id=\"S2.p5.3.m3.1.1\" xref=\"S2.p5.3.m3.1.1.cmml\"><mi id=\"S2.p5.3.m3.1.1.2\" xref=\"S2.p5.3.m3.1.1.2.cmml\">A</mi><mrow id=\"S2.p5.3.m3.1.1.3\" xref=\"S2.p5.3.m3.1.1.3.cmml\"><mi id=\"S2.p5.3.m3.1.1.3.2\" xref=\"S2.p5.3.m3.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.3.m3.1.1.3.1\" xref=\"S2.p5.3.m3.1.1.3.1.cmml\">​</mo><mi id=\"S2.p5.3.m3.1.1.3.3\" xref=\"S2.p5.3.m3.1.1.3.3.cmml\">P</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.3.m3.1b\"><apply id=\"S2.p5.3.m3.1.1.cmml\" xref=\"S2.p5.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p5.3.m3.1.1.1.cmml\" xref=\"S2.p5.3.m3.1.1\">subscript</csymbol><ci id=\"S2.p5.3.m3.1.1.2.cmml\" xref=\"S2.p5.3.m3.1.1.2\">𝐴</ci><apply id=\"S2.p5.3.m3.1.1.3.cmml\" xref=\"S2.p5.3.m3.1.1.3\"><times id=\"S2.p5.3.m3.1.1.3.1.cmml\" xref=\"S2.p5.3.m3.1.1.3.1\"></times><ci id=\"S2.p5.3.m3.1.1.3.2.cmml\" xref=\"S2.p5.3.m3.1.1.3.2\">𝐶</ci><ci id=\"S2.p5.3.m3.1.1.3.3.cmml\" xref=\"S2.p5.3.m3.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.3.m3.1c\">A_{CP}</annotation></semantics></math> in bins of <math id=\"S2.p5.4.m4.1\" class=\"ltx_Math\" alttext=\"M_{K^{+}K^{-}}\" display=\"inline\"><semantics id=\"S2.p5.4.m4.1a\"><msub id=\"S2.p5.4.m4.1.1\" xref=\"S2.p5.4.m4.1.1.cmml\"><mi id=\"S2.p5.4.m4.1.1.2\" xref=\"S2.p5.4.m4.1.1.2.cmml\">M</mi><mrow id=\"S2.p5.4.m4.1.1.3\" xref=\"S2.p5.4.m4.1.1.3.cmml\"><msup id=\"S2.p5.4.m4.1.1.3.2\" xref=\"S2.p5.4.m4.1.1.3.2.cmml\"><mi id=\"S2.p5.4.m4.1.1.3.2.2\" xref=\"S2.p5.4.m4.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.p5.4.m4.1.1.3.2.3\" xref=\"S2.p5.4.m4.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.4.m4.1.1.3.1\" xref=\"S2.p5.4.m4.1.1.3.1.cmml\">​</mo><msup id=\"S2.p5.4.m4.1.1.3.3\" xref=\"S2.p5.4.m4.1.1.3.3.cmml\"><mi id=\"S2.p5.4.m4.1.1.3.3.2\" xref=\"S2.p5.4.m4.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.p5.4.m4.1.1.3.3.3\" xref=\"S2.p5.4.m4.1.1.3.3.3.cmml\">−</mo></msup></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.4.m4.1b\"><apply id=\"S2.p5.4.m4.1.1.cmml\" xref=\"S2.p5.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p5.4.m4.1.1.1.cmml\" xref=\"S2.p5.4.m4.1.1\">subscript</csymbol><ci id=\"S2.p5.4.m4.1.1.2.cmml\" xref=\"S2.p5.4.m4.1.1.2\">𝑀</ci><apply id=\"S2.p5.4.m4.1.1.3.cmml\" xref=\"S2.p5.4.m4.1.1.3\"><times id=\"S2.p5.4.m4.1.1.3.1.cmml\" xref=\"S2.p5.4.m4.1.1.3.1\"></times><apply id=\"S2.p5.4.m4.1.1.3.2.cmml\" xref=\"S2.p5.4.m4.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p5.4.m4.1.1.3.2.1.cmml\" xref=\"S2.p5.4.m4.1.1.3.2\">superscript</csymbol><ci id=\"S2.p5.4.m4.1.1.3.2.2.cmml\" xref=\"S2.p5.4.m4.1.1.3.2.2\">𝐾</ci><plus id=\"S2.p5.4.m4.1.1.3.2.3.cmml\" xref=\"S2.p5.4.m4.1.1.3.2.3\"></plus></apply><apply id=\"S2.p5.4.m4.1.1.3.3.cmml\" xref=\"S2.p5.4.m4.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p5.4.m4.1.1.3.3.1.cmml\" xref=\"S2.p5.4.m4.1.1.3.3\">superscript</csymbol><ci id=\"S2.p5.4.m4.1.1.3.3.2.cmml\" xref=\"S2.p5.4.m4.1.1.3.3.2\">𝐾</ci><minus id=\"S2.p5.4.m4.1.1.3.3.3.cmml\" xref=\"S2.p5.4.m4.1.1.3.3.3\"></minus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.4.m4.1c\">M_{K^{+}K^{-}}</annotation></semantics></math>. The fitted results are shown in Table 1 and Fig. 2, where an excess of signal yield as well as a large <math id=\"S2.p5.5.m5.1\" class=\"ltx_Math\" alttext=\"A_{CP}\" display=\"inline\"><semantics id=\"S2.p5.5.m5.1a\"><msub id=\"S2.p5.5.m5.1.1\" xref=\"S2.p5.5.m5.1.1.cmml\"><mi id=\"S2.p5.5.m5.1.1.2\" xref=\"S2.p5.5.m5.1.1.2.cmml\">A</mi><mrow id=\"S2.p5.5.m5.1.1.3\" xref=\"S2.p5.5.m5.1.1.3.cmml\"><mi id=\"S2.p5.5.m5.1.1.3.2\" xref=\"S2.p5.5.m5.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.5.m5.1.1.3.1\" xref=\"S2.p5.5.m5.1.1.3.1.cmml\">​</mo><mi id=\"S2.p5.5.m5.1.1.3.3\" xref=\"S2.p5.5.m5.1.1.3.3.cmml\">P</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.5.m5.1b\"><apply id=\"S2.p5.5.m5.1.1.cmml\" xref=\"S2.p5.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.p5.5.m5.1.1.1.cmml\" xref=\"S2.p5.5.m5.1.1\">subscript</csymbol><ci id=\"S2.p5.5.m5.1.1.2.cmml\" xref=\"S2.p5.5.m5.1.1.2\">𝐴</ci><apply id=\"S2.p5.5.m5.1.1.3.cmml\" xref=\"S2.p5.5.m5.1.1.3\"><times id=\"S2.p5.5.m5.1.1.3.1.cmml\" xref=\"S2.p5.5.m5.1.1.3.1\"></times><ci id=\"S2.p5.5.m5.1.1.3.2.cmml\" xref=\"S2.p5.5.m5.1.1.3.2\">𝐶</ci><ci id=\"S2.p5.5.m5.1.1.3.3.cmml\" xref=\"S2.p5.5.m5.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.5.m5.1c\">A_{CP}</annotation></semantics></math> are seen in <math id=\"S2.p5.6.m6.1\" class=\"ltx_Math\" alttext=\"M_{K^{+}K^{-}}&lt;1.5~{}\\mathrm{GeV/}c^{2}\" display=\"inline\"><semantics id=\"S2.p5.6.m6.1a\"><mrow id=\"S2.p5.6.m6.1.1\" xref=\"S2.p5.6.m6.1.1.cmml\"><msub id=\"S2.p5.6.m6.1.1.2\" xref=\"S2.p5.6.m6.1.1.2.cmml\"><mi id=\"S2.p5.6.m6.1.1.2.2\" xref=\"S2.p5.6.m6.1.1.2.2.cmml\">M</mi><mrow id=\"S2.p5.6.m6.1.1.2.3\" xref=\"S2.p5.6.m6.1.1.2.3.cmml\"><msup id=\"S2.p5.6.m6.1.1.2.3.2\" xref=\"S2.p5.6.m6.1.1.2.3.2.cmml\"><mi id=\"S2.p5.6.m6.1.1.2.3.2.2\" xref=\"S2.p5.6.m6.1.1.2.3.2.2.cmml\">K</mi><mo id=\"S2.p5.6.m6.1.1.2.3.2.3\" xref=\"S2.p5.6.m6.1.1.2.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.6.m6.1.1.2.3.1\" xref=\"S2.p5.6.m6.1.1.2.3.1.cmml\">​</mo><msup id=\"S2.p5.6.m6.1.1.2.3.3\" xref=\"S2.p5.6.m6.1.1.2.3.3.cmml\"><mi id=\"S2.p5.6.m6.1.1.2.3.3.2\" xref=\"S2.p5.6.m6.1.1.2.3.3.2.cmml\">K</mi><mo id=\"S2.p5.6.m6.1.1.2.3.3.3\" xref=\"S2.p5.6.m6.1.1.2.3.3.3.cmml\">−</mo></msup></mrow></msub><mo id=\"S2.p5.6.m6.1.1.1\" xref=\"S2.p5.6.m6.1.1.1.cmml\">&lt;</mo><mrow id=\"S2.p5.6.m6.1.1.3\" xref=\"S2.p5.6.m6.1.1.3.cmml\"><mrow id=\"S2.p5.6.m6.1.1.3.2\" xref=\"S2.p5.6.m6.1.1.3.2.cmml\"><mn id=\"S2.p5.6.m6.1.1.3.2.2\" xref=\"S2.p5.6.m6.1.1.3.2.2.cmml\">1.5</mn><mo lspace=\"0.330em\" rspace=\"0em\" id=\"S2.p5.6.m6.1.1.3.2.1\" xref=\"S2.p5.6.m6.1.1.3.2.1.cmml\">​</mo><mi id=\"S2.p5.6.m6.1.1.3.2.3\" xref=\"S2.p5.6.m6.1.1.3.2.3.cmml\">GeV</mi></mrow><mo id=\"S2.p5.6.m6.1.1.3.1\" xref=\"S2.p5.6.m6.1.1.3.1.cmml\">/</mo><msup id=\"S2.p5.6.m6.1.1.3.3\" xref=\"S2.p5.6.m6.1.1.3.3.cmml\"><mi id=\"S2.p5.6.m6.1.1.3.3.2\" xref=\"S2.p5.6.m6.1.1.3.3.2.cmml\">c</mi><mn id=\"S2.p5.6.m6.1.1.3.3.3\" xref=\"S2.p5.6.m6.1.1.3.3.3.cmml\">2</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.6.m6.1b\"><apply id=\"S2.p5.6.m6.1.1.cmml\" xref=\"S2.p5.6.m6.1.1\"><lt id=\"S2.p5.6.m6.1.1.1.cmml\" xref=\"S2.p5.6.m6.1.1.1\"></lt><apply id=\"S2.p5.6.m6.1.1.2.cmml\" xref=\"S2.p5.6.m6.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.p5.6.m6.1.1.2.1.cmml\" xref=\"S2.p5.6.m6.1.1.2\">subscript</csymbol><ci id=\"S2.p5.6.m6.1.1.2.2.cmml\" xref=\"S2.p5.6.m6.1.1.2.2\">𝑀</ci><apply id=\"S2.p5.6.m6.1.1.2.3.cmml\" xref=\"S2.p5.6.m6.1.1.2.3\"><times id=\"S2.p5.6.m6.1.1.2.3.1.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.1\"></times><apply id=\"S2.p5.6.m6.1.1.2.3.2.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p5.6.m6.1.1.2.3.2.1.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.2\">superscript</csymbol><ci id=\"S2.p5.6.m6.1.1.2.3.2.2.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.2.2\">𝐾</ci><plus id=\"S2.p5.6.m6.1.1.2.3.2.3.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.2.3\"></plus></apply><apply id=\"S2.p5.6.m6.1.1.2.3.3.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p5.6.m6.1.1.2.3.3.1.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.3\">superscript</csymbol><ci id=\"S2.p5.6.m6.1.1.2.3.3.2.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.3.2\">𝐾</ci><minus id=\"S2.p5.6.m6.1.1.2.3.3.3.cmml\" xref=\"S2.p5.6.m6.1.1.2.3.3.3\"></minus></apply></apply></apply><apply id=\"S2.p5.6.m6.1.1.3.cmml\" xref=\"S2.p5.6.m6.1.1.3\"><divide id=\"S2.p5.6.m6.1.1.3.1.cmml\" xref=\"S2.p5.6.m6.1.1.3.1\"></divide><apply id=\"S2.p5.6.m6.1.1.3.2.cmml\" xref=\"S2.p5.6.m6.1.1.3.2\"><times id=\"S2.p5.6.m6.1.1.3.2.1.cmml\" xref=\"S2.p5.6.m6.1.1.3.2.1\"></times><cn type=\"float\" id=\"S2.p5.6.m6.1.1.3.2.2.cmml\" xref=\"S2.p5.6.m6.1.1.3.2.2\">1.5</cn><ci id=\"S2.p5.6.m6.1.1.3.2.3.cmml\" xref=\"S2.p5.6.m6.1.1.3.2.3\">GeV</ci></apply><apply id=\"S2.p5.6.m6.1.1.3.3.cmml\" xref=\"S2.p5.6.m6.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p5.6.m6.1.1.3.3.1.cmml\" xref=\"S2.p5.6.m6.1.1.3.3\">superscript</csymbol><ci id=\"S2.p5.6.m6.1.1.3.3.2.cmml\" xref=\"S2.p5.6.m6.1.1.3.3.2\">𝑐</ci><cn type=\"integer\" id=\"S2.p5.6.m6.1.1.3.3.3.cmml\" xref=\"S2.p5.6.m6.1.1.3.3.3\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.6.m6.1c\">M_{K^{+}K^{-}}&lt;1.5~{}\\mathrm{GeV/}c^{2}</annotation></semantics></math>, confirming the observations by BaBar and LHCb. We find strong evidence for a large <math id=\"S2.p5.7.m7.1\" class=\"ltx_Math\" alttext=\"CP\" display=\"inline\"><semantics id=\"S2.p5.7.m7.1a\"><mrow id=\"S2.p5.7.m7.1.1\" xref=\"S2.p5.7.m7.1.1.cmml\"><mi id=\"S2.p5.7.m7.1.1.2\" xref=\"S2.p5.7.m7.1.1.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.7.m7.1.1.1\" xref=\"S2.p5.7.m7.1.1.1.cmml\">​</mo><mi id=\"S2.p5.7.m7.1.1.3\" xref=\"S2.p5.7.m7.1.1.3.cmml\">P</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.7.m7.1b\"><apply id=\"S2.p5.7.m7.1.1.cmml\" xref=\"S2.p5.7.m7.1.1\"><times id=\"S2.p5.7.m7.1.1.1.cmml\" xref=\"S2.p5.7.m7.1.1.1\"></times><ci id=\"S2.p5.7.m7.1.1.2.cmml\" xref=\"S2.p5.7.m7.1.1.2\">𝐶</ci><ci id=\"S2.p5.7.m7.1.1.3.cmml\" xref=\"S2.p5.7.m7.1.1.3\">𝑃</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.7.m7.1c\">CP</annotation></semantics></math> asymmetry of <math id=\"S2.p5.8.m8.1\" class=\"ltx_Math\" alttext=\"-0.90\\pm 0.17\\pm 0.03\" display=\"inline\"><semantics id=\"S2.p5.8.m8.1a\"><mrow id=\"S2.p5.8.m8.1.1\" xref=\"S2.p5.8.m8.1.1.cmml\"><mrow id=\"S2.p5.8.m8.1.1.2\" xref=\"S2.p5.8.m8.1.1.2.cmml\"><mo id=\"S2.p5.8.m8.1.1.2a\" xref=\"S2.p5.8.m8.1.1.2.cmml\">−</mo><mn id=\"S2.p5.8.m8.1.1.2.2\" xref=\"S2.p5.8.m8.1.1.2.2.cmml\">0.90</mn></mrow><mo id=\"S2.p5.8.m8.1.1.1\" xref=\"S2.p5.8.m8.1.1.1.cmml\">±</mo><mn id=\"S2.p5.8.m8.1.1.3\" xref=\"S2.p5.8.m8.1.1.3.cmml\">0.17</mn><mo id=\"S2.p5.8.m8.1.1.1a\" xref=\"S2.p5.8.m8.1.1.1.cmml\">±</mo><mn id=\"S2.p5.8.m8.1.1.4\" xref=\"S2.p5.8.m8.1.1.4.cmml\">0.03</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.8.m8.1b\"><apply id=\"S2.p5.8.m8.1.1.cmml\" xref=\"S2.p5.8.m8.1.1\"><csymbol cd=\"latexml\" id=\"S2.p5.8.m8.1.1.1.cmml\" xref=\"S2.p5.8.m8.1.1.1\">plus-or-minus</csymbol><apply id=\"S2.p5.8.m8.1.1.2.cmml\" xref=\"S2.p5.8.m8.1.1.2\"><minus id=\"S2.p5.8.m8.1.1.2.1.cmml\" xref=\"S2.p5.8.m8.1.1.2\"></minus><cn type=\"float\" id=\"S2.p5.8.m8.1.1.2.2.cmml\" xref=\"S2.p5.8.m8.1.1.2.2\">0.90</cn></apply><cn type=\"float\" id=\"S2.p5.8.m8.1.1.3.cmml\" xref=\"S2.p5.8.m8.1.1.3\">0.17</cn><cn type=\"float\" id=\"S2.p5.8.m8.1.1.4.cmml\" xref=\"S2.p5.8.m8.1.1.4\">0.03</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.8.m8.1c\">-0.90\\pm 0.17\\pm 0.03</annotation></semantics></math> with a significance of <math id=\"S2.p5.9.m9.1\" class=\"ltx_Math\" alttext=\"4.8\\sigma\" display=\"inline\"><semantics id=\"S2.p5.9.m9.1a\"><mrow id=\"S2.p5.9.m9.1.1\" xref=\"S2.p5.9.m9.1.1.cmml\"><mn id=\"S2.p5.9.m9.1.1.2\" xref=\"S2.p5.9.m9.1.1.2.cmml\">4.8</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.9.m9.1.1.1\" xref=\"S2.p5.9.m9.1.1.1.cmml\">​</mo><mi id=\"S2.p5.9.m9.1.1.3\" xref=\"S2.p5.9.m9.1.1.3.cmml\">σ</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.9.m9.1b\"><apply id=\"S2.p5.9.m9.1.1.cmml\" xref=\"S2.p5.9.m9.1.1\"><times id=\"S2.p5.9.m9.1.1.1.cmml\" xref=\"S2.p5.9.m9.1.1.1\"></times><cn type=\"float\" id=\"S2.p5.9.m9.1.1.2.cmml\" xref=\"S2.p5.9.m9.1.1.2\">4.8</cn><ci id=\"S2.p5.9.m9.1.1.3.cmml\" xref=\"S2.p5.9.m9.1.1.3\">𝜎</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.9.m9.1c\">4.8\\sigma</annotation></semantics></math> for <math id=\"S2.p5.10.m10.1\" class=\"ltx_Math\" alttext=\"M_{K^{+}K^{-}}&lt;1.1~{}\\mathrm{GeV/}c^{2}\" display=\"inline\"><semantics id=\"S2.p5.10.m10.1a\"><mrow id=\"S2.p5.10.m10.1.1\" xref=\"S2.p5.10.m10.1.1.cmml\"><msub id=\"S2.p5.10.m10.1.1.2\" xref=\"S2.p5.10.m10.1.1.2.cmml\"><mi id=\"S2.p5.10.m10.1.1.2.2\" xref=\"S2.p5.10.m10.1.1.2.2.cmml\">M</mi><mrow id=\"S2.p5.10.m10.1.1.2.3\" xref=\"S2.p5.10.m10.1.1.2.3.cmml\"><msup id=\"S2.p5.10.m10.1.1.2.3.2\" xref=\"S2.p5.10.m10.1.1.2.3.2.cmml\"><mi id=\"S2.p5.10.m10.1.1.2.3.2.2\" xref=\"S2.p5.10.m10.1.1.2.3.2.2.cmml\">K</mi><mo id=\"S2.p5.10.m10.1.1.2.3.2.3\" xref=\"S2.p5.10.m10.1.1.2.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.p5.10.m10.1.1.2.3.1\" xref=\"S2.p5.10.m10.1.1.2.3.1.cmml\">​</mo><msup id=\"S2.p5.10.m10.1.1.2.3.3\" xref=\"S2.p5.10.m10.1.1.2.3.3.cmml\"><mi id=\"S2.p5.10.m10.1.1.2.3.3.2\" xref=\"S2.p5.10.m10.1.1.2.3.3.2.cmml\">K</mi><mo id=\"S2.p5.10.m10.1.1.2.3.3.3\" xref=\"S2.p5.10.m10.1.1.2.3.3.3.cmml\">−</mo></msup></mrow></msub><mo id=\"S2.p5.10.m10.1.1.1\" xref=\"S2.p5.10.m10.1.1.1.cmml\">&lt;</mo><mrow id=\"S2.p5.10.m10.1.1.3\" xref=\"S2.p5.10.m10.1.1.3.cmml\"><mrow id=\"S2.p5.10.m10.1.1.3.2\" xref=\"S2.p5.10.m10.1.1.3.2.cmml\"><mn id=\"S2.p5.10.m10.1.1.3.2.2\" xref=\"S2.p5.10.m10.1.1.3.2.2.cmml\">1.1</mn><mo lspace=\"0.330em\" rspace=\"0em\" id=\"S2.p5.10.m10.1.1.3.2.1\" xref=\"S2.p5.10.m10.1.1.3.2.1.cmml\">​</mo><mi id=\"S2.p5.10.m10.1.1.3.2.3\" xref=\"S2.p5.10.m10.1.1.3.2.3.cmml\">GeV</mi></mrow><mo id=\"S2.p5.10.m10.1.1.3.1\" xref=\"S2.p5.10.m10.1.1.3.1.cmml\">/</mo><msup id=\"S2.p5.10.m10.1.1.3.3\" xref=\"S2.p5.10.m10.1.1.3.3.cmml\"><mi id=\"S2.p5.10.m10.1.1.3.3.2\" xref=\"S2.p5.10.m10.1.1.3.3.2.cmml\">c</mi><mn id=\"S2.p5.10.m10.1.1.3.3.3\" xref=\"S2.p5.10.m10.1.1.3.3.3.cmml\">2</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.p5.10.m10.1b\"><apply id=\"S2.p5.10.m10.1.1.cmml\" xref=\"S2.p5.10.m10.1.1\"><lt id=\"S2.p5.10.m10.1.1.1.cmml\" xref=\"S2.p5.10.m10.1.1.1\"></lt><apply id=\"S2.p5.10.m10.1.1.2.cmml\" xref=\"S2.p5.10.m10.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.p5.10.m10.1.1.2.1.cmml\" xref=\"S2.p5.10.m10.1.1.2\">subscript</csymbol><ci id=\"S2.p5.10.m10.1.1.2.2.cmml\" xref=\"S2.p5.10.m10.1.1.2.2\">𝑀</ci><apply id=\"S2.p5.10.m10.1.1.2.3.cmml\" xref=\"S2.p5.10.m10.1.1.2.3\"><times id=\"S2.p5.10.m10.1.1.2.3.1.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.1\"></times><apply id=\"S2.p5.10.m10.1.1.2.3.2.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.p5.10.m10.1.1.2.3.2.1.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.2\">superscript</csymbol><ci id=\"S2.p5.10.m10.1.1.2.3.2.2.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.2.2\">𝐾</ci><plus id=\"S2.p5.10.m10.1.1.2.3.2.3.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.2.3\"></plus></apply><apply id=\"S2.p5.10.m10.1.1.2.3.3.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p5.10.m10.1.1.2.3.3.1.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.3\">superscript</csymbol><ci id=\"S2.p5.10.m10.1.1.2.3.3.2.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.3.2\">𝐾</ci><minus id=\"S2.p5.10.m10.1.1.2.3.3.3.cmml\" xref=\"S2.p5.10.m10.1.1.2.3.3.3\"></minus></apply></apply></apply><apply id=\"S2.p5.10.m10.1.1.3.cmml\" xref=\"S2.p5.10.m10.1.1.3\"><divide id=\"S2.p5.10.m10.1.1.3.1.cmml\" xref=\"S2.p5.10.m10.1.1.3.1\"></divide><apply id=\"S2.p5.10.m10.1.1.3.2.cmml\" xref=\"S2.p5.10.m10.1.1.3.2\"><times id=\"S2.p5.10.m10.1.1.3.2.1.cmml\" xref=\"S2.p5.10.m10.1.1.3.2.1\"></times><cn type=\"float\" id=\"S2.p5.10.m10.1.1.3.2.2.cmml\" xref=\"S2.p5.10.m10.1.1.3.2.2\">1.1</cn><ci id=\"S2.p5.10.m10.1.1.3.2.3.cmml\" xref=\"S2.p5.10.m10.1.1.3.2.3\">GeV</ci></apply><apply id=\"S2.p5.10.m10.1.1.3.3.cmml\" xref=\"S2.p5.10.m10.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.p5.10.m10.1.1.3.3.1.cmml\" xref=\"S2.p5.10.m10.1.1.3.3\">superscript</csymbol><ci id=\"S2.p5.10.m10.1.1.3.3.2.cmml\" xref=\"S2.p5.10.m10.1.1.3.3.2\">𝑐</ci><cn type=\"integer\" id=\"S2.p5.10.m10.1.1.3.3.3.cmml\" xref=\"S2.p5.10.m10.1.1.3.3.3\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.p5.10.m10.1c\">M_{K^{+}K^{-}}&lt;1.1~{}\\mathrm{GeV/}c^{2}</annotation></semantics></math>.</p>\n",
      "</div>\n",
      "<figure id=\"S2.F1\" class=\"ltx_figure\">\n",
      "<div class=\"ltx_flex_figure\">\n",
      "<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/1710.02962/assets/x1.png\" id=\"S2.F1.1.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"162\" height=\"77\" alt=\"Refer to caption\"></div>\n",
      "<div class=\"ltx_flex_break\"></div>\n",
      "<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/1710.02962/assets/x2.png\" id=\"S2.F1.2.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"162\" height=\"77\" alt=\"Refer to caption\"></div>\n",
      "</div>\n",
      "<figcaption class=\"ltx_caption\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Signal-enhanced projections of the <math id=\"S2.F1.8.m1.1\" class=\"ltx_Math\" alttext=\"M_{bc}\" display=\"inline\"><semantics id=\"S2.F1.8.m1.1b\"><msub id=\"S2.F1.8.m1.1.1\" xref=\"S2.F1.8.m1.1.1.cmml\"><mi id=\"S2.F1.8.m1.1.1.2\" xref=\"S2.F1.8.m1.1.1.2.cmml\">M</mi><mrow id=\"S2.F1.8.m1.1.1.3\" xref=\"S2.F1.8.m1.1.1.3.cmml\"><mi id=\"S2.F1.8.m1.1.1.3.2\" xref=\"S2.F1.8.m1.1.1.3.2.cmml\">b</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.F1.8.m1.1.1.3.1\" xref=\"S2.F1.8.m1.1.1.3.1.cmml\">​</mo><mi id=\"S2.F1.8.m1.1.1.3.3\" xref=\"S2.F1.8.m1.1.1.3.3.cmml\">c</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.F1.8.m1.1c\"><apply id=\"S2.F1.8.m1.1.1.cmml\" xref=\"S2.F1.8.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.F1.8.m1.1.1.1.cmml\" xref=\"S2.F1.8.m1.1.1\">subscript</csymbol><ci id=\"S2.F1.8.m1.1.1.2.cmml\" xref=\"S2.F1.8.m1.1.1.2\">𝑀</ci><apply id=\"S2.F1.8.m1.1.1.3.cmml\" xref=\"S2.F1.8.m1.1.1.3\"><times id=\"S2.F1.8.m1.1.1.3.1.cmml\" xref=\"S2.F1.8.m1.1.1.3.1\"></times><ci id=\"S2.F1.8.m1.1.1.3.2.cmml\" xref=\"S2.F1.8.m1.1.1.3.2\">𝑏</ci><ci id=\"S2.F1.8.m1.1.1.3.3.cmml\" xref=\"S2.F1.8.m1.1.1.3.3\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.F1.8.m1.1d\">M_{bc}</annotation></semantics></math>-<math id=\"S2.F1.9.m2.1\" class=\"ltx_Math\" alttext=\"\\Delta E\" display=\"inline\"><semantics id=\"S2.F1.9.m2.1b\"><mrow id=\"S2.F1.9.m2.1.1\" xref=\"S2.F1.9.m2.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.F1.9.m2.1.1.2\" xref=\"S2.F1.9.m2.1.1.2.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.F1.9.m2.1.1.1\" xref=\"S2.F1.9.m2.1.1.1.cmml\">​</mo><mi id=\"S2.F1.9.m2.1.1.3\" xref=\"S2.F1.9.m2.1.1.3.cmml\">E</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.F1.9.m2.1c\"><apply id=\"S2.F1.9.m2.1.1.cmml\" xref=\"S2.F1.9.m2.1.1\"><times id=\"S2.F1.9.m2.1.1.1.cmml\" xref=\"S2.F1.9.m2.1.1.1\"></times><ci id=\"S2.F1.9.m2.1.1.2.cmml\" xref=\"S2.F1.9.m2.1.1.2\">Δ</ci><ci id=\"S2.F1.9.m2.1.1.3.cmml\" xref=\"S2.F1.9.m2.1.1.3\">𝐸</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.F1.9.m2.1d\">\\Delta E</annotation></semantics></math> fit to data in the first (left) and second (right) <math id=\"S2.F1.10.m3.1\" class=\"ltx_Math\" alttext=\"M_{K^{+}K^{-}}\" display=\"inline\"><semantics id=\"S2.F1.10.m3.1b\"><msub id=\"S2.F1.10.m3.1.1\" xref=\"S2.F1.10.m3.1.1.cmml\"><mi id=\"S2.F1.10.m3.1.1.2\" xref=\"S2.F1.10.m3.1.1.2.cmml\">M</mi><mrow id=\"S2.F1.10.m3.1.1.3\" xref=\"S2.F1.10.m3.1.1.3.cmml\"><msup id=\"S2.F1.10.m3.1.1.3.2\" xref=\"S2.F1.10.m3.1.1.3.2.cmml\"><mi id=\"S2.F1.10.m3.1.1.3.2.2\" xref=\"S2.F1.10.m3.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.F1.10.m3.1.1.3.2.3\" xref=\"S2.F1.10.m3.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.F1.10.m3.1.1.3.1\" xref=\"S2.F1.10.m3.1.1.3.1.cmml\">​</mo><msup id=\"S2.F1.10.m3.1.1.3.3\" xref=\"S2.F1.10.m3.1.1.3.3.cmml\"><mi id=\"S2.F1.10.m3.1.1.3.3.2\" xref=\"S2.F1.10.m3.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.F1.10.m3.1.1.3.3.3\" xref=\"S2.F1.10.m3.1.1.3.3.3.cmml\">−</mo></msup></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.F1.10.m3.1c\"><apply id=\"S2.F1.10.m3.1.1.cmml\" xref=\"S2.F1.10.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.F1.10.m3.1.1.1.cmml\" xref=\"S2.F1.10.m3.1.1\">subscript</csymbol><ci id=\"S2.F1.10.m3.1.1.2.cmml\" xref=\"S2.F1.10.m3.1.1.2\">𝑀</ci><apply id=\"S2.F1.10.m3.1.1.3.cmml\" xref=\"S2.F1.10.m3.1.1.3\"><times id=\"S2.F1.10.m3.1.1.3.1.cmml\" xref=\"S2.F1.10.m3.1.1.3.1\"></times><apply id=\"S2.F1.10.m3.1.1.3.2.cmml\" xref=\"S2.F1.10.m3.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.F1.10.m3.1.1.3.2.1.cmml\" xref=\"S2.F1.10.m3.1.1.3.2\">superscript</csymbol><ci id=\"S2.F1.10.m3.1.1.3.2.2.cmml\" xref=\"S2.F1.10.m3.1.1.3.2.2\">𝐾</ci><plus id=\"S2.F1.10.m3.1.1.3.2.3.cmml\" xref=\"S2.F1.10.m3.1.1.3.2.3\"></plus></apply><apply id=\"S2.F1.10.m3.1.1.3.3.cmml\" xref=\"S2.F1.10.m3.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.F1.10.m3.1.1.3.3.1.cmml\" xref=\"S2.F1.10.m3.1.1.3.3\">superscript</csymbol><ci id=\"S2.F1.10.m3.1.1.3.3.2.cmml\" xref=\"S2.F1.10.m3.1.1.3.3.2\">𝐾</ci><minus id=\"S2.F1.10.m3.1.1.3.3.3.cmml\" xref=\"S2.F1.10.m3.1.1.3.3.3\"></minus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.F1.10.m3.1d\">M_{K^{+}K^{-}}</annotation></semantics></math> bins. Points with error bars are the data, red solid curves are the fit result, blue solid curves are the sum of the signal and the self cross-feed, cyan dotted curves are the continuum background, brown dash dotted curves are the generic <math id=\"S2.F1.11.m4.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S2.F1.11.m4.1b\"><mi id=\"S2.F1.11.m4.1.1\" xref=\"S2.F1.11.m4.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.F1.11.m4.1c\"><ci id=\"S2.F1.11.m4.1.1.cmml\" xref=\"S2.F1.11.m4.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.F1.11.m4.1d\">B</annotation></semantics></math> backgrounds, and green dashed curves are the rare <math id=\"S2.F1.12.m5.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S2.F1.12.m5.1b\"><mi id=\"S2.F1.12.m5.1.1\" xref=\"S2.F1.12.m5.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.F1.12.m5.1c\"><ci id=\"S2.F1.12.m5.1.1.cmml\" xref=\"S2.F1.12.m5.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.F1.12.m5.1d\">B</annotation></semantics></math> backgrounds.</figcaption>\n",
      "</figure>\n",
      "<figure id=\"S2.T1\" class=\"ltx_table\">\n",
      "<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Signal yield, efficiency, differential branching fraction, and <math id=\"S2.T1.3.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{A}_{CP}\" display=\"inline\"><semantics id=\"S2.T1.3.m1.1b\"><msub id=\"S2.T1.3.m1.1.1\" xref=\"S2.T1.3.m1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.T1.3.m1.1.1.2\" xref=\"S2.T1.3.m1.1.1.2.cmml\">𝒜</mi><mrow id=\"S2.T1.3.m1.1.1.3\" xref=\"S2.T1.3.m1.1.1.3.cmml\"><mi id=\"S2.T1.3.m1.1.1.3.2\" xref=\"S2.T1.3.m1.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T1.3.m1.1.1.3.1\" xref=\"S2.T1.3.m1.1.1.3.1.cmml\">​</mo><mi id=\"S2.T1.3.m1.1.1.3.3\" xref=\"S2.T1.3.m1.1.1.3.3.cmml\">P</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.3.m1.1c\"><apply id=\"S2.T1.3.m1.1.1.cmml\" xref=\"S2.T1.3.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.3.m1.1.1.1.cmml\" xref=\"S2.T1.3.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.3.m1.1.1.2.cmml\" xref=\"S2.T1.3.m1.1.1.2\">𝒜</ci><apply id=\"S2.T1.3.m1.1.1.3.cmml\" xref=\"S2.T1.3.m1.1.1.3\"><times id=\"S2.T1.3.m1.1.1.3.1.cmml\" xref=\"S2.T1.3.m1.1.1.3.1\"></times><ci id=\"S2.T1.3.m1.1.1.3.2.cmml\" xref=\"S2.T1.3.m1.1.1.3.2\">𝐶</ci><ci id=\"S2.T1.3.m1.1.1.3.3.cmml\" xref=\"S2.T1.3.m1.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.3.m1.1d\">\\mathcal{A}_{CP}</annotation></semantics></math> for individual <math id=\"S2.T1.4.m2.1\" class=\"ltx_Math\" alttext=\"M_{\\mathrm{{KK}}}\" display=\"inline\"><semantics id=\"S2.T1.4.m2.1b\"><msub id=\"S2.T1.4.m2.1.1\" xref=\"S2.T1.4.m2.1.1.cmml\"><mi id=\"S2.T1.4.m2.1.1.2\" xref=\"S2.T1.4.m2.1.1.2.cmml\">M</mi><mi id=\"S2.T1.4.m2.1.1.3\" xref=\"S2.T1.4.m2.1.1.3.cmml\">KK</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.4.m2.1c\"><apply id=\"S2.T1.4.m2.1.1.cmml\" xref=\"S2.T1.4.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.4.m2.1.1.1.cmml\" xref=\"S2.T1.4.m2.1.1\">subscript</csymbol><ci id=\"S2.T1.4.m2.1.1.2.cmml\" xref=\"S2.T1.4.m2.1.1.2\">𝑀</ci><ci id=\"S2.T1.4.m2.1.1.3.cmml\" xref=\"S2.T1.4.m2.1.1.3\">KK</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.4.m2.1d\">M_{\\mathrm{{KK}}}</annotation></semantics></math> bins </figcaption>\n",
      "<table id=\"S2.T1.24.20\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n",
      "<thead class=\"ltx_thead\">\n",
      "<tr id=\"S2.T1.8.4.4\" class=\"ltx_tr\">\n",
      "<th id=\"S2.T1.5.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"S2.T1.5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\textit{M}_{\\mathrm{{K^{+}K^{-}}}}\" display=\"inline\"><semantics id=\"S2.T1.5.1.1.1.m1.1a\"><msub id=\"S2.T1.5.1.1.1.m1.1.1\" xref=\"S2.T1.5.1.1.1.m1.1.1.cmml\"><mtext class=\"ltx_mathvariant_italic\" id=\"S2.T1.5.1.1.1.m1.1.1.2\" xref=\"S2.T1.5.1.1.1.m1.1.1.2a.cmml\">M</mtext><mrow id=\"S2.T1.5.1.1.1.m1.1.1.3\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.cmml\"><msup id=\"S2.T1.5.1.1.1.m1.1.1.3.2\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.2.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.5.1.1.1.m1.1.1.3.2.2\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.T1.5.1.1.1.m1.1.1.3.2.3\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T1.5.1.1.1.m1.1.1.3.1\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.1.cmml\">​</mo><msup id=\"S2.T1.5.1.1.1.m1.1.1.3.3\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.3.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.5.1.1.1.m1.1.1.3.3.2\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.T1.5.1.1.1.m1.1.1.3.3.3\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.3.3.cmml\">−</mo></msup></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.5.1.1.1.m1.1b\"><apply id=\"S2.T1.5.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.5.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.5.1.1.1.m1.1.1.2a.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.2\"><mtext class=\"ltx_mathvariant_italic\" id=\"S2.T1.5.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.2\">M</mtext></ci><apply id=\"S2.T1.5.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3\"><times id=\"S2.T1.5.1.1.1.m1.1.1.3.1.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.1\"></times><apply id=\"S2.T1.5.1.1.1.m1.1.1.3.2.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.T1.5.1.1.1.m1.1.1.3.2.1.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.2\">superscript</csymbol><ci id=\"S2.T1.5.1.1.1.m1.1.1.3.2.2.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.2.2\">K</ci><plus id=\"S2.T1.5.1.1.1.m1.1.1.3.2.3.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.2.3\"></plus></apply><apply id=\"S2.T1.5.1.1.1.m1.1.1.3.3.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.5.1.1.1.m1.1.1.3.3.1.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.3\">superscript</csymbol><ci id=\"S2.T1.5.1.1.1.m1.1.1.3.3.2.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.3.2\">K</ci><minus id=\"S2.T1.5.1.1.1.m1.1.1.3.3.3.cmml\" xref=\"S2.T1.5.1.1.1.m1.1.1.3.3.3\"></minus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.5.1.1.1.m1.1c\">\\textit{M}_{\\mathrm{{K^{+}K^{-}}}}</annotation></semantics></math></th>\n",
      "<th id=\"S2.T1.6.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"S2.T1.6.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"N_{\\mathrm{{sig}}}\" display=\"inline\"><semantics id=\"S2.T1.6.2.2.2.m1.1a\"><msub id=\"S2.T1.6.2.2.2.m1.1.1\" xref=\"S2.T1.6.2.2.2.m1.1.1.cmml\"><mi id=\"S2.T1.6.2.2.2.m1.1.1.2\" xref=\"S2.T1.6.2.2.2.m1.1.1.2.cmml\">N</mi><mi id=\"S2.T1.6.2.2.2.m1.1.1.3\" xref=\"S2.T1.6.2.2.2.m1.1.1.3.cmml\">sig</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.6.2.2.2.m1.1b\"><apply id=\"S2.T1.6.2.2.2.m1.1.1.cmml\" xref=\"S2.T1.6.2.2.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.6.2.2.2.m1.1.1.1.cmml\" xref=\"S2.T1.6.2.2.2.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.6.2.2.2.m1.1.1.2.cmml\" xref=\"S2.T1.6.2.2.2.m1.1.1.2\">𝑁</ci><ci id=\"S2.T1.6.2.2.2.m1.1.1.3.cmml\" xref=\"S2.T1.6.2.2.2.m1.1.1.3\">sig</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.6.2.2.2.m1.1c\">N_{\\mathrm{{sig}}}</annotation></semantics></math></th>\n",
      "<th id=\"S2.T1.8.4.4.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Eff. (%)</th>\n",
      "<th id=\"S2.T1.7.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"S2.T1.7.3.3.3.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\textit{d}\\mathrm{\\mathcal{B}/\\textit{dM}~{}(\\times 10^{-7}})\" display=\"inline\"><semantics id=\"S2.T1.7.3.3.3.m1.1a\"><mrow id=\"S2.T1.7.3.3.3.m1.1b\"><mtext class=\"ltx_mathvariant_italic\" id=\"S2.T1.7.3.3.3.m1.1.1\">d</mtext><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.T1.7.3.3.3.m1.1.2\">ℬ</mi><mo id=\"S2.T1.7.3.3.3.m1.1.3\">/</mo><mtext class=\"ltx_mathvariant_italic\" id=\"S2.T1.7.3.3.3.m1.1.4\">dM</mtext><mrow id=\"S2.T1.7.3.3.3.m1.1.5\"><mo lspace=\"0.330em\" stretchy=\"false\" id=\"S2.T1.7.3.3.3.m1.1.5.1\">(</mo><mo lspace=\"0em\" rspace=\"0.222em\" id=\"S2.T1.7.3.3.3.m1.1.5.2\">×</mo><msup id=\"S2.T1.7.3.3.3.m1.1.5.3\"><mn id=\"S2.T1.7.3.3.3.m1.1.5.3.2\">10</mn><mrow id=\"S2.T1.7.3.3.3.m1.1.5.3.3\"><mo id=\"S2.T1.7.3.3.3.m1.1.5.3.3a\">−</mo><mn id=\"S2.T1.7.3.3.3.m1.1.5.3.3.2\">7</mn></mrow></msup><mo stretchy=\"false\" id=\"S2.T1.7.3.3.3.m1.1.5.4\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S2.T1.7.3.3.3.m1.1c\">\\textit{d}\\mathrm{\\mathcal{B}/\\textit{dM}~{}(\\times 10^{-7}})</annotation></semantics></math></th>\n",
      "<th id=\"S2.T1.8.4.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"S2.T1.8.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"{A}_{CP}\" display=\"inline\"><semantics id=\"S2.T1.8.4.4.4.m1.1a\"><msub id=\"S2.T1.8.4.4.4.m1.1.1\" xref=\"S2.T1.8.4.4.4.m1.1.1.cmml\"><mi id=\"S2.T1.8.4.4.4.m1.1.1.2\" xref=\"S2.T1.8.4.4.4.m1.1.1.2.cmml\">A</mi><mrow id=\"S2.T1.8.4.4.4.m1.1.1.3\" xref=\"S2.T1.8.4.4.4.m1.1.1.3.cmml\"><mi id=\"S2.T1.8.4.4.4.m1.1.1.3.2\" xref=\"S2.T1.8.4.4.4.m1.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T1.8.4.4.4.m1.1.1.3.1\" xref=\"S2.T1.8.4.4.4.m1.1.1.3.1.cmml\">​</mo><mi id=\"S2.T1.8.4.4.4.m1.1.1.3.3\" xref=\"S2.T1.8.4.4.4.m1.1.1.3.3.cmml\">P</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.8.4.4.4.m1.1b\"><apply id=\"S2.T1.8.4.4.4.m1.1.1.cmml\" xref=\"S2.T1.8.4.4.4.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.8.4.4.4.m1.1.1.1.cmml\" xref=\"S2.T1.8.4.4.4.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.8.4.4.4.m1.1.1.2.cmml\" xref=\"S2.T1.8.4.4.4.m1.1.1.2\">𝐴</ci><apply id=\"S2.T1.8.4.4.4.m1.1.1.3.cmml\" xref=\"S2.T1.8.4.4.4.m1.1.1.3\"><times id=\"S2.T1.8.4.4.4.m1.1.1.3.1.cmml\" xref=\"S2.T1.8.4.4.4.m1.1.1.3.1\"></times><ci id=\"S2.T1.8.4.4.4.m1.1.1.3.2.cmml\" xref=\"S2.T1.8.4.4.4.m1.1.1.3.2\">𝐶</ci><ci id=\"S2.T1.8.4.4.4.m1.1.1.3.3.cmml\" xref=\"S2.T1.8.4.4.4.m1.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.8.4.4.4.m1.1c\">{A}_{CP}</annotation></semantics></math></th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody class=\"ltx_tbody\">\n",
      "<tr id=\"S2.T1.9.5.5\" class=\"ltx_tr\">\n",
      "<th id=\"S2.T1.9.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><math id=\"S2.T1.9.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathrm{(GeV/c^{2})}\" display=\"inline\"><semantics id=\"S2.T1.9.5.5.1.m1.1a\"><mrow id=\"S2.T1.9.5.5.1.m1.1.1.1\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S2.T1.9.5.5.1.m1.1.1.1.2\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.cmml\">(</mo><mrow id=\"S2.T1.9.5.5.1.m1.1.1.1.1\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.cmml\"><mi id=\"S2.T1.9.5.5.1.m1.1.1.1.1.2\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.2.cmml\">GeV</mi><mo id=\"S2.T1.9.5.5.1.m1.1.1.1.1.1\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.1.cmml\">/</mo><msup id=\"S2.T1.9.5.5.1.m1.1.1.1.1.3\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.2\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.2.cmml\">c</mi><mn id=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.3\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.3.cmml\">2</mn></msup></mrow><mo stretchy=\"false\" id=\"S2.T1.9.5.5.1.m1.1.1.1.3\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.9.5.5.1.m1.1b\"><apply id=\"S2.T1.9.5.5.1.m1.1.1.1.1.cmml\" xref=\"S2.T1.9.5.5.1.m1.1.1.1\"><divide id=\"S2.T1.9.5.5.1.m1.1.1.1.1.1.cmml\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.1\"></divide><ci id=\"S2.T1.9.5.5.1.m1.1.1.1.1.2.cmml\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.2\">GeV</ci><apply id=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.cmml\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.1.cmml\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.3\">superscript</csymbol><ci id=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.2.cmml\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.2\">c</ci><cn type=\"integer\" id=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.3.cmml\" xref=\"S2.T1.9.5.5.1.m1.1.1.1.1.3.3\">2</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.9.5.5.1.m1.1c\">\\mathrm{(GeV/c^{2})}</annotation></semantics></math></th>\n",
      "<th id=\"S2.T1.9.5.5.2\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n",
      "<th id=\"S2.T1.9.5.5.3\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n",
      "<th id=\"S2.T1.9.5.5.4\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n",
      "<td id=\"S2.T1.9.5.5.5\" class=\"ltx_td\"></td>\n",
      "</tr>\n",
      "<tr id=\"S2.T1.12.8.8\" class=\"ltx_tr\">\n",
      "<td id=\"S2.T1.12.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8–1.1</td>\n",
      "<td id=\"S2.T1.10.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\">  <math id=\"S2.T1.10.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"59.8\\pm 11.4\\pm 2.6\" display=\"inline\"><semantics id=\"S2.T1.10.6.6.1.m1.1a\"><mrow id=\"S2.T1.10.6.6.1.m1.1.1\" xref=\"S2.T1.10.6.6.1.m1.1.1.cmml\"><mn id=\"S2.T1.10.6.6.1.m1.1.1.2\" xref=\"S2.T1.10.6.6.1.m1.1.1.2.cmml\">59.8</mn><mo id=\"S2.T1.10.6.6.1.m1.1.1.1\" xref=\"S2.T1.10.6.6.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.10.6.6.1.m1.1.1.3\" xref=\"S2.T1.10.6.6.1.m1.1.1.3.cmml\">11.4</mn><mo id=\"S2.T1.10.6.6.1.m1.1.1.1a\" xref=\"S2.T1.10.6.6.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.10.6.6.1.m1.1.1.4\" xref=\"S2.T1.10.6.6.1.m1.1.1.4.cmml\">2.6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.10.6.6.1.m1.1b\"><apply id=\"S2.T1.10.6.6.1.m1.1.1.cmml\" xref=\"S2.T1.10.6.6.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.10.6.6.1.m1.1.1.1.cmml\" xref=\"S2.T1.10.6.6.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.10.6.6.1.m1.1.1.2.cmml\" xref=\"S2.T1.10.6.6.1.m1.1.1.2\">59.8</cn><cn type=\"float\" id=\"S2.T1.10.6.6.1.m1.1.1.3.cmml\" xref=\"S2.T1.10.6.6.1.m1.1.1.3\">11.4</cn><cn type=\"float\" id=\"S2.T1.10.6.6.1.m1.1.1.4.cmml\" xref=\"S2.T1.10.6.6.1.m1.1.1.4\">2.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.10.6.6.1.m1.1c\">59.8\\pm 11.4\\pm 2.6</annotation></semantics></math>\n",
      "</td>\n",
      "<td id=\"S2.T1.12.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_t\">19.7</td>\n",
      "<td id=\"S2.T1.11.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S2.T1.11.7.7.2.m1.1\" class=\"ltx_Math\" alttext=\"14.0\\pm 2.7\\pm 0.8\" display=\"inline\"><semantics id=\"S2.T1.11.7.7.2.m1.1a\"><mrow id=\"S2.T1.11.7.7.2.m1.1.1\" xref=\"S2.T1.11.7.7.2.m1.1.1.cmml\"><mn id=\"S2.T1.11.7.7.2.m1.1.1.2\" xref=\"S2.T1.11.7.7.2.m1.1.1.2.cmml\">14.0</mn><mo id=\"S2.T1.11.7.7.2.m1.1.1.1\" xref=\"S2.T1.11.7.7.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.11.7.7.2.m1.1.1.3\" xref=\"S2.T1.11.7.7.2.m1.1.1.3.cmml\">2.7</mn><mo id=\"S2.T1.11.7.7.2.m1.1.1.1a\" xref=\"S2.T1.11.7.7.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.11.7.7.2.m1.1.1.4\" xref=\"S2.T1.11.7.7.2.m1.1.1.4.cmml\">0.8</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.11.7.7.2.m1.1b\"><apply id=\"S2.T1.11.7.7.2.m1.1.1.cmml\" xref=\"S2.T1.11.7.7.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.11.7.7.2.m1.1.1.1.cmml\" xref=\"S2.T1.11.7.7.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.11.7.7.2.m1.1.1.2.cmml\" xref=\"S2.T1.11.7.7.2.m1.1.1.2\">14.0</cn><cn type=\"float\" id=\"S2.T1.11.7.7.2.m1.1.1.3.cmml\" xref=\"S2.T1.11.7.7.2.m1.1.1.3\">2.7</cn><cn type=\"float\" id=\"S2.T1.11.7.7.2.m1.1.1.4.cmml\" xref=\"S2.T1.11.7.7.2.m1.1.1.4\">0.8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.11.7.7.2.m1.1c\">14.0\\pm 2.7\\pm 0.8</annotation></semantics></math></td>\n",
      "<td id=\"S2.T1.12.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S2.T1.12.8.8.3.m1.1\" class=\"ltx_Math\" alttext=\"-0.90\\pm 0.17\\pm 0.03\" display=\"inline\"><semantics id=\"S2.T1.12.8.8.3.m1.1a\"><mrow id=\"S2.T1.12.8.8.3.m1.1.1\" xref=\"S2.T1.12.8.8.3.m1.1.1.cmml\"><mrow id=\"S2.T1.12.8.8.3.m1.1.1.2\" xref=\"S2.T1.12.8.8.3.m1.1.1.2.cmml\"><mo id=\"S2.T1.12.8.8.3.m1.1.1.2a\" xref=\"S2.T1.12.8.8.3.m1.1.1.2.cmml\">−</mo><mn id=\"S2.T1.12.8.8.3.m1.1.1.2.2\" xref=\"S2.T1.12.8.8.3.m1.1.1.2.2.cmml\">0.90</mn></mrow><mo id=\"S2.T1.12.8.8.3.m1.1.1.1\" xref=\"S2.T1.12.8.8.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.12.8.8.3.m1.1.1.3\" xref=\"S2.T1.12.8.8.3.m1.1.1.3.cmml\">0.17</mn><mo id=\"S2.T1.12.8.8.3.m1.1.1.1a\" xref=\"S2.T1.12.8.8.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.12.8.8.3.m1.1.1.4\" xref=\"S2.T1.12.8.8.3.m1.1.1.4.cmml\">0.03</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.12.8.8.3.m1.1b\"><apply id=\"S2.T1.12.8.8.3.m1.1.1.cmml\" xref=\"S2.T1.12.8.8.3.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.12.8.8.3.m1.1.1.1.cmml\" xref=\"S2.T1.12.8.8.3.m1.1.1.1\">plus-or-minus</csymbol><apply id=\"S2.T1.12.8.8.3.m1.1.1.2.cmml\" xref=\"S2.T1.12.8.8.3.m1.1.1.2\"><minus id=\"S2.T1.12.8.8.3.m1.1.1.2.1.cmml\" xref=\"S2.T1.12.8.8.3.m1.1.1.2\"></minus><cn type=\"float\" id=\"S2.T1.12.8.8.3.m1.1.1.2.2.cmml\" xref=\"S2.T1.12.8.8.3.m1.1.1.2.2\">0.90</cn></apply><cn type=\"float\" id=\"S2.T1.12.8.8.3.m1.1.1.3.cmml\" xref=\"S2.T1.12.8.8.3.m1.1.1.3\">0.17</cn><cn type=\"float\" id=\"S2.T1.12.8.8.3.m1.1.1.4.cmml\" xref=\"S2.T1.12.8.8.3.m1.1.1.4\">0.03</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.12.8.8.3.m1.1c\">-0.90\\pm 0.17\\pm 0.03</annotation></semantics></math></td>\n",
      "</tr>\n",
      "<tr id=\"S2.T1.15.11.11\" class=\"ltx_tr\">\n",
      "<td id=\"S2.T1.15.11.11.4\" class=\"ltx_td ltx_align_center\">1.1–1.5</td>\n",
      "<td id=\"S2.T1.13.9.9.1\" class=\"ltx_td ltx_align_center\"><math id=\"S2.T1.13.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"212.4\\pm 21.3\\pm 6.6\" display=\"inline\"><semantics id=\"S2.T1.13.9.9.1.m1.1a\"><mrow id=\"S2.T1.13.9.9.1.m1.1.1\" xref=\"S2.T1.13.9.9.1.m1.1.1.cmml\"><mn id=\"S2.T1.13.9.9.1.m1.1.1.2\" xref=\"S2.T1.13.9.9.1.m1.1.1.2.cmml\">212.4</mn><mo id=\"S2.T1.13.9.9.1.m1.1.1.1\" xref=\"S2.T1.13.9.9.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.13.9.9.1.m1.1.1.3\" xref=\"S2.T1.13.9.9.1.m1.1.1.3.cmml\">21.3</mn><mo id=\"S2.T1.13.9.9.1.m1.1.1.1a\" xref=\"S2.T1.13.9.9.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.13.9.9.1.m1.1.1.4\" xref=\"S2.T1.13.9.9.1.m1.1.1.4.cmml\">6.6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.13.9.9.1.m1.1b\"><apply id=\"S2.T1.13.9.9.1.m1.1.1.cmml\" xref=\"S2.T1.13.9.9.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.13.9.9.1.m1.1.1.1.cmml\" xref=\"S2.T1.13.9.9.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.13.9.9.1.m1.1.1.2.cmml\" xref=\"S2.T1.13.9.9.1.m1.1.1.2\">212.4</cn><cn type=\"float\" id=\"S2.T1.13.9.9.1.m1.1.1.3.cmml\" xref=\"S2.T1.13.9.9.1.m1.1.1.3\">21.3</cn><cn type=\"float\" id=\"S2.T1.13.9.9.1.m1.1.1.4.cmml\" xref=\"S2.T1.13.9.9.1.m1.1.1.4\">6.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.13.9.9.1.m1.1c\">212.4\\pm 21.3\\pm 6.6</annotation></semantics></math></td>\n",
      "<td id=\"S2.T1.15.11.11.5\" class=\"ltx_td ltx_align_center\">19.3</td>\n",
      "<td id=\"S2.T1.14.10.10.2\" class=\"ltx_td ltx_align_center\"><math id=\"S2.T1.14.10.10.2.m1.1\" class=\"ltx_Math\" alttext=\"37.8\\pm 3.8\\pm 1.9\" display=\"inline\"><semantics id=\"S2.T1.14.10.10.2.m1.1a\"><mrow id=\"S2.T1.14.10.10.2.m1.1.1\" xref=\"S2.T1.14.10.10.2.m1.1.1.cmml\"><mn id=\"S2.T1.14.10.10.2.m1.1.1.2\" xref=\"S2.T1.14.10.10.2.m1.1.1.2.cmml\">37.8</mn><mo id=\"S2.T1.14.10.10.2.m1.1.1.1\" xref=\"S2.T1.14.10.10.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.14.10.10.2.m1.1.1.3\" xref=\"S2.T1.14.10.10.2.m1.1.1.3.cmml\">3.8</mn><mo id=\"S2.T1.14.10.10.2.m1.1.1.1a\" xref=\"S2.T1.14.10.10.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.14.10.10.2.m1.1.1.4\" xref=\"S2.T1.14.10.10.2.m1.1.1.4.cmml\">1.9</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.14.10.10.2.m1.1b\"><apply id=\"S2.T1.14.10.10.2.m1.1.1.cmml\" xref=\"S2.T1.14.10.10.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.14.10.10.2.m1.1.1.1.cmml\" xref=\"S2.T1.14.10.10.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.14.10.10.2.m1.1.1.2.cmml\" xref=\"S2.T1.14.10.10.2.m1.1.1.2\">37.8</cn><cn type=\"float\" id=\"S2.T1.14.10.10.2.m1.1.1.3.cmml\" xref=\"S2.T1.14.10.10.2.m1.1.1.3\">3.8</cn><cn type=\"float\" id=\"S2.T1.14.10.10.2.m1.1.1.4.cmml\" xref=\"S2.T1.14.10.10.2.m1.1.1.4\">1.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.14.10.10.2.m1.1c\">37.8\\pm 3.8\\pm 1.9</annotation></semantics></math></td>\n",
      "<td id=\"S2.T1.15.11.11.3\" class=\"ltx_td ltx_align_center\"><math id=\"S2.T1.15.11.11.3.m1.1\" class=\"ltx_Math\" alttext=\"-0.16\\pm 0.10\\pm 0.01\" display=\"inline\"><semantics id=\"S2.T1.15.11.11.3.m1.1a\"><mrow id=\"S2.T1.15.11.11.3.m1.1.1\" xref=\"S2.T1.15.11.11.3.m1.1.1.cmml\"><mrow id=\"S2.T1.15.11.11.3.m1.1.1.2\" xref=\"S2.T1.15.11.11.3.m1.1.1.2.cmml\"><mo id=\"S2.T1.15.11.11.3.m1.1.1.2a\" xref=\"S2.T1.15.11.11.3.m1.1.1.2.cmml\">−</mo><mn id=\"S2.T1.15.11.11.3.m1.1.1.2.2\" xref=\"S2.T1.15.11.11.3.m1.1.1.2.2.cmml\">0.16</mn></mrow><mo id=\"S2.T1.15.11.11.3.m1.1.1.1\" xref=\"S2.T1.15.11.11.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.15.11.11.3.m1.1.1.3\" xref=\"S2.T1.15.11.11.3.m1.1.1.3.cmml\">0.10</mn><mo id=\"S2.T1.15.11.11.3.m1.1.1.1a\" xref=\"S2.T1.15.11.11.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.15.11.11.3.m1.1.1.4\" xref=\"S2.T1.15.11.11.3.m1.1.1.4.cmml\">0.01</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.15.11.11.3.m1.1b\"><apply id=\"S2.T1.15.11.11.3.m1.1.1.cmml\" xref=\"S2.T1.15.11.11.3.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.15.11.11.3.m1.1.1.1.cmml\" xref=\"S2.T1.15.11.11.3.m1.1.1.1\">plus-or-minus</csymbol><apply id=\"S2.T1.15.11.11.3.m1.1.1.2.cmml\" xref=\"S2.T1.15.11.11.3.m1.1.1.2\"><minus id=\"S2.T1.15.11.11.3.m1.1.1.2.1.cmml\" xref=\"S2.T1.15.11.11.3.m1.1.1.2\"></minus><cn type=\"float\" id=\"S2.T1.15.11.11.3.m1.1.1.2.2.cmml\" xref=\"S2.T1.15.11.11.3.m1.1.1.2.2\">0.16</cn></apply><cn type=\"float\" id=\"S2.T1.15.11.11.3.m1.1.1.3.cmml\" xref=\"S2.T1.15.11.11.3.m1.1.1.3\">0.10</cn><cn type=\"float\" id=\"S2.T1.15.11.11.3.m1.1.1.4.cmml\" xref=\"S2.T1.15.11.11.3.m1.1.1.4\">0.01</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.15.11.11.3.m1.1c\">-0.16\\pm 0.10\\pm 0.01</annotation></semantics></math></td>\n",
      "</tr>\n",
      "<tr id=\"S2.T1.18.14.14\" class=\"ltx_tr\">\n",
      "<td id=\"S2.T1.18.14.14.4\" class=\"ltx_td ltx_align_center\">1.5–2.5</td>\n",
      "<td id=\"S2.T1.16.12.12.1\" class=\"ltx_td ltx_align_center\"> <math id=\"S2.T1.16.12.12.1.m1.1\" class=\"ltx_Math\" alttext=\"113.5\\pm 26.7\\pm 18.0\" display=\"inline\"><semantics id=\"S2.T1.16.12.12.1.m1.1a\"><mrow id=\"S2.T1.16.12.12.1.m1.1.1\" xref=\"S2.T1.16.12.12.1.m1.1.1.cmml\"><mn id=\"S2.T1.16.12.12.1.m1.1.1.2\" xref=\"S2.T1.16.12.12.1.m1.1.1.2.cmml\">113.5</mn><mo id=\"S2.T1.16.12.12.1.m1.1.1.1\" xref=\"S2.T1.16.12.12.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.16.12.12.1.m1.1.1.3\" xref=\"S2.T1.16.12.12.1.m1.1.1.3.cmml\">26.7</mn><mo id=\"S2.T1.16.12.12.1.m1.1.1.1a\" xref=\"S2.T1.16.12.12.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.16.12.12.1.m1.1.1.4\" xref=\"S2.T1.16.12.12.1.m1.1.1.4.cmml\">18.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.16.12.12.1.m1.1b\"><apply id=\"S2.T1.16.12.12.1.m1.1.1.cmml\" xref=\"S2.T1.16.12.12.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.16.12.12.1.m1.1.1.1.cmml\" xref=\"S2.T1.16.12.12.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.16.12.12.1.m1.1.1.2.cmml\" xref=\"S2.T1.16.12.12.1.m1.1.1.2\">113.5</cn><cn type=\"float\" id=\"S2.T1.16.12.12.1.m1.1.1.3.cmml\" xref=\"S2.T1.16.12.12.1.m1.1.1.3\">26.7</cn><cn type=\"float\" id=\"S2.T1.16.12.12.1.m1.1.1.4.cmml\" xref=\"S2.T1.16.12.12.1.m1.1.1.4\">18.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.16.12.12.1.m1.1c\">113.5\\pm 26.7\\pm 18.0</annotation></semantics></math>\n",
      "</td>\n",
      "<td id=\"S2.T1.18.14.14.5\" class=\"ltx_td ltx_align_center\">15.6</td>\n",
      "<td id=\"S2.T1.17.13.13.2\" class=\"ltx_td ltx_align_center\"><math id=\"S2.T1.17.13.13.2.m1.1\" class=\"ltx_Math\" alttext=\"10.0\\pm 2.3\\pm 1.6\" display=\"inline\"><semantics id=\"S2.T1.17.13.13.2.m1.1a\"><mrow id=\"S2.T1.17.13.13.2.m1.1.1\" xref=\"S2.T1.17.13.13.2.m1.1.1.cmml\"><mn id=\"S2.T1.17.13.13.2.m1.1.1.2\" xref=\"S2.T1.17.13.13.2.m1.1.1.2.cmml\">10.0</mn><mo id=\"S2.T1.17.13.13.2.m1.1.1.1\" xref=\"S2.T1.17.13.13.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.17.13.13.2.m1.1.1.3\" xref=\"S2.T1.17.13.13.2.m1.1.1.3.cmml\">2.3</mn><mo id=\"S2.T1.17.13.13.2.m1.1.1.1a\" xref=\"S2.T1.17.13.13.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.17.13.13.2.m1.1.1.4\" xref=\"S2.T1.17.13.13.2.m1.1.1.4.cmml\">1.6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.17.13.13.2.m1.1b\"><apply id=\"S2.T1.17.13.13.2.m1.1.1.cmml\" xref=\"S2.T1.17.13.13.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.17.13.13.2.m1.1.1.1.cmml\" xref=\"S2.T1.17.13.13.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.17.13.13.2.m1.1.1.2.cmml\" xref=\"S2.T1.17.13.13.2.m1.1.1.2\">10.0</cn><cn type=\"float\" id=\"S2.T1.17.13.13.2.m1.1.1.3.cmml\" xref=\"S2.T1.17.13.13.2.m1.1.1.3\">2.3</cn><cn type=\"float\" id=\"S2.T1.17.13.13.2.m1.1.1.4.cmml\" xref=\"S2.T1.17.13.13.2.m1.1.1.4\">1.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.17.13.13.2.m1.1c\">10.0\\pm 2.3\\pm 1.6</annotation></semantics></math></td>\n",
      "<td id=\"S2.T1.18.14.14.3\" class=\"ltx_td ltx_align_center\"><math id=\"S2.T1.18.14.14.3.m1.1\" class=\"ltx_Math\" alttext=\"-0.15\\pm 0.23\\pm 0.03\" display=\"inline\"><semantics id=\"S2.T1.18.14.14.3.m1.1a\"><mrow id=\"S2.T1.18.14.14.3.m1.1.1\" xref=\"S2.T1.18.14.14.3.m1.1.1.cmml\"><mrow id=\"S2.T1.18.14.14.3.m1.1.1.2\" xref=\"S2.T1.18.14.14.3.m1.1.1.2.cmml\"><mo id=\"S2.T1.18.14.14.3.m1.1.1.2a\" xref=\"S2.T1.18.14.14.3.m1.1.1.2.cmml\">−</mo><mn id=\"S2.T1.18.14.14.3.m1.1.1.2.2\" xref=\"S2.T1.18.14.14.3.m1.1.1.2.2.cmml\">0.15</mn></mrow><mo id=\"S2.T1.18.14.14.3.m1.1.1.1\" xref=\"S2.T1.18.14.14.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.18.14.14.3.m1.1.1.3\" xref=\"S2.T1.18.14.14.3.m1.1.1.3.cmml\">0.23</mn><mo id=\"S2.T1.18.14.14.3.m1.1.1.1a\" xref=\"S2.T1.18.14.14.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.18.14.14.3.m1.1.1.4\" xref=\"S2.T1.18.14.14.3.m1.1.1.4.cmml\">0.03</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.18.14.14.3.m1.1b\"><apply id=\"S2.T1.18.14.14.3.m1.1.1.cmml\" xref=\"S2.T1.18.14.14.3.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.18.14.14.3.m1.1.1.1.cmml\" xref=\"S2.T1.18.14.14.3.m1.1.1.1\">plus-or-minus</csymbol><apply id=\"S2.T1.18.14.14.3.m1.1.1.2.cmml\" xref=\"S2.T1.18.14.14.3.m1.1.1.2\"><minus id=\"S2.T1.18.14.14.3.m1.1.1.2.1.cmml\" xref=\"S2.T1.18.14.14.3.m1.1.1.2\"></minus><cn type=\"float\" id=\"S2.T1.18.14.14.3.m1.1.1.2.2.cmml\" xref=\"S2.T1.18.14.14.3.m1.1.1.2.2\">0.15</cn></apply><cn type=\"float\" id=\"S2.T1.18.14.14.3.m1.1.1.3.cmml\" xref=\"S2.T1.18.14.14.3.m1.1.1.3\">0.23</cn><cn type=\"float\" id=\"S2.T1.18.14.14.3.m1.1.1.4.cmml\" xref=\"S2.T1.18.14.14.3.m1.1.1.4\">0.03</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.18.14.14.3.m1.1c\">-0.15\\pm 0.23\\pm 0.03</annotation></semantics></math></td>\n",
      "</tr>\n",
      "<tr id=\"S2.T1.21.17.17\" class=\"ltx_tr\">\n",
      "<td id=\"S2.T1.21.17.17.4\" class=\"ltx_td ltx_align_center\">2.5–3.5</td>\n",
      "<td id=\"S2.T1.19.15.15.1\" class=\"ltx_td ltx_align_center\"><math id=\"S2.T1.19.15.15.1.m1.1\" class=\"ltx_Math\" alttext=\"110.1\\pm 17.6\\pm 4.1\" display=\"inline\"><semantics id=\"S2.T1.19.15.15.1.m1.1a\"><mrow id=\"S2.T1.19.15.15.1.m1.1.1\" xref=\"S2.T1.19.15.15.1.m1.1.1.cmml\"><mn id=\"S2.T1.19.15.15.1.m1.1.1.2\" xref=\"S2.T1.19.15.15.1.m1.1.1.2.cmml\">110.1</mn><mo id=\"S2.T1.19.15.15.1.m1.1.1.1\" xref=\"S2.T1.19.15.15.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.19.15.15.1.m1.1.1.3\" xref=\"S2.T1.19.15.15.1.m1.1.1.3.cmml\">17.6</mn><mo id=\"S2.T1.19.15.15.1.m1.1.1.1a\" xref=\"S2.T1.19.15.15.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.19.15.15.1.m1.1.1.4\" xref=\"S2.T1.19.15.15.1.m1.1.1.4.cmml\">4.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.19.15.15.1.m1.1b\"><apply id=\"S2.T1.19.15.15.1.m1.1.1.cmml\" xref=\"S2.T1.19.15.15.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.19.15.15.1.m1.1.1.1.cmml\" xref=\"S2.T1.19.15.15.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.19.15.15.1.m1.1.1.2.cmml\" xref=\"S2.T1.19.15.15.1.m1.1.1.2\">110.1</cn><cn type=\"float\" id=\"S2.T1.19.15.15.1.m1.1.1.3.cmml\" xref=\"S2.T1.19.15.15.1.m1.1.1.3\">17.6</cn><cn type=\"float\" id=\"S2.T1.19.15.15.1.m1.1.1.4.cmml\" xref=\"S2.T1.19.15.15.1.m1.1.1.4\">4.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.19.15.15.1.m1.1c\">110.1\\pm 17.6\\pm 4.1</annotation></semantics></math></td>\n",
      "<td id=\"S2.T1.21.17.17.5\" class=\"ltx_td ltx_align_center\">15.1</td>\n",
      "<td id=\"S2.T1.20.16.16.2\" class=\"ltx_td ltx_align_center\"><math id=\"S2.T1.20.16.16.2.m1.1\" class=\"ltx_Math\" alttext=\"10.0\\pm 1.6\\pm 0.5\" display=\"inline\"><semantics id=\"S2.T1.20.16.16.2.m1.1a\"><mrow id=\"S2.T1.20.16.16.2.m1.1.1\" xref=\"S2.T1.20.16.16.2.m1.1.1.cmml\"><mn id=\"S2.T1.20.16.16.2.m1.1.1.2\" xref=\"S2.T1.20.16.16.2.m1.1.1.2.cmml\">10.0</mn><mo id=\"S2.T1.20.16.16.2.m1.1.1.1\" xref=\"S2.T1.20.16.16.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.20.16.16.2.m1.1.1.3\" xref=\"S2.T1.20.16.16.2.m1.1.1.3.cmml\">1.6</mn><mo id=\"S2.T1.20.16.16.2.m1.1.1.1a\" xref=\"S2.T1.20.16.16.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.20.16.16.2.m1.1.1.4\" xref=\"S2.T1.20.16.16.2.m1.1.1.4.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.20.16.16.2.m1.1b\"><apply id=\"S2.T1.20.16.16.2.m1.1.1.cmml\" xref=\"S2.T1.20.16.16.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.20.16.16.2.m1.1.1.1.cmml\" xref=\"S2.T1.20.16.16.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.20.16.16.2.m1.1.1.2.cmml\" xref=\"S2.T1.20.16.16.2.m1.1.1.2\">10.0</cn><cn type=\"float\" id=\"S2.T1.20.16.16.2.m1.1.1.3.cmml\" xref=\"S2.T1.20.16.16.2.m1.1.1.3\">1.6</cn><cn type=\"float\" id=\"S2.T1.20.16.16.2.m1.1.1.4.cmml\" xref=\"S2.T1.20.16.16.2.m1.1.1.4\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.20.16.16.2.m1.1c\">10.0\\pm 1.6\\pm 0.5</annotation></semantics></math></td>\n",
      "<td id=\"S2.T1.21.17.17.3\" class=\"ltx_td ltx_align_center\"><math id=\"S2.T1.21.17.17.3.m1.1\" class=\"ltx_Math\" alttext=\"-0.09\\pm 0.16\\pm 0.01\" display=\"inline\"><semantics id=\"S2.T1.21.17.17.3.m1.1a\"><mrow id=\"S2.T1.21.17.17.3.m1.1.1\" xref=\"S2.T1.21.17.17.3.m1.1.1.cmml\"><mrow id=\"S2.T1.21.17.17.3.m1.1.1.2\" xref=\"S2.T1.21.17.17.3.m1.1.1.2.cmml\"><mo id=\"S2.T1.21.17.17.3.m1.1.1.2a\" xref=\"S2.T1.21.17.17.3.m1.1.1.2.cmml\">−</mo><mn id=\"S2.T1.21.17.17.3.m1.1.1.2.2\" xref=\"S2.T1.21.17.17.3.m1.1.1.2.2.cmml\">0.09</mn></mrow><mo id=\"S2.T1.21.17.17.3.m1.1.1.1\" xref=\"S2.T1.21.17.17.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.21.17.17.3.m1.1.1.3\" xref=\"S2.T1.21.17.17.3.m1.1.1.3.cmml\">0.16</mn><mo id=\"S2.T1.21.17.17.3.m1.1.1.1a\" xref=\"S2.T1.21.17.17.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.21.17.17.3.m1.1.1.4\" xref=\"S2.T1.21.17.17.3.m1.1.1.4.cmml\">0.01</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.21.17.17.3.m1.1b\"><apply id=\"S2.T1.21.17.17.3.m1.1.1.cmml\" xref=\"S2.T1.21.17.17.3.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.21.17.17.3.m1.1.1.1.cmml\" xref=\"S2.T1.21.17.17.3.m1.1.1.1\">plus-or-minus</csymbol><apply id=\"S2.T1.21.17.17.3.m1.1.1.2.cmml\" xref=\"S2.T1.21.17.17.3.m1.1.1.2\"><minus id=\"S2.T1.21.17.17.3.m1.1.1.2.1.cmml\" xref=\"S2.T1.21.17.17.3.m1.1.1.2\"></minus><cn type=\"float\" id=\"S2.T1.21.17.17.3.m1.1.1.2.2.cmml\" xref=\"S2.T1.21.17.17.3.m1.1.1.2.2\">0.09</cn></apply><cn type=\"float\" id=\"S2.T1.21.17.17.3.m1.1.1.3.cmml\" xref=\"S2.T1.21.17.17.3.m1.1.1.3\">0.16</cn><cn type=\"float\" id=\"S2.T1.21.17.17.3.m1.1.1.4.cmml\" xref=\"S2.T1.21.17.17.3.m1.1.1.4\">0.01</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.21.17.17.3.m1.1c\">-0.09\\pm 0.16\\pm 0.01</annotation></semantics></math></td>\n",
      "</tr>\n",
      "<tr id=\"S2.T1.24.20.20\" class=\"ltx_tr\">\n",
      "<td id=\"S2.T1.24.20.20.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">3.5–5.3</td>\n",
      "<td id=\"S2.T1.22.18.18.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"> <math id=\"S2.T1.22.18.18.1.m1.1\" class=\"ltx_Math\" alttext=\"172.6\\pm 25.7\\pm 6.87\" display=\"inline\"><semantics id=\"S2.T1.22.18.18.1.m1.1a\"><mrow id=\"S2.T1.22.18.18.1.m1.1.1\" xref=\"S2.T1.22.18.18.1.m1.1.1.cmml\"><mn id=\"S2.T1.22.18.18.1.m1.1.1.2\" xref=\"S2.T1.22.18.18.1.m1.1.1.2.cmml\">172.6</mn><mo id=\"S2.T1.22.18.18.1.m1.1.1.1\" xref=\"S2.T1.22.18.18.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.22.18.18.1.m1.1.1.3\" xref=\"S2.T1.22.18.18.1.m1.1.1.3.cmml\">25.7</mn><mo id=\"S2.T1.22.18.18.1.m1.1.1.1a\" xref=\"S2.T1.22.18.18.1.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.22.18.18.1.m1.1.1.4\" xref=\"S2.T1.22.18.18.1.m1.1.1.4.cmml\">6.87</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.22.18.18.1.m1.1b\"><apply id=\"S2.T1.22.18.18.1.m1.1.1.cmml\" xref=\"S2.T1.22.18.18.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.22.18.18.1.m1.1.1.1.cmml\" xref=\"S2.T1.22.18.18.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.22.18.18.1.m1.1.1.2.cmml\" xref=\"S2.T1.22.18.18.1.m1.1.1.2\">172.6</cn><cn type=\"float\" id=\"S2.T1.22.18.18.1.m1.1.1.3.cmml\" xref=\"S2.T1.22.18.18.1.m1.1.1.3\">25.7</cn><cn type=\"float\" id=\"S2.T1.22.18.18.1.m1.1.1.4.cmml\" xref=\"S2.T1.22.18.18.1.m1.1.1.4\">6.87</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.22.18.18.1.m1.1c\">172.6\\pm 25.7\\pm 6.87</annotation></semantics></math>\n",
      "</td>\n",
      "<td id=\"S2.T1.24.20.20.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">16.3</td>\n",
      "<td id=\"S2.T1.23.19.19.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">  <math id=\"S2.T1.23.19.19.2.m1.1\" class=\"ltx_Math\" alttext=\"8.1\\pm 1.2\\pm 0.5\" display=\"inline\"><semantics id=\"S2.T1.23.19.19.2.m1.1a\"><mrow id=\"S2.T1.23.19.19.2.m1.1.1\" xref=\"S2.T1.23.19.19.2.m1.1.1.cmml\"><mn id=\"S2.T1.23.19.19.2.m1.1.1.2\" xref=\"S2.T1.23.19.19.2.m1.1.1.2.cmml\">8.1</mn><mo id=\"S2.T1.23.19.19.2.m1.1.1.1\" xref=\"S2.T1.23.19.19.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.23.19.19.2.m1.1.1.3\" xref=\"S2.T1.23.19.19.2.m1.1.1.3.cmml\">1.2</mn><mo id=\"S2.T1.23.19.19.2.m1.1.1.1a\" xref=\"S2.T1.23.19.19.2.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.23.19.19.2.m1.1.1.4\" xref=\"S2.T1.23.19.19.2.m1.1.1.4.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.23.19.19.2.m1.1b\"><apply id=\"S2.T1.23.19.19.2.m1.1.1.cmml\" xref=\"S2.T1.23.19.19.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.23.19.19.2.m1.1.1.1.cmml\" xref=\"S2.T1.23.19.19.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S2.T1.23.19.19.2.m1.1.1.2.cmml\" xref=\"S2.T1.23.19.19.2.m1.1.1.2\">8.1</cn><cn type=\"float\" id=\"S2.T1.23.19.19.2.m1.1.1.3.cmml\" xref=\"S2.T1.23.19.19.2.m1.1.1.3\">1.2</cn><cn type=\"float\" id=\"S2.T1.23.19.19.2.m1.1.1.4.cmml\" xref=\"S2.T1.23.19.19.2.m1.1.1.4\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.23.19.19.2.m1.1c\">8.1\\pm 1.2\\pm 0.5</annotation></semantics></math>\n",
      "</td>\n",
      "<td id=\"S2.T1.24.20.20.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"S2.T1.24.20.20.3.m1.1\" class=\"ltx_Math\" alttext=\"-0.05\\pm 0.15\\pm 0.00\" display=\"inline\"><semantics id=\"S2.T1.24.20.20.3.m1.1a\"><mrow id=\"S2.T1.24.20.20.3.m1.1.1\" xref=\"S2.T1.24.20.20.3.m1.1.1.cmml\"><mrow id=\"S2.T1.24.20.20.3.m1.1.1.2\" xref=\"S2.T1.24.20.20.3.m1.1.1.2.cmml\"><mo id=\"S2.T1.24.20.20.3.m1.1.1.2a\" xref=\"S2.T1.24.20.20.3.m1.1.1.2.cmml\">−</mo><mn id=\"S2.T1.24.20.20.3.m1.1.1.2.2\" xref=\"S2.T1.24.20.20.3.m1.1.1.2.2.cmml\">0.05</mn></mrow><mo id=\"S2.T1.24.20.20.3.m1.1.1.1\" xref=\"S2.T1.24.20.20.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.24.20.20.3.m1.1.1.3\" xref=\"S2.T1.24.20.20.3.m1.1.1.3.cmml\">0.15</mn><mo id=\"S2.T1.24.20.20.3.m1.1.1.1a\" xref=\"S2.T1.24.20.20.3.m1.1.1.1.cmml\">±</mo><mn id=\"S2.T1.24.20.20.3.m1.1.1.4\" xref=\"S2.T1.24.20.20.3.m1.1.1.4.cmml\">0.00</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.24.20.20.3.m1.1b\"><apply id=\"S2.T1.24.20.20.3.m1.1.1.cmml\" xref=\"S2.T1.24.20.20.3.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T1.24.20.20.3.m1.1.1.1.cmml\" xref=\"S2.T1.24.20.20.3.m1.1.1.1\">plus-or-minus</csymbol><apply id=\"S2.T1.24.20.20.3.m1.1.1.2.cmml\" xref=\"S2.T1.24.20.20.3.m1.1.1.2\"><minus id=\"S2.T1.24.20.20.3.m1.1.1.2.1.cmml\" xref=\"S2.T1.24.20.20.3.m1.1.1.2\"></minus><cn type=\"float\" id=\"S2.T1.24.20.20.3.m1.1.1.2.2.cmml\" xref=\"S2.T1.24.20.20.3.m1.1.1.2.2\">0.05</cn></apply><cn type=\"float\" id=\"S2.T1.24.20.20.3.m1.1.1.3.cmml\" xref=\"S2.T1.24.20.20.3.m1.1.1.3\">0.15</cn><cn type=\"float\" id=\"S2.T1.24.20.20.3.m1.1.1.4.cmml\" xref=\"S2.T1.24.20.20.3.m1.1.1.4\">0.00</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.24.20.20.3.m1.1c\">-0.05\\pm 0.15\\pm 0.00</annotation></semantics></math></td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "</figure>\n",
      "<figure id=\"S2.F2\" class=\"ltx_figure\">\n",
      "<div class=\"ltx_flex_figure\">\n",
      "<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/1710.02962/assets/x3.png\" id=\"S2.F2.1.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\" width=\"109\" height=\"105\" alt=\"Refer to caption\"></div>\n",
      "<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/1710.02962/assets/x4.png\" id=\"S2.F2.2.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\" width=\"109\" height=\"106\" alt=\"Refer to caption\"></div>\n",
      "</div>\n",
      "<figcaption class=\"ltx_caption\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Measured differential branching fractions (left) and <math id=\"S2.F2.6.m1.1\" class=\"ltx_Math\" alttext=\"A_{CP}\" display=\"inline\"><semantics id=\"S2.F2.6.m1.1b\"><msub id=\"S2.F2.6.m1.1.1\" xref=\"S2.F2.6.m1.1.1.cmml\"><mi id=\"S2.F2.6.m1.1.1.2\" xref=\"S2.F2.6.m1.1.1.2.cmml\">A</mi><mrow id=\"S2.F2.6.m1.1.1.3\" xref=\"S2.F2.6.m1.1.1.3.cmml\"><mi id=\"S2.F2.6.m1.1.1.3.2\" xref=\"S2.F2.6.m1.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.F2.6.m1.1.1.3.1\" xref=\"S2.F2.6.m1.1.1.3.1.cmml\">​</mo><mi id=\"S2.F2.6.m1.1.1.3.3\" xref=\"S2.F2.6.m1.1.1.3.3.cmml\">P</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.F2.6.m1.1c\"><apply id=\"S2.F2.6.m1.1.1.cmml\" xref=\"S2.F2.6.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.F2.6.m1.1.1.1.cmml\" xref=\"S2.F2.6.m1.1.1\">subscript</csymbol><ci id=\"S2.F2.6.m1.1.1.2.cmml\" xref=\"S2.F2.6.m1.1.1.2\">𝐴</ci><apply id=\"S2.F2.6.m1.1.1.3.cmml\" xref=\"S2.F2.6.m1.1.1.3\"><times id=\"S2.F2.6.m1.1.1.3.1.cmml\" xref=\"S2.F2.6.m1.1.1.3.1\"></times><ci id=\"S2.F2.6.m1.1.1.3.2.cmml\" xref=\"S2.F2.6.m1.1.1.3.2\">𝐶</ci><ci id=\"S2.F2.6.m1.1.1.3.3.cmml\" xref=\"S2.F2.6.m1.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.F2.6.m1.1d\">A_{CP}</annotation></semantics></math> (right) as a function of <math id=\"S2.F2.7.m2.1\" class=\"ltx_Math\" alttext=\"M_{K^{+}K^{-}}\" display=\"inline\"><semantics id=\"S2.F2.7.m2.1b\"><msub id=\"S2.F2.7.m2.1.1\" xref=\"S2.F2.7.m2.1.1.cmml\"><mi id=\"S2.F2.7.m2.1.1.2\" xref=\"S2.F2.7.m2.1.1.2.cmml\">M</mi><mrow id=\"S2.F2.7.m2.1.1.3\" xref=\"S2.F2.7.m2.1.1.3.cmml\"><msup id=\"S2.F2.7.m2.1.1.3.2\" xref=\"S2.F2.7.m2.1.1.3.2.cmml\"><mi id=\"S2.F2.7.m2.1.1.3.2.2\" xref=\"S2.F2.7.m2.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.F2.7.m2.1.1.3.2.3\" xref=\"S2.F2.7.m2.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.F2.7.m2.1.1.3.1\" xref=\"S2.F2.7.m2.1.1.3.1.cmml\">​</mo><msup id=\"S2.F2.7.m2.1.1.3.3\" xref=\"S2.F2.7.m2.1.1.3.3.cmml\"><mi id=\"S2.F2.7.m2.1.1.3.3.2\" xref=\"S2.F2.7.m2.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.F2.7.m2.1.1.3.3.3\" xref=\"S2.F2.7.m2.1.1.3.3.3.cmml\">−</mo></msup></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.F2.7.m2.1c\"><apply id=\"S2.F2.7.m2.1.1.cmml\" xref=\"S2.F2.7.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.F2.7.m2.1.1.1.cmml\" xref=\"S2.F2.7.m2.1.1\">subscript</csymbol><ci id=\"S2.F2.7.m2.1.1.2.cmml\" xref=\"S2.F2.7.m2.1.1.2\">𝑀</ci><apply id=\"S2.F2.7.m2.1.1.3.cmml\" xref=\"S2.F2.7.m2.1.1.3\"><times id=\"S2.F2.7.m2.1.1.3.1.cmml\" xref=\"S2.F2.7.m2.1.1.3.1\"></times><apply id=\"S2.F2.7.m2.1.1.3.2.cmml\" xref=\"S2.F2.7.m2.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.F2.7.m2.1.1.3.2.1.cmml\" xref=\"S2.F2.7.m2.1.1.3.2\">superscript</csymbol><ci id=\"S2.F2.7.m2.1.1.3.2.2.cmml\" xref=\"S2.F2.7.m2.1.1.3.2.2\">𝐾</ci><plus id=\"S2.F2.7.m2.1.1.3.2.3.cmml\" xref=\"S2.F2.7.m2.1.1.3.2.3\"></plus></apply><apply id=\"S2.F2.7.m2.1.1.3.3.cmml\" xref=\"S2.F2.7.m2.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.F2.7.m2.1.1.3.3.1.cmml\" xref=\"S2.F2.7.m2.1.1.3.3\">superscript</csymbol><ci id=\"S2.F2.7.m2.1.1.3.3.2.cmml\" xref=\"S2.F2.7.m2.1.1.3.3.2\">𝐾</ci><minus id=\"S2.F2.7.m2.1.1.3.3.3.cmml\" xref=\"S2.F2.7.m2.1.1.3.3.3\"></minus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.F2.7.m2.1d\">M_{K^{+}K^{-}}</annotation></semantics></math>. Each point is obtained from a two-dimensional fit with systematic uncertainty included. Red squares with error bars in the left plot show the expected signal distribution for a three-body phase space MC sample. Note that the phase space hypothesis is rescaled to the experimentally observed total <math id=\"S2.F2.8.m3.1\" class=\"ltx_Math\" alttext=\"B^{+}\\rightarrow K^{+}K^{-}\\pi^{+}\" display=\"inline\"><semantics id=\"S2.F2.8.m3.1b\"><mrow id=\"S2.F2.8.m3.1.1\" xref=\"S2.F2.8.m3.1.1.cmml\"><msup id=\"S2.F2.8.m3.1.1.2\" xref=\"S2.F2.8.m3.1.1.2.cmml\"><mi id=\"S2.F2.8.m3.1.1.2.2\" xref=\"S2.F2.8.m3.1.1.2.2.cmml\">B</mi><mo id=\"S2.F2.8.m3.1.1.2.3\" xref=\"S2.F2.8.m3.1.1.2.3.cmml\">+</mo></msup><mo stretchy=\"false\" id=\"S2.F2.8.m3.1.1.1\" xref=\"S2.F2.8.m3.1.1.1.cmml\">→</mo><mrow id=\"S2.F2.8.m3.1.1.3\" xref=\"S2.F2.8.m3.1.1.3.cmml\"><msup id=\"S2.F2.8.m3.1.1.3.2\" xref=\"S2.F2.8.m3.1.1.3.2.cmml\"><mi id=\"S2.F2.8.m3.1.1.3.2.2\" xref=\"S2.F2.8.m3.1.1.3.2.2.cmml\">K</mi><mo id=\"S2.F2.8.m3.1.1.3.2.3\" xref=\"S2.F2.8.m3.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.F2.8.m3.1.1.3.1\" xref=\"S2.F2.8.m3.1.1.3.1.cmml\">​</mo><msup id=\"S2.F2.8.m3.1.1.3.3\" xref=\"S2.F2.8.m3.1.1.3.3.cmml\"><mi id=\"S2.F2.8.m3.1.1.3.3.2\" xref=\"S2.F2.8.m3.1.1.3.3.2.cmml\">K</mi><mo id=\"S2.F2.8.m3.1.1.3.3.3\" xref=\"S2.F2.8.m3.1.1.3.3.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.F2.8.m3.1.1.3.1b\" xref=\"S2.F2.8.m3.1.1.3.1.cmml\">​</mo><msup id=\"S2.F2.8.m3.1.1.3.4\" xref=\"S2.F2.8.m3.1.1.3.4.cmml\"><mi id=\"S2.F2.8.m3.1.1.3.4.2\" xref=\"S2.F2.8.m3.1.1.3.4.2.cmml\">π</mi><mo id=\"S2.F2.8.m3.1.1.3.4.3\" xref=\"S2.F2.8.m3.1.1.3.4.3.cmml\">+</mo></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.F2.8.m3.1c\"><apply id=\"S2.F2.8.m3.1.1.cmml\" xref=\"S2.F2.8.m3.1.1\"><ci id=\"S2.F2.8.m3.1.1.1.cmml\" xref=\"S2.F2.8.m3.1.1.1\">→</ci><apply id=\"S2.F2.8.m3.1.1.2.cmml\" xref=\"S2.F2.8.m3.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.F2.8.m3.1.1.2.1.cmml\" xref=\"S2.F2.8.m3.1.1.2\">superscript</csymbol><ci id=\"S2.F2.8.m3.1.1.2.2.cmml\" xref=\"S2.F2.8.m3.1.1.2.2\">𝐵</ci><plus id=\"S2.F2.8.m3.1.1.2.3.cmml\" xref=\"S2.F2.8.m3.1.1.2.3\"></plus></apply><apply id=\"S2.F2.8.m3.1.1.3.cmml\" xref=\"S2.F2.8.m3.1.1.3\"><times id=\"S2.F2.8.m3.1.1.3.1.cmml\" xref=\"S2.F2.8.m3.1.1.3.1\"></times><apply id=\"S2.F2.8.m3.1.1.3.2.cmml\" xref=\"S2.F2.8.m3.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.F2.8.m3.1.1.3.2.1.cmml\" xref=\"S2.F2.8.m3.1.1.3.2\">superscript</csymbol><ci id=\"S2.F2.8.m3.1.1.3.2.2.cmml\" xref=\"S2.F2.8.m3.1.1.3.2.2\">𝐾</ci><plus id=\"S2.F2.8.m3.1.1.3.2.3.cmml\" xref=\"S2.F2.8.m3.1.1.3.2.3\"></plus></apply><apply id=\"S2.F2.8.m3.1.1.3.3.cmml\" xref=\"S2.F2.8.m3.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.F2.8.m3.1.1.3.3.1.cmml\" xref=\"S2.F2.8.m3.1.1.3.3\">superscript</csymbol><ci id=\"S2.F2.8.m3.1.1.3.3.2.cmml\" xref=\"S2.F2.8.m3.1.1.3.3.2\">𝐾</ci><minus id=\"S2.F2.8.m3.1.1.3.3.3.cmml\" xref=\"S2.F2.8.m3.1.1.3.3.3\"></minus></apply><apply id=\"S2.F2.8.m3.1.1.3.4.cmml\" xref=\"S2.F2.8.m3.1.1.3.4\"><csymbol cd=\"ambiguous\" id=\"S2.F2.8.m3.1.1.3.4.1.cmml\" xref=\"S2.F2.8.m3.1.1.3.4\">superscript</csymbol><ci id=\"S2.F2.8.m3.1.1.3.4.2.cmml\" xref=\"S2.F2.8.m3.1.1.3.4.2\">𝜋</ci><plus id=\"S2.F2.8.m3.1.1.3.4.3.cmml\" xref=\"S2.F2.8.m3.1.1.3.4.3\"></plus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.F2.8.m3.1d\">B^{+}\\rightarrow K^{+}K^{-}\\pi^{+}</annotation></semantics></math> signal yield.\n",
      "</figcaption>\n",
      "</figure>\n",
      "</section>\n",
      "<section id=\"S3\" class=\"ltx_section\">\n",
      "<h2 class=\"ltx_title ltx_title_section\">\n",
      "<span class=\"ltx_tag ltx_tag_section\">3 </span><math id=\"S3.1.m1.1\" class=\"ltx_Math\" alttext=\"B^{0}\\rightarrow\\pi^{0}\\pi^{0}\" display=\"inline\"><semantics id=\"S3.1.m1.1b\"><mrow id=\"S3.1.m1.1.1\" xref=\"S3.1.m1.1.1.cmml\"><msup id=\"S3.1.m1.1.1.2\" xref=\"S3.1.m1.1.1.2.cmml\"><mi id=\"S3.1.m1.1.1.2.2\" xref=\"S3.1.m1.1.1.2.2.cmml\">B</mi><mn id=\"S3.1.m1.1.1.2.3\" xref=\"S3.1.m1.1.1.2.3.cmml\">0</mn></msup><mo stretchy=\"false\" id=\"S3.1.m1.1.1.1\" xref=\"S3.1.m1.1.1.1.cmml\">→</mo><mrow id=\"S3.1.m1.1.1.3\" xref=\"S3.1.m1.1.1.3.cmml\"><msup id=\"S3.1.m1.1.1.3.2\" xref=\"S3.1.m1.1.1.3.2.cmml\"><mi id=\"S3.1.m1.1.1.3.2.2\" xref=\"S3.1.m1.1.1.3.2.2.cmml\">π</mi><mn id=\"S3.1.m1.1.1.3.2.3\" xref=\"S3.1.m1.1.1.3.2.3.cmml\">0</mn></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.1.m1.1.1.3.1\" xref=\"S3.1.m1.1.1.3.1.cmml\">​</mo><msup id=\"S3.1.m1.1.1.3.3\" xref=\"S3.1.m1.1.1.3.3.cmml\"><mi id=\"S3.1.m1.1.1.3.3.2\" xref=\"S3.1.m1.1.1.3.3.2.cmml\">π</mi><mn id=\"S3.1.m1.1.1.3.3.3\" xref=\"S3.1.m1.1.1.3.3.3.cmml\">0</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.1.m1.1c\"><apply id=\"S3.1.m1.1.1.cmml\" xref=\"S3.1.m1.1.1\"><ci id=\"S3.1.m1.1.1.1.cmml\" xref=\"S3.1.m1.1.1.1\">→</ci><apply id=\"S3.1.m1.1.1.2.cmml\" xref=\"S3.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.1.m1.1.1.2.1.cmml\" xref=\"S3.1.m1.1.1.2\">superscript</csymbol><ci id=\"S3.1.m1.1.1.2.2.cmml\" xref=\"S3.1.m1.1.1.2.2\">𝐵</ci><cn type=\"integer\" id=\"S3.1.m1.1.1.2.3.cmml\" xref=\"S3.1.m1.1.1.2.3\">0</cn></apply><apply id=\"S3.1.m1.1.1.3.cmml\" xref=\"S3.1.m1.1.1.3\"><times id=\"S3.1.m1.1.1.3.1.cmml\" xref=\"S3.1.m1.1.1.3.1\"></times><apply id=\"S3.1.m1.1.1.3.2.cmml\" xref=\"S3.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.1.m1.1.1.3.2.1.cmml\" xref=\"S3.1.m1.1.1.3.2\">superscript</csymbol><ci id=\"S3.1.m1.1.1.3.2.2.cmml\" xref=\"S3.1.m1.1.1.3.2.2\">𝜋</ci><cn type=\"integer\" id=\"S3.1.m1.1.1.3.2.3.cmml\" xref=\"S3.1.m1.1.1.3.2.3\">0</cn></apply><apply id=\"S3.1.m1.1.1.3.3.cmml\" xref=\"S3.1.m1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.1.m1.1.1.3.3.1.cmml\" xref=\"S3.1.m1.1.1.3.3\">superscript</csymbol><ci id=\"S3.1.m1.1.1.3.3.2.cmml\" xref=\"S3.1.m1.1.1.3.3.2\">𝜋</ci><cn type=\"integer\" id=\"S3.1.m1.1.1.3.3.3.cmml\" xref=\"S3.1.m1.1.1.3.3.3\">0</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.1.m1.1d\">B^{0}\\rightarrow\\pi^{0}\\pi^{0}</annotation></semantics></math> decay</h2>\n",
      "\n",
      "<div id=\"S3.p1\" class=\"ltx_para\">\n",
      "<p id=\"S3.p1.24\" class=\"ltx_p\">One of the proposed techniques to measure <math id=\"S3.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\phi_{2}\" display=\"inline\"><semantics id=\"S3.p1.1.m1.1a\"><msub id=\"S3.p1.1.m1.1.1\" xref=\"S3.p1.1.m1.1.1.cmml\"><mi id=\"S3.p1.1.m1.1.1.2\" xref=\"S3.p1.1.m1.1.1.2.cmml\">ϕ</mi><mn id=\"S3.p1.1.m1.1.1.3\" xref=\"S3.p1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.1.m1.1b\"><apply id=\"S3.p1.1.m1.1.1.cmml\" xref=\"S3.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p1.1.m1.1.1.1.cmml\" xref=\"S3.p1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.p1.1.m1.1.1.2.cmml\" xref=\"S3.p1.1.m1.1.1.2\">italic-ϕ</ci><cn type=\"integer\" id=\"S3.p1.1.m1.1.1.3.cmml\" xref=\"S3.p1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.1.m1.1c\">\\phi_{2}</annotation></semantics></math> is to perform an isospin analysis of the entire <math id=\"S3.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"B\\rightarrow\\pi\\pi\" display=\"inline\"><semantics id=\"S3.p1.2.m2.1a\"><mrow id=\"S3.p1.2.m2.1.1\" xref=\"S3.p1.2.m2.1.1.cmml\"><mi id=\"S3.p1.2.m2.1.1.2\" xref=\"S3.p1.2.m2.1.1.2.cmml\">B</mi><mo stretchy=\"false\" id=\"S3.p1.2.m2.1.1.1\" xref=\"S3.p1.2.m2.1.1.1.cmml\">→</mo><mrow id=\"S3.p1.2.m2.1.1.3\" xref=\"S3.p1.2.m2.1.1.3.cmml\"><mi id=\"S3.p1.2.m2.1.1.3.2\" xref=\"S3.p1.2.m2.1.1.3.2.cmml\">π</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.2.m2.1.1.3.1\" xref=\"S3.p1.2.m2.1.1.3.1.cmml\">​</mo><mi id=\"S3.p1.2.m2.1.1.3.3\" xref=\"S3.p1.2.m2.1.1.3.3.cmml\">π</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.2.m2.1b\"><apply id=\"S3.p1.2.m2.1.1.cmml\" xref=\"S3.p1.2.m2.1.1\"><ci id=\"S3.p1.2.m2.1.1.1.cmml\" xref=\"S3.p1.2.m2.1.1.1\">→</ci><ci id=\"S3.p1.2.m2.1.1.2.cmml\" xref=\"S3.p1.2.m2.1.1.2\">𝐵</ci><apply id=\"S3.p1.2.m2.1.1.3.cmml\" xref=\"S3.p1.2.m2.1.1.3\"><times id=\"S3.p1.2.m2.1.1.3.1.cmml\" xref=\"S3.p1.2.m2.1.1.3.1\"></times><ci id=\"S3.p1.2.m2.1.1.3.2.cmml\" xref=\"S3.p1.2.m2.1.1.3.2\">𝜋</ci><ci id=\"S3.p1.2.m2.1.1.3.3.cmml\" xref=\"S3.p1.2.m2.1.1.3.3\">𝜋</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.2.m2.1c\">B\\rightarrow\\pi\\pi</annotation></semantics></math> system [11]. This requires measurements of <math id=\"S3.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"\\mathcal{B}\" display=\"inline\"><semantics id=\"S3.p1.3.m3.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S3.p1.3.m3.1.1\" xref=\"S3.p1.3.m3.1.1.cmml\">ℬ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.3.m3.1b\"><ci id=\"S3.p1.3.m3.1.1.cmml\" xref=\"S3.p1.3.m3.1.1\">ℬ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.3.m3.1c\">\\mathcal{B}</annotation></semantics></math> and <math id=\"S3.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"A_{CP}\" display=\"inline\"><semantics id=\"S3.p1.4.m4.1a\"><msub id=\"S3.p1.4.m4.1.1\" xref=\"S3.p1.4.m4.1.1.cmml\"><mi id=\"S3.p1.4.m4.1.1.2\" xref=\"S3.p1.4.m4.1.1.2.cmml\">A</mi><mrow id=\"S3.p1.4.m4.1.1.3\" xref=\"S3.p1.4.m4.1.1.3.cmml\"><mi id=\"S3.p1.4.m4.1.1.3.2\" xref=\"S3.p1.4.m4.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.4.m4.1.1.3.1\" xref=\"S3.p1.4.m4.1.1.3.1.cmml\">​</mo><mi id=\"S3.p1.4.m4.1.1.3.3\" xref=\"S3.p1.4.m4.1.1.3.3.cmml\">P</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.4.m4.1b\"><apply id=\"S3.p1.4.m4.1.1.cmml\" xref=\"S3.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p1.4.m4.1.1.1.cmml\" xref=\"S3.p1.4.m4.1.1\">subscript</csymbol><ci id=\"S3.p1.4.m4.1.1.2.cmml\" xref=\"S3.p1.4.m4.1.1.2\">𝐴</ci><apply id=\"S3.p1.4.m4.1.1.3.cmml\" xref=\"S3.p1.4.m4.1.1.3\"><times id=\"S3.p1.4.m4.1.1.3.1.cmml\" xref=\"S3.p1.4.m4.1.1.3.1\"></times><ci id=\"S3.p1.4.m4.1.1.3.2.cmml\" xref=\"S3.p1.4.m4.1.1.3.2\">𝐶</ci><ci id=\"S3.p1.4.m4.1.1.3.3.cmml\" xref=\"S3.p1.4.m4.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.4.m4.1c\">A_{CP}</annotation></semantics></math> for <math id=\"S3.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"B^{+}\\rightarrow\\pi^{+}\\pi^{0}\" display=\"inline\"><semantics id=\"S3.p1.5.m5.1a\"><mrow id=\"S3.p1.5.m5.1.1\" xref=\"S3.p1.5.m5.1.1.cmml\"><msup id=\"S3.p1.5.m5.1.1.2\" xref=\"S3.p1.5.m5.1.1.2.cmml\"><mi id=\"S3.p1.5.m5.1.1.2.2\" xref=\"S3.p1.5.m5.1.1.2.2.cmml\">B</mi><mo id=\"S3.p1.5.m5.1.1.2.3\" xref=\"S3.p1.5.m5.1.1.2.3.cmml\">+</mo></msup><mo stretchy=\"false\" id=\"S3.p1.5.m5.1.1.1\" xref=\"S3.p1.5.m5.1.1.1.cmml\">→</mo><mrow id=\"S3.p1.5.m5.1.1.3\" xref=\"S3.p1.5.m5.1.1.3.cmml\"><msup id=\"S3.p1.5.m5.1.1.3.2\" xref=\"S3.p1.5.m5.1.1.3.2.cmml\"><mi id=\"S3.p1.5.m5.1.1.3.2.2\" xref=\"S3.p1.5.m5.1.1.3.2.2.cmml\">π</mi><mo id=\"S3.p1.5.m5.1.1.3.2.3\" xref=\"S3.p1.5.m5.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.5.m5.1.1.3.1\" xref=\"S3.p1.5.m5.1.1.3.1.cmml\">​</mo><msup id=\"S3.p1.5.m5.1.1.3.3\" xref=\"S3.p1.5.m5.1.1.3.3.cmml\"><mi id=\"S3.p1.5.m5.1.1.3.3.2\" xref=\"S3.p1.5.m5.1.1.3.3.2.cmml\">π</mi><mn id=\"S3.p1.5.m5.1.1.3.3.3\" xref=\"S3.p1.5.m5.1.1.3.3.3.cmml\">0</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.5.m5.1b\"><apply id=\"S3.p1.5.m5.1.1.cmml\" xref=\"S3.p1.5.m5.1.1\"><ci id=\"S3.p1.5.m5.1.1.1.cmml\" xref=\"S3.p1.5.m5.1.1.1\">→</ci><apply id=\"S3.p1.5.m5.1.1.2.cmml\" xref=\"S3.p1.5.m5.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.p1.5.m5.1.1.2.1.cmml\" xref=\"S3.p1.5.m5.1.1.2\">superscript</csymbol><ci id=\"S3.p1.5.m5.1.1.2.2.cmml\" xref=\"S3.p1.5.m5.1.1.2.2\">𝐵</ci><plus id=\"S3.p1.5.m5.1.1.2.3.cmml\" xref=\"S3.p1.5.m5.1.1.2.3\"></plus></apply><apply id=\"S3.p1.5.m5.1.1.3.cmml\" xref=\"S3.p1.5.m5.1.1.3\"><times id=\"S3.p1.5.m5.1.1.3.1.cmml\" xref=\"S3.p1.5.m5.1.1.3.1\"></times><apply id=\"S3.p1.5.m5.1.1.3.2.cmml\" xref=\"S3.p1.5.m5.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.p1.5.m5.1.1.3.2.1.cmml\" xref=\"S3.p1.5.m5.1.1.3.2\">superscript</csymbol><ci id=\"S3.p1.5.m5.1.1.3.2.2.cmml\" xref=\"S3.p1.5.m5.1.1.3.2.2\">𝜋</ci><plus id=\"S3.p1.5.m5.1.1.3.2.3.cmml\" xref=\"S3.p1.5.m5.1.1.3.2.3\"></plus></apply><apply id=\"S3.p1.5.m5.1.1.3.3.cmml\" xref=\"S3.p1.5.m5.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.p1.5.m5.1.1.3.3.1.cmml\" xref=\"S3.p1.5.m5.1.1.3.3\">superscript</csymbol><ci id=\"S3.p1.5.m5.1.1.3.3.2.cmml\" xref=\"S3.p1.5.m5.1.1.3.3.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p1.5.m5.1.1.3.3.3.cmml\" xref=\"S3.p1.5.m5.1.1.3.3.3\">0</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.5.m5.1c\">B^{+}\\rightarrow\\pi^{+}\\pi^{0}</annotation></semantics></math> and <math id=\"S3.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"B^{0}\\rightarrow\\pi^{0}\\pi^{0}\" display=\"inline\"><semantics id=\"S3.p1.6.m6.1a\"><mrow id=\"S3.p1.6.m6.1.1\" xref=\"S3.p1.6.m6.1.1.cmml\"><msup id=\"S3.p1.6.m6.1.1.2\" xref=\"S3.p1.6.m6.1.1.2.cmml\"><mi id=\"S3.p1.6.m6.1.1.2.2\" xref=\"S3.p1.6.m6.1.1.2.2.cmml\">B</mi><mn id=\"S3.p1.6.m6.1.1.2.3\" xref=\"S3.p1.6.m6.1.1.2.3.cmml\">0</mn></msup><mo stretchy=\"false\" id=\"S3.p1.6.m6.1.1.1\" xref=\"S3.p1.6.m6.1.1.1.cmml\">→</mo><mrow id=\"S3.p1.6.m6.1.1.3\" xref=\"S3.p1.6.m6.1.1.3.cmml\"><msup id=\"S3.p1.6.m6.1.1.3.2\" xref=\"S3.p1.6.m6.1.1.3.2.cmml\"><mi id=\"S3.p1.6.m6.1.1.3.2.2\" xref=\"S3.p1.6.m6.1.1.3.2.2.cmml\">π</mi><mn id=\"S3.p1.6.m6.1.1.3.2.3\" xref=\"S3.p1.6.m6.1.1.3.2.3.cmml\">0</mn></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.6.m6.1.1.3.1\" xref=\"S3.p1.6.m6.1.1.3.1.cmml\">​</mo><msup id=\"S3.p1.6.m6.1.1.3.3\" xref=\"S3.p1.6.m6.1.1.3.3.cmml\"><mi id=\"S3.p1.6.m6.1.1.3.3.2\" xref=\"S3.p1.6.m6.1.1.3.3.2.cmml\">π</mi><mn id=\"S3.p1.6.m6.1.1.3.3.3\" xref=\"S3.p1.6.m6.1.1.3.3.3.cmml\">0</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.6.m6.1b\"><apply id=\"S3.p1.6.m6.1.1.cmml\" xref=\"S3.p1.6.m6.1.1\"><ci id=\"S3.p1.6.m6.1.1.1.cmml\" xref=\"S3.p1.6.m6.1.1.1\">→</ci><apply id=\"S3.p1.6.m6.1.1.2.cmml\" xref=\"S3.p1.6.m6.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.p1.6.m6.1.1.2.1.cmml\" xref=\"S3.p1.6.m6.1.1.2\">superscript</csymbol><ci id=\"S3.p1.6.m6.1.1.2.2.cmml\" xref=\"S3.p1.6.m6.1.1.2.2\">𝐵</ci><cn type=\"integer\" id=\"S3.p1.6.m6.1.1.2.3.cmml\" xref=\"S3.p1.6.m6.1.1.2.3\">0</cn></apply><apply id=\"S3.p1.6.m6.1.1.3.cmml\" xref=\"S3.p1.6.m6.1.1.3\"><times id=\"S3.p1.6.m6.1.1.3.1.cmml\" xref=\"S3.p1.6.m6.1.1.3.1\"></times><apply id=\"S3.p1.6.m6.1.1.3.2.cmml\" xref=\"S3.p1.6.m6.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.p1.6.m6.1.1.3.2.1.cmml\" xref=\"S3.p1.6.m6.1.1.3.2\">superscript</csymbol><ci id=\"S3.p1.6.m6.1.1.3.2.2.cmml\" xref=\"S3.p1.6.m6.1.1.3.2.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p1.6.m6.1.1.3.2.3.cmml\" xref=\"S3.p1.6.m6.1.1.3.2.3\">0</cn></apply><apply id=\"S3.p1.6.m6.1.1.3.3.cmml\" xref=\"S3.p1.6.m6.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.p1.6.m6.1.1.3.3.1.cmml\" xref=\"S3.p1.6.m6.1.1.3.3\">superscript</csymbol><ci id=\"S3.p1.6.m6.1.1.3.3.2.cmml\" xref=\"S3.p1.6.m6.1.1.3.3.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p1.6.m6.1.1.3.3.3.cmml\" xref=\"S3.p1.6.m6.1.1.3.3.3\">0</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.6.m6.1c\">B^{0}\\rightarrow\\pi^{0}\\pi^{0}</annotation></semantics></math> decays, along with that of <math id=\"S3.p1.7.m7.1\" class=\"ltx_Math\" alttext=\"\\mathcal{B}\" display=\"inline\"><semantics id=\"S3.p1.7.m7.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S3.p1.7.m7.1.1\" xref=\"S3.p1.7.m7.1.1.cmml\">ℬ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.7.m7.1b\"><ci id=\"S3.p1.7.m7.1.1.cmml\" xref=\"S3.p1.7.m7.1.1\">ℬ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.7.m7.1c\">\\mathcal{B}</annotation></semantics></math> and time-dependent <math id=\"S3.p1.8.m8.1\" class=\"ltx_Math\" alttext=\"CP\" display=\"inline\"><semantics id=\"S3.p1.8.m8.1a\"><mrow id=\"S3.p1.8.m8.1.1\" xref=\"S3.p1.8.m8.1.1.cmml\"><mi id=\"S3.p1.8.m8.1.1.2\" xref=\"S3.p1.8.m8.1.1.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.8.m8.1.1.1\" xref=\"S3.p1.8.m8.1.1.1.cmml\">​</mo><mi id=\"S3.p1.8.m8.1.1.3\" xref=\"S3.p1.8.m8.1.1.3.cmml\">P</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.8.m8.1b\"><apply id=\"S3.p1.8.m8.1.1.cmml\" xref=\"S3.p1.8.m8.1.1\"><times id=\"S3.p1.8.m8.1.1.1.cmml\" xref=\"S3.p1.8.m8.1.1.1\"></times><ci id=\"S3.p1.8.m8.1.1.2.cmml\" xref=\"S3.p1.8.m8.1.1.2\">𝐶</ci><ci id=\"S3.p1.8.m8.1.1.3.cmml\" xref=\"S3.p1.8.m8.1.1.3\">𝑃</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.8.m8.1c\">CP</annotation></semantics></math> asymmetry for the <math id=\"S3.p1.9.m9.1\" class=\"ltx_Math\" alttext=\"B^{0}\\rightarrow\\pi^{+}\\pi^{-}\" display=\"inline\"><semantics id=\"S3.p1.9.m9.1a\"><mrow id=\"S3.p1.9.m9.1.1\" xref=\"S3.p1.9.m9.1.1.cmml\"><msup id=\"S3.p1.9.m9.1.1.2\" xref=\"S3.p1.9.m9.1.1.2.cmml\"><mi id=\"S3.p1.9.m9.1.1.2.2\" xref=\"S3.p1.9.m9.1.1.2.2.cmml\">B</mi><mn id=\"S3.p1.9.m9.1.1.2.3\" xref=\"S3.p1.9.m9.1.1.2.3.cmml\">0</mn></msup><mo stretchy=\"false\" id=\"S3.p1.9.m9.1.1.1\" xref=\"S3.p1.9.m9.1.1.1.cmml\">→</mo><mrow id=\"S3.p1.9.m9.1.1.3\" xref=\"S3.p1.9.m9.1.1.3.cmml\"><msup id=\"S3.p1.9.m9.1.1.3.2\" xref=\"S3.p1.9.m9.1.1.3.2.cmml\"><mi id=\"S3.p1.9.m9.1.1.3.2.2\" xref=\"S3.p1.9.m9.1.1.3.2.2.cmml\">π</mi><mo id=\"S3.p1.9.m9.1.1.3.2.3\" xref=\"S3.p1.9.m9.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.9.m9.1.1.3.1\" xref=\"S3.p1.9.m9.1.1.3.1.cmml\">​</mo><msup id=\"S3.p1.9.m9.1.1.3.3\" xref=\"S3.p1.9.m9.1.1.3.3.cmml\"><mi id=\"S3.p1.9.m9.1.1.3.3.2\" xref=\"S3.p1.9.m9.1.1.3.3.2.cmml\">π</mi><mo id=\"S3.p1.9.m9.1.1.3.3.3\" xref=\"S3.p1.9.m9.1.1.3.3.3.cmml\">−</mo></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.9.m9.1b\"><apply id=\"S3.p1.9.m9.1.1.cmml\" xref=\"S3.p1.9.m9.1.1\"><ci id=\"S3.p1.9.m9.1.1.1.cmml\" xref=\"S3.p1.9.m9.1.1.1\">→</ci><apply id=\"S3.p1.9.m9.1.1.2.cmml\" xref=\"S3.p1.9.m9.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.p1.9.m9.1.1.2.1.cmml\" xref=\"S3.p1.9.m9.1.1.2\">superscript</csymbol><ci id=\"S3.p1.9.m9.1.1.2.2.cmml\" xref=\"S3.p1.9.m9.1.1.2.2\">𝐵</ci><cn type=\"integer\" id=\"S3.p1.9.m9.1.1.2.3.cmml\" xref=\"S3.p1.9.m9.1.1.2.3\">0</cn></apply><apply id=\"S3.p1.9.m9.1.1.3.cmml\" xref=\"S3.p1.9.m9.1.1.3\"><times id=\"S3.p1.9.m9.1.1.3.1.cmml\" xref=\"S3.p1.9.m9.1.1.3.1\"></times><apply id=\"S3.p1.9.m9.1.1.3.2.cmml\" xref=\"S3.p1.9.m9.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.p1.9.m9.1.1.3.2.1.cmml\" xref=\"S3.p1.9.m9.1.1.3.2\">superscript</csymbol><ci id=\"S3.p1.9.m9.1.1.3.2.2.cmml\" xref=\"S3.p1.9.m9.1.1.3.2.2\">𝜋</ci><plus id=\"S3.p1.9.m9.1.1.3.2.3.cmml\" xref=\"S3.p1.9.m9.1.1.3.2.3\"></plus></apply><apply id=\"S3.p1.9.m9.1.1.3.3.cmml\" xref=\"S3.p1.9.m9.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.p1.9.m9.1.1.3.3.1.cmml\" xref=\"S3.p1.9.m9.1.1.3.3\">superscript</csymbol><ci id=\"S3.p1.9.m9.1.1.3.3.2.cmml\" xref=\"S3.p1.9.m9.1.1.3.3.2\">𝜋</ci><minus id=\"S3.p1.9.m9.1.1.3.3.3.cmml\" xref=\"S3.p1.9.m9.1.1.3.3.3\"></minus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.9.m9.1c\">B^{0}\\rightarrow\\pi^{+}\\pi^{-}</annotation></semantics></math> decay. One needs all these observables in order to determine <math id=\"S3.p1.10.m10.1\" class=\"ltx_Math\" alttext=\"\\phi_{2}\" display=\"inline\"><semantics id=\"S3.p1.10.m10.1a\"><msub id=\"S3.p1.10.m10.1.1\" xref=\"S3.p1.10.m10.1.1.cmml\"><mi id=\"S3.p1.10.m10.1.1.2\" xref=\"S3.p1.10.m10.1.1.2.cmml\">ϕ</mi><mn id=\"S3.p1.10.m10.1.1.3\" xref=\"S3.p1.10.m10.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.10.m10.1b\"><apply id=\"S3.p1.10.m10.1.1.cmml\" xref=\"S3.p1.10.m10.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p1.10.m10.1.1.1.cmml\" xref=\"S3.p1.10.m10.1.1\">subscript</csymbol><ci id=\"S3.p1.10.m10.1.1.2.cmml\" xref=\"S3.p1.10.m10.1.1.2\">italic-ϕ</ci><cn type=\"integer\" id=\"S3.p1.10.m10.1.1.3.cmml\" xref=\"S3.p1.10.m10.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.10.m10.1c\">\\phi_{2}</annotation></semantics></math> as electroweak tree and loop processes contribute with different phases to <math id=\"S3.p1.11.m11.1\" class=\"ltx_Math\" alttext=\"B\\rightarrow\\pi\\pi\" display=\"inline\"><semantics id=\"S3.p1.11.m11.1a\"><mrow id=\"S3.p1.11.m11.1.1\" xref=\"S3.p1.11.m11.1.1.cmml\"><mi id=\"S3.p1.11.m11.1.1.2\" xref=\"S3.p1.11.m11.1.1.2.cmml\">B</mi><mo stretchy=\"false\" id=\"S3.p1.11.m11.1.1.1\" xref=\"S3.p1.11.m11.1.1.1.cmml\">→</mo><mrow id=\"S3.p1.11.m11.1.1.3\" xref=\"S3.p1.11.m11.1.1.3.cmml\"><mi id=\"S3.p1.11.m11.1.1.3.2\" xref=\"S3.p1.11.m11.1.1.3.2.cmml\">π</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.11.m11.1.1.3.1\" xref=\"S3.p1.11.m11.1.1.3.1.cmml\">​</mo><mi id=\"S3.p1.11.m11.1.1.3.3\" xref=\"S3.p1.11.m11.1.1.3.3.cmml\">π</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.11.m11.1b\"><apply id=\"S3.p1.11.m11.1.1.cmml\" xref=\"S3.p1.11.m11.1.1\"><ci id=\"S3.p1.11.m11.1.1.1.cmml\" xref=\"S3.p1.11.m11.1.1.1\">→</ci><ci id=\"S3.p1.11.m11.1.1.2.cmml\" xref=\"S3.p1.11.m11.1.1.2\">𝐵</ci><apply id=\"S3.p1.11.m11.1.1.3.cmml\" xref=\"S3.p1.11.m11.1.1.3\"><times id=\"S3.p1.11.m11.1.1.3.1.cmml\" xref=\"S3.p1.11.m11.1.1.3.1\"></times><ci id=\"S3.p1.11.m11.1.1.3.2.cmml\" xref=\"S3.p1.11.m11.1.1.3.2\">𝜋</ci><ci id=\"S3.p1.11.m11.1.1.3.3.cmml\" xref=\"S3.p1.11.m11.1.1.3.3\">𝜋</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.11.m11.1c\">B\\rightarrow\\pi\\pi</annotation></semantics></math> decays. The <math id=\"S3.p1.12.m12.1\" class=\"ltx_Math\" alttext=\"\\mathcal{B}\" display=\"inline\"><semantics id=\"S3.p1.12.m12.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S3.p1.12.m12.1.1\" xref=\"S3.p1.12.m12.1.1.cmml\">ℬ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.12.m12.1b\"><ci id=\"S3.p1.12.m12.1.1.cmml\" xref=\"S3.p1.12.m12.1.1\">ℬ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.12.m12.1c\">\\mathcal{B}</annotation></semantics></math> and <math id=\"S3.p1.13.m13.1\" class=\"ltx_Math\" alttext=\"A_{CP}\" display=\"inline\"><semantics id=\"S3.p1.13.m13.1a\"><msub id=\"S3.p1.13.m13.1.1\" xref=\"S3.p1.13.m13.1.1.cmml\"><mi id=\"S3.p1.13.m13.1.1.2\" xref=\"S3.p1.13.m13.1.1.2.cmml\">A</mi><mrow id=\"S3.p1.13.m13.1.1.3\" xref=\"S3.p1.13.m13.1.1.3.cmml\"><mi id=\"S3.p1.13.m13.1.1.3.2\" xref=\"S3.p1.13.m13.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.13.m13.1.1.3.1\" xref=\"S3.p1.13.m13.1.1.3.1.cmml\">​</mo><mi id=\"S3.p1.13.m13.1.1.3.3\" xref=\"S3.p1.13.m13.1.1.3.3.cmml\">P</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.13.m13.1b\"><apply id=\"S3.p1.13.m13.1.1.cmml\" xref=\"S3.p1.13.m13.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p1.13.m13.1.1.1.cmml\" xref=\"S3.p1.13.m13.1.1\">subscript</csymbol><ci id=\"S3.p1.13.m13.1.1.2.cmml\" xref=\"S3.p1.13.m13.1.1.2\">𝐴</ci><apply id=\"S3.p1.13.m13.1.1.3.cmml\" xref=\"S3.p1.13.m13.1.1.3\"><times id=\"S3.p1.13.m13.1.1.3.1.cmml\" xref=\"S3.p1.13.m13.1.1.3.1\"></times><ci id=\"S3.p1.13.m13.1.1.3.2.cmml\" xref=\"S3.p1.13.m13.1.1.3.2\">𝐶</ci><ci id=\"S3.p1.13.m13.1.1.3.3.cmml\" xref=\"S3.p1.13.m13.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.13.m13.1c\">A_{CP}</annotation></semantics></math> for <math id=\"S3.p1.14.m14.1\" class=\"ltx_Math\" alttext=\"B^{0}\\rightarrow\\pi^{0}\\pi^{0}\" display=\"inline\"><semantics id=\"S3.p1.14.m14.1a\"><mrow id=\"S3.p1.14.m14.1.1\" xref=\"S3.p1.14.m14.1.1.cmml\"><msup id=\"S3.p1.14.m14.1.1.2\" xref=\"S3.p1.14.m14.1.1.2.cmml\"><mi id=\"S3.p1.14.m14.1.1.2.2\" xref=\"S3.p1.14.m14.1.1.2.2.cmml\">B</mi><mn id=\"S3.p1.14.m14.1.1.2.3\" xref=\"S3.p1.14.m14.1.1.2.3.cmml\">0</mn></msup><mo stretchy=\"false\" id=\"S3.p1.14.m14.1.1.1\" xref=\"S3.p1.14.m14.1.1.1.cmml\">→</mo><mrow id=\"S3.p1.14.m14.1.1.3\" xref=\"S3.p1.14.m14.1.1.3.cmml\"><msup id=\"S3.p1.14.m14.1.1.3.2\" xref=\"S3.p1.14.m14.1.1.3.2.cmml\"><mi id=\"S3.p1.14.m14.1.1.3.2.2\" xref=\"S3.p1.14.m14.1.1.3.2.2.cmml\">π</mi><mn id=\"S3.p1.14.m14.1.1.3.2.3\" xref=\"S3.p1.14.m14.1.1.3.2.3.cmml\">0</mn></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.14.m14.1.1.3.1\" xref=\"S3.p1.14.m14.1.1.3.1.cmml\">​</mo><msup id=\"S3.p1.14.m14.1.1.3.3\" xref=\"S3.p1.14.m14.1.1.3.3.cmml\"><mi id=\"S3.p1.14.m14.1.1.3.3.2\" xref=\"S3.p1.14.m14.1.1.3.3.2.cmml\">π</mi><mn id=\"S3.p1.14.m14.1.1.3.3.3\" xref=\"S3.p1.14.m14.1.1.3.3.3.cmml\">0</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.14.m14.1b\"><apply id=\"S3.p1.14.m14.1.1.cmml\" xref=\"S3.p1.14.m14.1.1\"><ci id=\"S3.p1.14.m14.1.1.1.cmml\" xref=\"S3.p1.14.m14.1.1.1\">→</ci><apply id=\"S3.p1.14.m14.1.1.2.cmml\" xref=\"S3.p1.14.m14.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.p1.14.m14.1.1.2.1.cmml\" xref=\"S3.p1.14.m14.1.1.2\">superscript</csymbol><ci id=\"S3.p1.14.m14.1.1.2.2.cmml\" xref=\"S3.p1.14.m14.1.1.2.2\">𝐵</ci><cn type=\"integer\" id=\"S3.p1.14.m14.1.1.2.3.cmml\" xref=\"S3.p1.14.m14.1.1.2.3\">0</cn></apply><apply id=\"S3.p1.14.m14.1.1.3.cmml\" xref=\"S3.p1.14.m14.1.1.3\"><times id=\"S3.p1.14.m14.1.1.3.1.cmml\" xref=\"S3.p1.14.m14.1.1.3.1\"></times><apply id=\"S3.p1.14.m14.1.1.3.2.cmml\" xref=\"S3.p1.14.m14.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.p1.14.m14.1.1.3.2.1.cmml\" xref=\"S3.p1.14.m14.1.1.3.2\">superscript</csymbol><ci id=\"S3.p1.14.m14.1.1.3.2.2.cmml\" xref=\"S3.p1.14.m14.1.1.3.2.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p1.14.m14.1.1.3.2.3.cmml\" xref=\"S3.p1.14.m14.1.1.3.2.3\">0</cn></apply><apply id=\"S3.p1.14.m14.1.1.3.3.cmml\" xref=\"S3.p1.14.m14.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.p1.14.m14.1.1.3.3.1.cmml\" xref=\"S3.p1.14.m14.1.1.3.3\">superscript</csymbol><ci id=\"S3.p1.14.m14.1.1.3.3.2.cmml\" xref=\"S3.p1.14.m14.1.1.3.3.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p1.14.m14.1.1.3.3.3.cmml\" xref=\"S3.p1.14.m14.1.1.3.3.3\">0</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.14.m14.1c\">B^{0}\\rightarrow\\pi^{0}\\pi^{0}</annotation></semantics></math> are the least well determined among the <math id=\"S3.p1.15.m15.1\" class=\"ltx_Math\" alttext=\"B\\rightarrow\\pi\\pi\" display=\"inline\"><semantics id=\"S3.p1.15.m15.1a\"><mrow id=\"S3.p1.15.m15.1.1\" xref=\"S3.p1.15.m15.1.1.cmml\"><mi id=\"S3.p1.15.m15.1.1.2\" xref=\"S3.p1.15.m15.1.1.2.cmml\">B</mi><mo stretchy=\"false\" id=\"S3.p1.15.m15.1.1.1\" xref=\"S3.p1.15.m15.1.1.1.cmml\">→</mo><mrow id=\"S3.p1.15.m15.1.1.3\" xref=\"S3.p1.15.m15.1.1.3.cmml\"><mi id=\"S3.p1.15.m15.1.1.3.2\" xref=\"S3.p1.15.m15.1.1.3.2.cmml\">π</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.15.m15.1.1.3.1\" xref=\"S3.p1.15.m15.1.1.3.1.cmml\">​</mo><mi id=\"S3.p1.15.m15.1.1.3.3\" xref=\"S3.p1.15.m15.1.1.3.3.cmml\">π</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.15.m15.1b\"><apply id=\"S3.p1.15.m15.1.1.cmml\" xref=\"S3.p1.15.m15.1.1\"><ci id=\"S3.p1.15.m15.1.1.1.cmml\" xref=\"S3.p1.15.m15.1.1.1\">→</ci><ci id=\"S3.p1.15.m15.1.1.2.cmml\" xref=\"S3.p1.15.m15.1.1.2\">𝐵</ci><apply id=\"S3.p1.15.m15.1.1.3.cmml\" xref=\"S3.p1.15.m15.1.1.3\"><times id=\"S3.p1.15.m15.1.1.3.1.cmml\" xref=\"S3.p1.15.m15.1.1.3.1\"></times><ci id=\"S3.p1.15.m15.1.1.3.2.cmml\" xref=\"S3.p1.15.m15.1.1.3.2\">𝜋</ci><ci id=\"S3.p1.15.m15.1.1.3.3.cmml\" xref=\"S3.p1.15.m15.1.1.3.3\">𝜋</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.15.m15.1c\">B\\rightarrow\\pi\\pi</annotation></semantics></math> decays. This decay is also important to probe the disagreement between quantum-chromodynamics-based factorization, which predicts <math id=\"S3.p1.16.m16.1\" class=\"ltx_Math\" alttext=\"\\mathcal{B}\" display=\"inline\"><semantics id=\"S3.p1.16.m16.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S3.p1.16.m16.1.1\" xref=\"S3.p1.16.m16.1.1.cmml\">ℬ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.16.m16.1b\"><ci id=\"S3.p1.16.m16.1.1.cmml\" xref=\"S3.p1.16.m16.1.1\">ℬ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.16.m16.1c\">\\mathcal{B}</annotation></semantics></math> below <math id=\"S3.p1.17.m17.1\" class=\"ltx_Math\" alttext=\"1\\times 10^{-6}\" display=\"inline\"><semantics id=\"S3.p1.17.m17.1a\"><mrow id=\"S3.p1.17.m17.1.1\" xref=\"S3.p1.17.m17.1.1.cmml\"><mn id=\"S3.p1.17.m17.1.1.2\" xref=\"S3.p1.17.m17.1.1.2.cmml\">1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S3.p1.17.m17.1.1.1\" xref=\"S3.p1.17.m17.1.1.1.cmml\">×</mo><msup id=\"S3.p1.17.m17.1.1.3\" xref=\"S3.p1.17.m17.1.1.3.cmml\"><mn id=\"S3.p1.17.m17.1.1.3.2\" xref=\"S3.p1.17.m17.1.1.3.2.cmml\">10</mn><mrow id=\"S3.p1.17.m17.1.1.3.3\" xref=\"S3.p1.17.m17.1.1.3.3.cmml\"><mo id=\"S3.p1.17.m17.1.1.3.3a\" xref=\"S3.p1.17.m17.1.1.3.3.cmml\">−</mo><mn id=\"S3.p1.17.m17.1.1.3.3.2\" xref=\"S3.p1.17.m17.1.1.3.3.2.cmml\">6</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.17.m17.1b\"><apply id=\"S3.p1.17.m17.1.1.cmml\" xref=\"S3.p1.17.m17.1.1\"><times id=\"S3.p1.17.m17.1.1.1.cmml\" xref=\"S3.p1.17.m17.1.1.1\"></times><cn type=\"integer\" id=\"S3.p1.17.m17.1.1.2.cmml\" xref=\"S3.p1.17.m17.1.1.2\">1</cn><apply id=\"S3.p1.17.m17.1.1.3.cmml\" xref=\"S3.p1.17.m17.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.p1.17.m17.1.1.3.1.cmml\" xref=\"S3.p1.17.m17.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S3.p1.17.m17.1.1.3.2.cmml\" xref=\"S3.p1.17.m17.1.1.3.2\">10</cn><apply id=\"S3.p1.17.m17.1.1.3.3.cmml\" xref=\"S3.p1.17.m17.1.1.3.3\"><minus id=\"S3.p1.17.m17.1.1.3.3.1.cmml\" xref=\"S3.p1.17.m17.1.1.3.3\"></minus><cn type=\"integer\" id=\"S3.p1.17.m17.1.1.3.3.2.cmml\" xref=\"S3.p1.17.m17.1.1.3.3.2\">6</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.17.m17.1c\">1\\times 10^{-6}</annotation></semantics></math> [12, 13], and previous measurements from Belle and BaBar of <math id=\"S3.p1.18.m18.1\" class=\"ltx_Math\" alttext=\"(1.8-2.3)\\times 10^{-6}\" display=\"inline\"><semantics id=\"S3.p1.18.m18.1a\"><mrow id=\"S3.p1.18.m18.1.1\" xref=\"S3.p1.18.m18.1.1.cmml\"><mrow id=\"S3.p1.18.m18.1.1.1.1\" xref=\"S3.p1.18.m18.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.p1.18.m18.1.1.1.1.2\" xref=\"S3.p1.18.m18.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.p1.18.m18.1.1.1.1.1\" xref=\"S3.p1.18.m18.1.1.1.1.1.cmml\"><mn id=\"S3.p1.18.m18.1.1.1.1.1.2\" xref=\"S3.p1.18.m18.1.1.1.1.1.2.cmml\">1.8</mn><mo id=\"S3.p1.18.m18.1.1.1.1.1.1\" xref=\"S3.p1.18.m18.1.1.1.1.1.1.cmml\">−</mo><mn id=\"S3.p1.18.m18.1.1.1.1.1.3\" xref=\"S3.p1.18.m18.1.1.1.1.1.3.cmml\">2.3</mn></mrow><mo rspace=\"0.055em\" stretchy=\"false\" id=\"S3.p1.18.m18.1.1.1.1.3\" xref=\"S3.p1.18.m18.1.1.1.1.1.cmml\">)</mo></mrow><mo rspace=\"0.222em\" id=\"S3.p1.18.m18.1.1.2\" xref=\"S3.p1.18.m18.1.1.2.cmml\">×</mo><msup id=\"S3.p1.18.m18.1.1.3\" xref=\"S3.p1.18.m18.1.1.3.cmml\"><mn id=\"S3.p1.18.m18.1.1.3.2\" xref=\"S3.p1.18.m18.1.1.3.2.cmml\">10</mn><mrow id=\"S3.p1.18.m18.1.1.3.3\" xref=\"S3.p1.18.m18.1.1.3.3.cmml\"><mo id=\"S3.p1.18.m18.1.1.3.3a\" xref=\"S3.p1.18.m18.1.1.3.3.cmml\">−</mo><mn id=\"S3.p1.18.m18.1.1.3.3.2\" xref=\"S3.p1.18.m18.1.1.3.3.2.cmml\">6</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.18.m18.1b\"><apply id=\"S3.p1.18.m18.1.1.cmml\" xref=\"S3.p1.18.m18.1.1\"><times id=\"S3.p1.18.m18.1.1.2.cmml\" xref=\"S3.p1.18.m18.1.1.2\"></times><apply id=\"S3.p1.18.m18.1.1.1.1.1.cmml\" xref=\"S3.p1.18.m18.1.1.1.1\"><minus id=\"S3.p1.18.m18.1.1.1.1.1.1.cmml\" xref=\"S3.p1.18.m18.1.1.1.1.1.1\"></minus><cn type=\"float\" id=\"S3.p1.18.m18.1.1.1.1.1.2.cmml\" xref=\"S3.p1.18.m18.1.1.1.1.1.2\">1.8</cn><cn type=\"float\" id=\"S3.p1.18.m18.1.1.1.1.1.3.cmml\" xref=\"S3.p1.18.m18.1.1.1.1.1.3\">2.3</cn></apply><apply id=\"S3.p1.18.m18.1.1.3.cmml\" xref=\"S3.p1.18.m18.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.p1.18.m18.1.1.3.1.cmml\" xref=\"S3.p1.18.m18.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S3.p1.18.m18.1.1.3.2.cmml\" xref=\"S3.p1.18.m18.1.1.3.2\">10</cn><apply id=\"S3.p1.18.m18.1.1.3.3.cmml\" xref=\"S3.p1.18.m18.1.1.3.3\"><minus id=\"S3.p1.18.m18.1.1.3.3.1.cmml\" xref=\"S3.p1.18.m18.1.1.3.3\"></minus><cn type=\"integer\" id=\"S3.p1.18.m18.1.1.3.3.2.cmml\" xref=\"S3.p1.18.m18.1.1.3.3.2\">6</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.18.m18.1c\">(1.8-2.3)\\times 10^{-6}</annotation></semantics></math> [14,15]. Our study is based on a data sample recorded at the <math id=\"S3.p1.19.m19.1\" class=\"ltx_Math\" alttext=\"\\Upsilon(4S)\" display=\"inline\"><semantics id=\"S3.p1.19.m19.1a\"><mrow id=\"S3.p1.19.m19.1.1\" xref=\"S3.p1.19.m19.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S3.p1.19.m19.1.1.3\" xref=\"S3.p1.19.m19.1.1.3.cmml\">Υ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.19.m19.1.1.2\" xref=\"S3.p1.19.m19.1.1.2.cmml\">​</mo><mrow id=\"S3.p1.19.m19.1.1.1.1\" xref=\"S3.p1.19.m19.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.p1.19.m19.1.1.1.1.2\" xref=\"S3.p1.19.m19.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.p1.19.m19.1.1.1.1.1\" xref=\"S3.p1.19.m19.1.1.1.1.1.cmml\"><mn id=\"S3.p1.19.m19.1.1.1.1.1.2\" xref=\"S3.p1.19.m19.1.1.1.1.1.2.cmml\">4</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.19.m19.1.1.1.1.1.1\" xref=\"S3.p1.19.m19.1.1.1.1.1.1.cmml\">​</mo><mi id=\"S3.p1.19.m19.1.1.1.1.1.3\" xref=\"S3.p1.19.m19.1.1.1.1.1.3.cmml\">S</mi></mrow><mo stretchy=\"false\" id=\"S3.p1.19.m19.1.1.1.1.3\" xref=\"S3.p1.19.m19.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.19.m19.1b\"><apply id=\"S3.p1.19.m19.1.1.cmml\" xref=\"S3.p1.19.m19.1.1\"><times id=\"S3.p1.19.m19.1.1.2.cmml\" xref=\"S3.p1.19.m19.1.1.2\"></times><ci id=\"S3.p1.19.m19.1.1.3.cmml\" xref=\"S3.p1.19.m19.1.1.3\">Υ</ci><apply id=\"S3.p1.19.m19.1.1.1.1.1.cmml\" xref=\"S3.p1.19.m19.1.1.1.1\"><times id=\"S3.p1.19.m19.1.1.1.1.1.1.cmml\" xref=\"S3.p1.19.m19.1.1.1.1.1.1\"></times><cn type=\"integer\" id=\"S3.p1.19.m19.1.1.1.1.1.2.cmml\" xref=\"S3.p1.19.m19.1.1.1.1.1.2\">4</cn><ci id=\"S3.p1.19.m19.1.1.1.1.1.3.cmml\" xref=\"S3.p1.19.m19.1.1.1.1.1.3\">𝑆</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.19.m19.1c\">\\Upsilon(4S)</annotation></semantics></math> resonance with the Belle detector comprising of <math id=\"S3.p1.20.m20.1\" class=\"ltx_Math\" alttext=\"752\\times 10^{6}\" display=\"inline\"><semantics id=\"S3.p1.20.m20.1a\"><mrow id=\"S3.p1.20.m20.1.1\" xref=\"S3.p1.20.m20.1.1.cmml\"><mn id=\"S3.p1.20.m20.1.1.2\" xref=\"S3.p1.20.m20.1.1.2.cmml\">752</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S3.p1.20.m20.1.1.1\" xref=\"S3.p1.20.m20.1.1.1.cmml\">×</mo><msup id=\"S3.p1.20.m20.1.1.3\" xref=\"S3.p1.20.m20.1.1.3.cmml\"><mn id=\"S3.p1.20.m20.1.1.3.2\" xref=\"S3.p1.20.m20.1.1.3.2.cmml\">10</mn><mn id=\"S3.p1.20.m20.1.1.3.3\" xref=\"S3.p1.20.m20.1.1.3.3.cmml\">6</mn></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.20.m20.1b\"><apply id=\"S3.p1.20.m20.1.1.cmml\" xref=\"S3.p1.20.m20.1.1\"><times id=\"S3.p1.20.m20.1.1.1.cmml\" xref=\"S3.p1.20.m20.1.1.1\"></times><cn type=\"integer\" id=\"S3.p1.20.m20.1.1.2.cmml\" xref=\"S3.p1.20.m20.1.1.2\">752</cn><apply id=\"S3.p1.20.m20.1.1.3.cmml\" xref=\"S3.p1.20.m20.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.p1.20.m20.1.1.3.1.cmml\" xref=\"S3.p1.20.m20.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S3.p1.20.m20.1.1.3.2.cmml\" xref=\"S3.p1.20.m20.1.1.3.2\">10</cn><cn type=\"integer\" id=\"S3.p1.20.m20.1.1.3.3.cmml\" xref=\"S3.p1.20.m20.1.1.3.3\">6</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.20.m20.1c\">752\\times 10^{6}</annotation></semantics></math> <math id=\"S3.p1.21.m21.1\" class=\"ltx_Math\" alttext=\"B\\bar{B}\" display=\"inline\"><semantics id=\"S3.p1.21.m21.1a\"><mrow id=\"S3.p1.21.m21.1.1\" xref=\"S3.p1.21.m21.1.1.cmml\"><mi id=\"S3.p1.21.m21.1.1.2\" xref=\"S3.p1.21.m21.1.1.2.cmml\">B</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.21.m21.1.1.1\" xref=\"S3.p1.21.m21.1.1.1.cmml\">​</mo><mover accent=\"true\" id=\"S3.p1.21.m21.1.1.3\" xref=\"S3.p1.21.m21.1.1.3.cmml\"><mi id=\"S3.p1.21.m21.1.1.3.2\" xref=\"S3.p1.21.m21.1.1.3.2.cmml\">B</mi><mo id=\"S3.p1.21.m21.1.1.3.1\" xref=\"S3.p1.21.m21.1.1.3.1.cmml\">¯</mo></mover></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.21.m21.1b\"><apply id=\"S3.p1.21.m21.1.1.cmml\" xref=\"S3.p1.21.m21.1.1\"><times id=\"S3.p1.21.m21.1.1.1.cmml\" xref=\"S3.p1.21.m21.1.1.1\"></times><ci id=\"S3.p1.21.m21.1.1.2.cmml\" xref=\"S3.p1.21.m21.1.1.2\">𝐵</ci><apply id=\"S3.p1.21.m21.1.1.3.cmml\" xref=\"S3.p1.21.m21.1.1.3\"><ci id=\"S3.p1.21.m21.1.1.3.1.cmml\" xref=\"S3.p1.21.m21.1.1.3.1\">¯</ci><ci id=\"S3.p1.21.m21.1.1.3.2.cmml\" xref=\"S3.p1.21.m21.1.1.3.2\">𝐵</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.21.m21.1c\">B\\bar{B}</annotation></semantics></math> pairs, which corresponds to an integrated luminosity of 693  fb<sup id=\"S3.p1.24.1\" class=\"ltx_sup\"><span id=\"S3.p1.24.1.1\" class=\"ltx_text ltx_font_italic\">-1</span></sup>, and an additional 83.35  fb<sup id=\"S3.p1.24.2\" class=\"ltx_sup\"><span id=\"S3.p1.24.2.1\" class=\"ltx_text ltx_font_italic\">-1</span></sup> recorded 60 MeV below the <math id=\"S3.p1.24.m24.1\" class=\"ltx_Math\" alttext=\"\\Upsilon(4S)\" display=\"inline\"><semantics id=\"S3.p1.24.m24.1a\"><mrow id=\"S3.p1.24.m24.1.1\" xref=\"S3.p1.24.m24.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S3.p1.24.m24.1.1.3\" xref=\"S3.p1.24.m24.1.1.3.cmml\">Υ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.24.m24.1.1.2\" xref=\"S3.p1.24.m24.1.1.2.cmml\">​</mo><mrow id=\"S3.p1.24.m24.1.1.1.1\" xref=\"S3.p1.24.m24.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.p1.24.m24.1.1.1.1.2\" xref=\"S3.p1.24.m24.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.p1.24.m24.1.1.1.1.1\" xref=\"S3.p1.24.m24.1.1.1.1.1.cmml\"><mn id=\"S3.p1.24.m24.1.1.1.1.1.2\" xref=\"S3.p1.24.m24.1.1.1.1.1.2.cmml\">4</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p1.24.m24.1.1.1.1.1.1\" xref=\"S3.p1.24.m24.1.1.1.1.1.1.cmml\">​</mo><mi id=\"S3.p1.24.m24.1.1.1.1.1.3\" xref=\"S3.p1.24.m24.1.1.1.1.1.3.cmml\">S</mi></mrow><mo stretchy=\"false\" id=\"S3.p1.24.m24.1.1.1.1.3\" xref=\"S3.p1.24.m24.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p1.24.m24.1b\"><apply id=\"S3.p1.24.m24.1.1.cmml\" xref=\"S3.p1.24.m24.1.1\"><times id=\"S3.p1.24.m24.1.1.2.cmml\" xref=\"S3.p1.24.m24.1.1.2\"></times><ci id=\"S3.p1.24.m24.1.1.3.cmml\" xref=\"S3.p1.24.m24.1.1.3\">Υ</ci><apply id=\"S3.p1.24.m24.1.1.1.1.1.cmml\" xref=\"S3.p1.24.m24.1.1.1.1\"><times id=\"S3.p1.24.m24.1.1.1.1.1.1.cmml\" xref=\"S3.p1.24.m24.1.1.1.1.1.1\"></times><cn type=\"integer\" id=\"S3.p1.24.m24.1.1.1.1.1.2.cmml\" xref=\"S3.p1.24.m24.1.1.1.1.1.2\">4</cn><ci id=\"S3.p1.24.m24.1.1.1.1.1.3.cmml\" xref=\"S3.p1.24.m24.1.1.1.1.1.3\">𝑆</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p1.24.m24.1c\">\\Upsilon(4S)</annotation></semantics></math> resonance.</p>\n",
      "</div>\n",
      "<div id=\"S3.p2\" class=\"ltx_para\">\n",
      "<p id=\"S3.p2.15\" class=\"ltx_p\">We reconstruct the signal <math id=\"S3.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"B^{0}\" display=\"inline\"><semantics id=\"S3.p2.1.m1.1a\"><msup id=\"S3.p2.1.m1.1.1\" xref=\"S3.p2.1.m1.1.1.cmml\"><mi id=\"S3.p2.1.m1.1.1.2\" xref=\"S3.p2.1.m1.1.1.2.cmml\">B</mi><mn id=\"S3.p2.1.m1.1.1.3\" xref=\"S3.p2.1.m1.1.1.3.cmml\">0</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.1.m1.1b\"><apply id=\"S3.p2.1.m1.1.1.cmml\" xref=\"S3.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p2.1.m1.1.1.1.cmml\" xref=\"S3.p2.1.m1.1.1\">superscript</csymbol><ci id=\"S3.p2.1.m1.1.1.2.cmml\" xref=\"S3.p2.1.m1.1.1.2\">𝐵</ci><cn type=\"integer\" id=\"S3.p2.1.m1.1.1.3.cmml\" xref=\"S3.p2.1.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.1.m1.1c\">B^{0}</annotation></semantics></math> candidate from a pair of <math id=\"S3.p2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\pi^{0}\" display=\"inline\"><semantics id=\"S3.p2.2.m2.1a\"><msup id=\"S3.p2.2.m2.1.1\" xref=\"S3.p2.2.m2.1.1.cmml\"><mi id=\"S3.p2.2.m2.1.1.2\" xref=\"S3.p2.2.m2.1.1.2.cmml\">π</mi><mn id=\"S3.p2.2.m2.1.1.3\" xref=\"S3.p2.2.m2.1.1.3.cmml\">0</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.2.m2.1b\"><apply id=\"S3.p2.2.m2.1.1.cmml\" xref=\"S3.p2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p2.2.m2.1.1.1.cmml\" xref=\"S3.p2.2.m2.1.1\">superscript</csymbol><ci id=\"S3.p2.2.m2.1.1.2.cmml\" xref=\"S3.p2.2.m2.1.1.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p2.2.m2.1.1.3.cmml\" xref=\"S3.p2.2.m2.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.2.m2.1c\">\\pi^{0}</annotation></semantics></math> candidates, each subsequently decaying to two photons. In addition to photons reconstructed from clusters in the electromagnetic calorimeter (ECL) that do not match any charged track, photons that convert to <math id=\"S3.p2.3.m3.1\" class=\"ltx_Math\" alttext=\"e^{+}e^{-}\" display=\"inline\"><semantics id=\"S3.p2.3.m3.1a\"><mrow id=\"S3.p2.3.m3.1.1\" xref=\"S3.p2.3.m3.1.1.cmml\"><msup id=\"S3.p2.3.m3.1.1.2\" xref=\"S3.p2.3.m3.1.1.2.cmml\"><mi id=\"S3.p2.3.m3.1.1.2.2\" xref=\"S3.p2.3.m3.1.1.2.2.cmml\">e</mi><mo id=\"S3.p2.3.m3.1.1.2.3\" xref=\"S3.p2.3.m3.1.1.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.3.m3.1.1.1\" xref=\"S3.p2.3.m3.1.1.1.cmml\">​</mo><msup id=\"S3.p2.3.m3.1.1.3\" xref=\"S3.p2.3.m3.1.1.3.cmml\"><mi id=\"S3.p2.3.m3.1.1.3.2\" xref=\"S3.p2.3.m3.1.1.3.2.cmml\">e</mi><mo id=\"S3.p2.3.m3.1.1.3.3\" xref=\"S3.p2.3.m3.1.1.3.3.cmml\">−</mo></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.3.m3.1b\"><apply id=\"S3.p2.3.m3.1.1.cmml\" xref=\"S3.p2.3.m3.1.1\"><times id=\"S3.p2.3.m3.1.1.1.cmml\" xref=\"S3.p2.3.m3.1.1.1\"></times><apply id=\"S3.p2.3.m3.1.1.2.cmml\" xref=\"S3.p2.3.m3.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.p2.3.m3.1.1.2.1.cmml\" xref=\"S3.p2.3.m3.1.1.2\">superscript</csymbol><ci id=\"S3.p2.3.m3.1.1.2.2.cmml\" xref=\"S3.p2.3.m3.1.1.2.2\">𝑒</ci><plus id=\"S3.p2.3.m3.1.1.2.3.cmml\" xref=\"S3.p2.3.m3.1.1.2.3\"></plus></apply><apply id=\"S3.p2.3.m3.1.1.3.cmml\" xref=\"S3.p2.3.m3.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.p2.3.m3.1.1.3.1.cmml\" xref=\"S3.p2.3.m3.1.1.3\">superscript</csymbol><ci id=\"S3.p2.3.m3.1.1.3.2.cmml\" xref=\"S3.p2.3.m3.1.1.3.2\">𝑒</ci><minus id=\"S3.p2.3.m3.1.1.3.3.cmml\" xref=\"S3.p2.3.m3.1.1.3.3\"></minus></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.3.m3.1c\">e^{+}e^{-}</annotation></semantics></math> pairs in the silicon vertex detector (SVD) are recovered and reconstructed as <math id=\"S3.p2.4.m4.1\" class=\"ltx_Math\" alttext=\"\\pi^{0}\\rightarrow\\gamma e^{+}e^{-}\" display=\"inline\"><semantics id=\"S3.p2.4.m4.1a\"><mrow id=\"S3.p2.4.m4.1.1\" xref=\"S3.p2.4.m4.1.1.cmml\"><msup id=\"S3.p2.4.m4.1.1.2\" xref=\"S3.p2.4.m4.1.1.2.cmml\"><mi id=\"S3.p2.4.m4.1.1.2.2\" xref=\"S3.p2.4.m4.1.1.2.2.cmml\">π</mi><mn id=\"S3.p2.4.m4.1.1.2.3\" xref=\"S3.p2.4.m4.1.1.2.3.cmml\">0</mn></msup><mo stretchy=\"false\" id=\"S3.p2.4.m4.1.1.1\" xref=\"S3.p2.4.m4.1.1.1.cmml\">→</mo><mrow id=\"S3.p2.4.m4.1.1.3\" xref=\"S3.p2.4.m4.1.1.3.cmml\"><mi id=\"S3.p2.4.m4.1.1.3.2\" xref=\"S3.p2.4.m4.1.1.3.2.cmml\">γ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.4.m4.1.1.3.1\" xref=\"S3.p2.4.m4.1.1.3.1.cmml\">​</mo><msup id=\"S3.p2.4.m4.1.1.3.3\" xref=\"S3.p2.4.m4.1.1.3.3.cmml\"><mi id=\"S3.p2.4.m4.1.1.3.3.2\" xref=\"S3.p2.4.m4.1.1.3.3.2.cmml\">e</mi><mo id=\"S3.p2.4.m4.1.1.3.3.3\" xref=\"S3.p2.4.m4.1.1.3.3.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.4.m4.1.1.3.1a\" xref=\"S3.p2.4.m4.1.1.3.1.cmml\">​</mo><msup id=\"S3.p2.4.m4.1.1.3.4\" xref=\"S3.p2.4.m4.1.1.3.4.cmml\"><mi id=\"S3.p2.4.m4.1.1.3.4.2\" xref=\"S3.p2.4.m4.1.1.3.4.2.cmml\">e</mi><mo id=\"S3.p2.4.m4.1.1.3.4.3\" xref=\"S3.p2.4.m4.1.1.3.4.3.cmml\">−</mo></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.4.m4.1b\"><apply id=\"S3.p2.4.m4.1.1.cmml\" xref=\"S3.p2.4.m4.1.1\"><ci id=\"S3.p2.4.m4.1.1.1.cmml\" xref=\"S3.p2.4.m4.1.1.1\">→</ci><apply id=\"S3.p2.4.m4.1.1.2.cmml\" xref=\"S3.p2.4.m4.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.p2.4.m4.1.1.2.1.cmml\" xref=\"S3.p2.4.m4.1.1.2\">superscript</csymbol><ci id=\"S3.p2.4.m4.1.1.2.2.cmml\" xref=\"S3.p2.4.m4.1.1.2.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p2.4.m4.1.1.2.3.cmml\" xref=\"S3.p2.4.m4.1.1.2.3\">0</cn></apply><apply id=\"S3.p2.4.m4.1.1.3.cmml\" xref=\"S3.p2.4.m4.1.1.3\"><times id=\"S3.p2.4.m4.1.1.3.1.cmml\" xref=\"S3.p2.4.m4.1.1.3.1\"></times><ci id=\"S3.p2.4.m4.1.1.3.2.cmml\" xref=\"S3.p2.4.m4.1.1.3.2\">𝛾</ci><apply id=\"S3.p2.4.m4.1.1.3.3.cmml\" xref=\"S3.p2.4.m4.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.p2.4.m4.1.1.3.3.1.cmml\" xref=\"S3.p2.4.m4.1.1.3.3\">superscript</csymbol><ci id=\"S3.p2.4.m4.1.1.3.3.2.cmml\" xref=\"S3.p2.4.m4.1.1.3.3.2\">𝑒</ci><plus id=\"S3.p2.4.m4.1.1.3.3.3.cmml\" xref=\"S3.p2.4.m4.1.1.3.3.3\"></plus></apply><apply id=\"S3.p2.4.m4.1.1.3.4.cmml\" xref=\"S3.p2.4.m4.1.1.3.4\"><csymbol cd=\"ambiguous\" id=\"S3.p2.4.m4.1.1.3.4.1.cmml\" xref=\"S3.p2.4.m4.1.1.3.4\">superscript</csymbol><ci id=\"S3.p2.4.m4.1.1.3.4.2.cmml\" xref=\"S3.p2.4.m4.1.1.3.4.2\">𝑒</ci><minus id=\"S3.p2.4.m4.1.1.3.4.3.cmml\" xref=\"S3.p2.4.m4.1.1.3.4.3\"></minus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.4.m4.1c\">\\pi^{0}\\rightarrow\\gamma e^{+}e^{-}</annotation></semantics></math>. This provides a 5.3% increase in detection efficiency. These photons must have an energy greater than 50 (100) MeV in the barrel (endcap) region of the ECL. The invariant mass of the two-photon combination\n",
      "must lie in the range <math id=\"S3.p2.5.m5.1\" class=\"ltx_Math\" alttext=\"\\mathrm{115}&lt;\\mathrm{m_{\\gamma\\gamma}&lt;152~{}MeV/}c^{2}\" display=\"inline\"><semantics id=\"S3.p2.5.m5.1a\"><mrow id=\"S3.p2.5.m5.1.1\" xref=\"S3.p2.5.m5.1.1.cmml\"><mn id=\"S3.p2.5.m5.1.1.2\" xref=\"S3.p2.5.m5.1.1.2.cmml\">115</mn><mo id=\"S3.p2.5.m5.1.1.3\" xref=\"S3.p2.5.m5.1.1.3.cmml\">&lt;</mo><msub id=\"S3.p2.5.m5.1.1.4\" xref=\"S3.p2.5.m5.1.1.4.cmml\"><mi mathvariant=\"normal\" id=\"S3.p2.5.m5.1.1.4.2\" xref=\"S3.p2.5.m5.1.1.4.2.cmml\">m</mi><mrow id=\"S3.p2.5.m5.1.1.4.3\" xref=\"S3.p2.5.m5.1.1.4.3.cmml\"><mi id=\"S3.p2.5.m5.1.1.4.3.2\" xref=\"S3.p2.5.m5.1.1.4.3.2.cmml\">γ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.5.m5.1.1.4.3.1\" xref=\"S3.p2.5.m5.1.1.4.3.1.cmml\">​</mo><mi id=\"S3.p2.5.m5.1.1.4.3.3\" xref=\"S3.p2.5.m5.1.1.4.3.3.cmml\">γ</mi></mrow></msub><mo id=\"S3.p2.5.m5.1.1.5\" xref=\"S3.p2.5.m5.1.1.5.cmml\">&lt;</mo><mrow id=\"S3.p2.5.m5.1.1.6\" xref=\"S3.p2.5.m5.1.1.6.cmml\"><mrow id=\"S3.p2.5.m5.1.1.6.2\" xref=\"S3.p2.5.m5.1.1.6.2.cmml\"><mn id=\"S3.p2.5.m5.1.1.6.2.2\" xref=\"S3.p2.5.m5.1.1.6.2.2.cmml\">152</mn><mo lspace=\"0.330em\" rspace=\"0em\" id=\"S3.p2.5.m5.1.1.6.2.1\" xref=\"S3.p2.5.m5.1.1.6.2.1.cmml\">​</mo><mi id=\"S3.p2.5.m5.1.1.6.2.3\" xref=\"S3.p2.5.m5.1.1.6.2.3.cmml\">MeV</mi></mrow><mo id=\"S3.p2.5.m5.1.1.6.1\" xref=\"S3.p2.5.m5.1.1.6.1.cmml\">/</mo><msup id=\"S3.p2.5.m5.1.1.6.3\" xref=\"S3.p2.5.m5.1.1.6.3.cmml\"><mi id=\"S3.p2.5.m5.1.1.6.3.2\" xref=\"S3.p2.5.m5.1.1.6.3.2.cmml\">c</mi><mn id=\"S3.p2.5.m5.1.1.6.3.3\" xref=\"S3.p2.5.m5.1.1.6.3.3.cmml\">2</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.5.m5.1b\"><apply id=\"S3.p2.5.m5.1.1.cmml\" xref=\"S3.p2.5.m5.1.1\"><and id=\"S3.p2.5.m5.1.1a.cmml\" xref=\"S3.p2.5.m5.1.1\"></and><apply id=\"S3.p2.5.m5.1.1b.cmml\" xref=\"S3.p2.5.m5.1.1\"><lt id=\"S3.p2.5.m5.1.1.3.cmml\" xref=\"S3.p2.5.m5.1.1.3\"></lt><cn type=\"integer\" id=\"S3.p2.5.m5.1.1.2.cmml\" xref=\"S3.p2.5.m5.1.1.2\">115</cn><apply id=\"S3.p2.5.m5.1.1.4.cmml\" xref=\"S3.p2.5.m5.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S3.p2.5.m5.1.1.4.1.cmml\" xref=\"S3.p2.5.m5.1.1.4\">subscript</csymbol><ci id=\"S3.p2.5.m5.1.1.4.2.cmml\" xref=\"S3.p2.5.m5.1.1.4.2\">m</ci><apply id=\"S3.p2.5.m5.1.1.4.3.cmml\" xref=\"S3.p2.5.m5.1.1.4.3\"><times id=\"S3.p2.5.m5.1.1.4.3.1.cmml\" xref=\"S3.p2.5.m5.1.1.4.3.1\"></times><ci id=\"S3.p2.5.m5.1.1.4.3.2.cmml\" xref=\"S3.p2.5.m5.1.1.4.3.2\">𝛾</ci><ci id=\"S3.p2.5.m5.1.1.4.3.3.cmml\" xref=\"S3.p2.5.m5.1.1.4.3.3\">𝛾</ci></apply></apply></apply><apply id=\"S3.p2.5.m5.1.1c.cmml\" xref=\"S3.p2.5.m5.1.1\"><lt id=\"S3.p2.5.m5.1.1.5.cmml\" xref=\"S3.p2.5.m5.1.1.5\"></lt><share href=\"#S3.p2.5.m5.1.1.4.cmml\" id=\"S3.p2.5.m5.1.1d.cmml\" xref=\"S3.p2.5.m5.1.1\"></share><apply id=\"S3.p2.5.m5.1.1.6.cmml\" xref=\"S3.p2.5.m5.1.1.6\"><divide id=\"S3.p2.5.m5.1.1.6.1.cmml\" xref=\"S3.p2.5.m5.1.1.6.1\"></divide><apply id=\"S3.p2.5.m5.1.1.6.2.cmml\" xref=\"S3.p2.5.m5.1.1.6.2\"><times id=\"S3.p2.5.m5.1.1.6.2.1.cmml\" xref=\"S3.p2.5.m5.1.1.6.2.1\"></times><cn type=\"integer\" id=\"S3.p2.5.m5.1.1.6.2.2.cmml\" xref=\"S3.p2.5.m5.1.1.6.2.2\">152</cn><ci id=\"S3.p2.5.m5.1.1.6.2.3.cmml\" xref=\"S3.p2.5.m5.1.1.6.2.3\">MeV</ci></apply><apply id=\"S3.p2.5.m5.1.1.6.3.cmml\" xref=\"S3.p2.5.m5.1.1.6.3\"><csymbol cd=\"ambiguous\" id=\"S3.p2.5.m5.1.1.6.3.1.cmml\" xref=\"S3.p2.5.m5.1.1.6.3\">superscript</csymbol><ci id=\"S3.p2.5.m5.1.1.6.3.2.cmml\" xref=\"S3.p2.5.m5.1.1.6.3.2\">𝑐</ci><cn type=\"integer\" id=\"S3.p2.5.m5.1.1.6.3.3.cmml\" xref=\"S3.p2.5.m5.1.1.6.3.3\">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.5.m5.1c\">\\mathrm{115}&lt;\\mathrm{m_{\\gamma\\gamma}&lt;152~{}MeV/}c^{2}</annotation></semantics></math>, corresponding to <math id=\"S3.p2.6.m6.1\" class=\"ltx_Math\" alttext=\"\\pm 2.6\\sigma\" display=\"inline\"><semantics id=\"S3.p2.6.m6.1a\"><mrow id=\"S3.p2.6.m6.1.1\" xref=\"S3.p2.6.m6.1.1.cmml\"><mo id=\"S3.p2.6.m6.1.1a\" xref=\"S3.p2.6.m6.1.1.cmml\">±</mo><mrow id=\"S3.p2.6.m6.1.1.2\" xref=\"S3.p2.6.m6.1.1.2.cmml\"><mn id=\"S3.p2.6.m6.1.1.2.2\" xref=\"S3.p2.6.m6.1.1.2.2.cmml\">2.6</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.6.m6.1.1.2.1\" xref=\"S3.p2.6.m6.1.1.2.1.cmml\">​</mo><mi id=\"S3.p2.6.m6.1.1.2.3\" xref=\"S3.p2.6.m6.1.1.2.3.cmml\">σ</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.6.m6.1b\"><apply id=\"S3.p2.6.m6.1.1.cmml\" xref=\"S3.p2.6.m6.1.1\"><csymbol cd=\"latexml\" id=\"S3.p2.6.m6.1.1.1.cmml\" xref=\"S3.p2.6.m6.1.1\">plus-or-minus</csymbol><apply id=\"S3.p2.6.m6.1.1.2.cmml\" xref=\"S3.p2.6.m6.1.1.2\"><times id=\"S3.p2.6.m6.1.1.2.1.cmml\" xref=\"S3.p2.6.m6.1.1.2.1\"></times><cn type=\"float\" id=\"S3.p2.6.m6.1.1.2.2.cmml\" xref=\"S3.p2.6.m6.1.1.2.2\">2.6</cn><ci id=\"S3.p2.6.m6.1.1.2.3.cmml\" xref=\"S3.p2.6.m6.1.1.2.3\">𝜎</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.6.m6.1c\">\\pm 2.6\\sigma</annotation></semantics></math> around the nominal <math id=\"S3.p2.7.m7.1\" class=\"ltx_Math\" alttext=\"\\pi^{0}\" display=\"inline\"><semantics id=\"S3.p2.7.m7.1a\"><msup id=\"S3.p2.7.m7.1.1\" xref=\"S3.p2.7.m7.1.1.cmml\"><mi id=\"S3.p2.7.m7.1.1.2\" xref=\"S3.p2.7.m7.1.1.2.cmml\">π</mi><mn id=\"S3.p2.7.m7.1.1.3\" xref=\"S3.p2.7.m7.1.1.3.cmml\">0</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.7.m7.1b\"><apply id=\"S3.p2.7.m7.1.1.cmml\" xref=\"S3.p2.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p2.7.m7.1.1.1.cmml\" xref=\"S3.p2.7.m7.1.1\">superscript</csymbol><ci id=\"S3.p2.7.m7.1.1.2.cmml\" xref=\"S3.p2.7.m7.1.1.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p2.7.m7.1.1.3.cmml\" xref=\"S3.p2.7.m7.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.7.m7.1c\">\\pi^{0}</annotation></semantics></math> mass [9]. As in the case of <math id=\"S3.p2.8.m8.1\" class=\"ltx_Math\" alttext=\"B^{+}\\rightarrow K^{+}K^{-}\\pi^{+}\" display=\"inline\"><semantics id=\"S3.p2.8.m8.1a\"><mrow id=\"S3.p2.8.m8.1.1\" xref=\"S3.p2.8.m8.1.1.cmml\"><msup id=\"S3.p2.8.m8.1.1.2\" xref=\"S3.p2.8.m8.1.1.2.cmml\"><mi id=\"S3.p2.8.m8.1.1.2.2\" xref=\"S3.p2.8.m8.1.1.2.2.cmml\">B</mi><mo id=\"S3.p2.8.m8.1.1.2.3\" xref=\"S3.p2.8.m8.1.1.2.3.cmml\">+</mo></msup><mo stretchy=\"false\" id=\"S3.p2.8.m8.1.1.1\" xref=\"S3.p2.8.m8.1.1.1.cmml\">→</mo><mrow id=\"S3.p2.8.m8.1.1.3\" xref=\"S3.p2.8.m8.1.1.3.cmml\"><msup id=\"S3.p2.8.m8.1.1.3.2\" xref=\"S3.p2.8.m8.1.1.3.2.cmml\"><mi id=\"S3.p2.8.m8.1.1.3.2.2\" xref=\"S3.p2.8.m8.1.1.3.2.2.cmml\">K</mi><mo id=\"S3.p2.8.m8.1.1.3.2.3\" xref=\"S3.p2.8.m8.1.1.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.8.m8.1.1.3.1\" xref=\"S3.p2.8.m8.1.1.3.1.cmml\">​</mo><msup id=\"S3.p2.8.m8.1.1.3.3\" xref=\"S3.p2.8.m8.1.1.3.3.cmml\"><mi id=\"S3.p2.8.m8.1.1.3.3.2\" xref=\"S3.p2.8.m8.1.1.3.3.2.cmml\">K</mi><mo id=\"S3.p2.8.m8.1.1.3.3.3\" xref=\"S3.p2.8.m8.1.1.3.3.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.8.m8.1.1.3.1a\" xref=\"S3.p2.8.m8.1.1.3.1.cmml\">​</mo><msup id=\"S3.p2.8.m8.1.1.3.4\" xref=\"S3.p2.8.m8.1.1.3.4.cmml\"><mi id=\"S3.p2.8.m8.1.1.3.4.2\" xref=\"S3.p2.8.m8.1.1.3.4.2.cmml\">π</mi><mo id=\"S3.p2.8.m8.1.1.3.4.3\" xref=\"S3.p2.8.m8.1.1.3.4.3.cmml\">+</mo></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.8.m8.1b\"><apply id=\"S3.p2.8.m8.1.1.cmml\" xref=\"S3.p2.8.m8.1.1\"><ci id=\"S3.p2.8.m8.1.1.1.cmml\" xref=\"S3.p2.8.m8.1.1.1\">→</ci><apply id=\"S3.p2.8.m8.1.1.2.cmml\" xref=\"S3.p2.8.m8.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.p2.8.m8.1.1.2.1.cmml\" xref=\"S3.p2.8.m8.1.1.2\">superscript</csymbol><ci id=\"S3.p2.8.m8.1.1.2.2.cmml\" xref=\"S3.p2.8.m8.1.1.2.2\">𝐵</ci><plus id=\"S3.p2.8.m8.1.1.2.3.cmml\" xref=\"S3.p2.8.m8.1.1.2.3\"></plus></apply><apply id=\"S3.p2.8.m8.1.1.3.cmml\" xref=\"S3.p2.8.m8.1.1.3\"><times id=\"S3.p2.8.m8.1.1.3.1.cmml\" xref=\"S3.p2.8.m8.1.1.3.1\"></times><apply id=\"S3.p2.8.m8.1.1.3.2.cmml\" xref=\"S3.p2.8.m8.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.p2.8.m8.1.1.3.2.1.cmml\" xref=\"S3.p2.8.m8.1.1.3.2\">superscript</csymbol><ci id=\"S3.p2.8.m8.1.1.3.2.2.cmml\" xref=\"S3.p2.8.m8.1.1.3.2.2\">𝐾</ci><plus id=\"S3.p2.8.m8.1.1.3.2.3.cmml\" xref=\"S3.p2.8.m8.1.1.3.2.3\"></plus></apply><apply id=\"S3.p2.8.m8.1.1.3.3.cmml\" xref=\"S3.p2.8.m8.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.p2.8.m8.1.1.3.3.1.cmml\" xref=\"S3.p2.8.m8.1.1.3.3\">superscript</csymbol><ci id=\"S3.p2.8.m8.1.1.3.3.2.cmml\" xref=\"S3.p2.8.m8.1.1.3.3.2\">𝐾</ci><minus id=\"S3.p2.8.m8.1.1.3.3.3.cmml\" xref=\"S3.p2.8.m8.1.1.3.3.3\"></minus></apply><apply id=\"S3.p2.8.m8.1.1.3.4.cmml\" xref=\"S3.p2.8.m8.1.1.3.4\"><csymbol cd=\"ambiguous\" id=\"S3.p2.8.m8.1.1.3.4.1.cmml\" xref=\"S3.p2.8.m8.1.1.3.4\">superscript</csymbol><ci id=\"S3.p2.8.m8.1.1.3.4.2.cmml\" xref=\"S3.p2.8.m8.1.1.3.4.2\">𝜋</ci><plus id=\"S3.p2.8.m8.1.1.3.4.3.cmml\" xref=\"S3.p2.8.m8.1.1.3.4.3\"></plus></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.8.m8.1c\">B^{+}\\rightarrow K^{+}K^{-}\\pi^{+}</annotation></semantics></math>, two kinematic variables <math id=\"S3.p2.9.m9.1\" class=\"ltx_Math\" alttext=\"\\Delta E\" display=\"inline\"><semantics id=\"S3.p2.9.m9.1a\"><mrow id=\"S3.p2.9.m9.1.1\" xref=\"S3.p2.9.m9.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S3.p2.9.m9.1.1.2\" xref=\"S3.p2.9.m9.1.1.2.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.9.m9.1.1.1\" xref=\"S3.p2.9.m9.1.1.1.cmml\">​</mo><mi id=\"S3.p2.9.m9.1.1.3\" xref=\"S3.p2.9.m9.1.1.3.cmml\">E</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.9.m9.1b\"><apply id=\"S3.p2.9.m9.1.1.cmml\" xref=\"S3.p2.9.m9.1.1\"><times id=\"S3.p2.9.m9.1.1.1.cmml\" xref=\"S3.p2.9.m9.1.1.1\"></times><ci id=\"S3.p2.9.m9.1.1.2.cmml\" xref=\"S3.p2.9.m9.1.1.2\">Δ</ci><ci id=\"S3.p2.9.m9.1.1.3.cmml\" xref=\"S3.p2.9.m9.1.1.3\">𝐸</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.9.m9.1c\">\\Delta E</annotation></semantics></math> and <math id=\"S3.p2.10.m10.1\" class=\"ltx_Math\" alttext=\"M_{bc}\" display=\"inline\"><semantics id=\"S3.p2.10.m10.1a\"><msub id=\"S3.p2.10.m10.1.1\" xref=\"S3.p2.10.m10.1.1.cmml\"><mi id=\"S3.p2.10.m10.1.1.2\" xref=\"S3.p2.10.m10.1.1.2.cmml\">M</mi><mrow id=\"S3.p2.10.m10.1.1.3\" xref=\"S3.p2.10.m10.1.1.3.cmml\"><mi id=\"S3.p2.10.m10.1.1.3.2\" xref=\"S3.p2.10.m10.1.1.3.2.cmml\">b</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.10.m10.1.1.3.1\" xref=\"S3.p2.10.m10.1.1.3.1.cmml\">​</mo><mi id=\"S3.p2.10.m10.1.1.3.3\" xref=\"S3.p2.10.m10.1.1.3.3.cmml\">c</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.10.m10.1b\"><apply id=\"S3.p2.10.m10.1.1.cmml\" xref=\"S3.p2.10.m10.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p2.10.m10.1.1.1.cmml\" xref=\"S3.p2.10.m10.1.1\">subscript</csymbol><ci id=\"S3.p2.10.m10.1.1.2.cmml\" xref=\"S3.p2.10.m10.1.1.2\">𝑀</ci><apply id=\"S3.p2.10.m10.1.1.3.cmml\" xref=\"S3.p2.10.m10.1.1.3\"><times id=\"S3.p2.10.m10.1.1.3.1.cmml\" xref=\"S3.p2.10.m10.1.1.3.1\"></times><ci id=\"S3.p2.10.m10.1.1.3.2.cmml\" xref=\"S3.p2.10.m10.1.1.3.2\">𝑏</ci><ci id=\"S3.p2.10.m10.1.1.3.3.cmml\" xref=\"S3.p2.10.m10.1.1.3.3\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.10.m10.1c\">M_{bc}</annotation></semantics></math> are used to select the signal candidates. All candidates satisfying <math id=\"S3.p2.11.m11.1\" class=\"ltx_Math\" alttext=\"M_{bc}&gt;5.26~{}\\mathrm{GeV}/c^{2}\" display=\"inline\"><semantics id=\"S3.p2.11.m11.1a\"><mrow id=\"S3.p2.11.m11.1.1\" xref=\"S3.p2.11.m11.1.1.cmml\"><msub id=\"S3.p2.11.m11.1.1.2\" xref=\"S3.p2.11.m11.1.1.2.cmml\"><mi id=\"S3.p2.11.m11.1.1.2.2\" xref=\"S3.p2.11.m11.1.1.2.2.cmml\">M</mi><mrow id=\"S3.p2.11.m11.1.1.2.3\" xref=\"S3.p2.11.m11.1.1.2.3.cmml\"><mi id=\"S3.p2.11.m11.1.1.2.3.2\" xref=\"S3.p2.11.m11.1.1.2.3.2.cmml\">b</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.11.m11.1.1.2.3.1\" xref=\"S3.p2.11.m11.1.1.2.3.1.cmml\">​</mo><mi id=\"S3.p2.11.m11.1.1.2.3.3\" xref=\"S3.p2.11.m11.1.1.2.3.3.cmml\">c</mi></mrow></msub><mo id=\"S3.p2.11.m11.1.1.1\" xref=\"S3.p2.11.m11.1.1.1.cmml\">&gt;</mo><mrow id=\"S3.p2.11.m11.1.1.3\" xref=\"S3.p2.11.m11.1.1.3.cmml\"><mrow id=\"S3.p2.11.m11.1.1.3.2\" xref=\"S3.p2.11.m11.1.1.3.2.cmml\"><mn id=\"S3.p2.11.m11.1.1.3.2.2\" xref=\"S3.p2.11.m11.1.1.3.2.2.cmml\">5.26</mn><mo lspace=\"0.330em\" rspace=\"0em\" id=\"S3.p2.11.m11.1.1.3.2.1\" xref=\"S3.p2.11.m11.1.1.3.2.1.cmml\">​</mo><mi id=\"S3.p2.11.m11.1.1.3.2.3\" xref=\"S3.p2.11.m11.1.1.3.2.3.cmml\">GeV</mi></mrow><mo id=\"S3.p2.11.m11.1.1.3.1\" xref=\"S3.p2.11.m11.1.1.3.1.cmml\">/</mo><msup id=\"S3.p2.11.m11.1.1.3.3\" xref=\"S3.p2.11.m11.1.1.3.3.cmml\"><mi id=\"S3.p2.11.m11.1.1.3.3.2\" xref=\"S3.p2.11.m11.1.1.3.3.2.cmml\">c</mi><mn id=\"S3.p2.11.m11.1.1.3.3.3\" xref=\"S3.p2.11.m11.1.1.3.3.3.cmml\">2</mn></msup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.11.m11.1b\"><apply id=\"S3.p2.11.m11.1.1.cmml\" xref=\"S3.p2.11.m11.1.1\"><gt id=\"S3.p2.11.m11.1.1.1.cmml\" xref=\"S3.p2.11.m11.1.1.1\"></gt><apply id=\"S3.p2.11.m11.1.1.2.cmml\" xref=\"S3.p2.11.m11.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.p2.11.m11.1.1.2.1.cmml\" xref=\"S3.p2.11.m11.1.1.2\">subscript</csymbol><ci id=\"S3.p2.11.m11.1.1.2.2.cmml\" xref=\"S3.p2.11.m11.1.1.2.2\">𝑀</ci><apply id=\"S3.p2.11.m11.1.1.2.3.cmml\" xref=\"S3.p2.11.m11.1.1.2.3\"><times id=\"S3.p2.11.m11.1.1.2.3.1.cmml\" xref=\"S3.p2.11.m11.1.1.2.3.1\"></times><ci id=\"S3.p2.11.m11.1.1.2.3.2.cmml\" xref=\"S3.p2.11.m11.1.1.2.3.2\">𝑏</ci><ci id=\"S3.p2.11.m11.1.1.2.3.3.cmml\" xref=\"S3.p2.11.m11.1.1.2.3.3\">𝑐</ci></apply></apply><apply id=\"S3.p2.11.m11.1.1.3.cmml\" xref=\"S3.p2.11.m11.1.1.3\"><divide id=\"S3.p2.11.m11.1.1.3.1.cmml\" xref=\"S3.p2.11.m11.1.1.3.1\"></divide><apply id=\"S3.p2.11.m11.1.1.3.2.cmml\" xref=\"S3.p2.11.m11.1.1.3.2\"><times id=\"S3.p2.11.m11.1.1.3.2.1.cmml\" xref=\"S3.p2.11.m11.1.1.3.2.1\"></times><cn type=\"float\" id=\"S3.p2.11.m11.1.1.3.2.2.cmml\" xref=\"S3.p2.11.m11.1.1.3.2.2\">5.26</cn><ci id=\"S3.p2.11.m11.1.1.3.2.3.cmml\" xref=\"S3.p2.11.m11.1.1.3.2.3\">GeV</ci></apply><apply id=\"S3.p2.11.m11.1.1.3.3.cmml\" xref=\"S3.p2.11.m11.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.p2.11.m11.1.1.3.3.1.cmml\" xref=\"S3.p2.11.m11.1.1.3.3\">superscript</csymbol><ci id=\"S3.p2.11.m11.1.1.3.3.2.cmml\" xref=\"S3.p2.11.m11.1.1.3.3.2\">𝑐</ci><cn type=\"integer\" id=\"S3.p2.11.m11.1.1.3.3.3.cmml\" xref=\"S3.p2.11.m11.1.1.3.3.3\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.11.m11.1c\">M_{bc}&gt;5.26~{}\\mathrm{GeV}/c^{2}</annotation></semantics></math> and <math id=\"S3.p2.12.m12.1\" class=\"ltx_Math\" alttext=\"-0.3&lt;\\Delta E&lt;0.2~{}\\rm{GeV}\" display=\"inline\"><semantics id=\"S3.p2.12.m12.1a\"><mrow id=\"S3.p2.12.m12.1.1\" xref=\"S3.p2.12.m12.1.1.cmml\"><mrow id=\"S3.p2.12.m12.1.1.2\" xref=\"S3.p2.12.m12.1.1.2.cmml\"><mo id=\"S3.p2.12.m12.1.1.2a\" xref=\"S3.p2.12.m12.1.1.2.cmml\">−</mo><mn id=\"S3.p2.12.m12.1.1.2.2\" xref=\"S3.p2.12.m12.1.1.2.2.cmml\">0.3</mn></mrow><mo id=\"S3.p2.12.m12.1.1.3\" xref=\"S3.p2.12.m12.1.1.3.cmml\">&lt;</mo><mrow id=\"S3.p2.12.m12.1.1.4\" xref=\"S3.p2.12.m12.1.1.4.cmml\"><mi mathvariant=\"normal\" id=\"S3.p2.12.m12.1.1.4.2\" xref=\"S3.p2.12.m12.1.1.4.2.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p2.12.m12.1.1.4.1\" xref=\"S3.p2.12.m12.1.1.4.1.cmml\">​</mo><mi id=\"S3.p2.12.m12.1.1.4.3\" xref=\"S3.p2.12.m12.1.1.4.3.cmml\">E</mi></mrow><mo id=\"S3.p2.12.m12.1.1.5\" xref=\"S3.p2.12.m12.1.1.5.cmml\">&lt;</mo><mrow id=\"S3.p2.12.m12.1.1.6\" xref=\"S3.p2.12.m12.1.1.6.cmml\"><mn id=\"S3.p2.12.m12.1.1.6.2\" xref=\"S3.p2.12.m12.1.1.6.2.cmml\">0.2</mn><mo lspace=\"0.330em\" rspace=\"0em\" id=\"S3.p2.12.m12.1.1.6.1\" xref=\"S3.p2.12.m12.1.1.6.1.cmml\">​</mo><mi id=\"S3.p2.12.m12.1.1.6.3\" xref=\"S3.p2.12.m12.1.1.6.3.cmml\">GeV</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.12.m12.1b\"><apply id=\"S3.p2.12.m12.1.1.cmml\" xref=\"S3.p2.12.m12.1.1\"><and id=\"S3.p2.12.m12.1.1a.cmml\" xref=\"S3.p2.12.m12.1.1\"></and><apply id=\"S3.p2.12.m12.1.1b.cmml\" xref=\"S3.p2.12.m12.1.1\"><lt id=\"S3.p2.12.m12.1.1.3.cmml\" xref=\"S3.p2.12.m12.1.1.3\"></lt><apply id=\"S3.p2.12.m12.1.1.2.cmml\" xref=\"S3.p2.12.m12.1.1.2\"><minus id=\"S3.p2.12.m12.1.1.2.1.cmml\" xref=\"S3.p2.12.m12.1.1.2\"></minus><cn type=\"float\" id=\"S3.p2.12.m12.1.1.2.2.cmml\" xref=\"S3.p2.12.m12.1.1.2.2\">0.3</cn></apply><apply id=\"S3.p2.12.m12.1.1.4.cmml\" xref=\"S3.p2.12.m12.1.1.4\"><times id=\"S3.p2.12.m12.1.1.4.1.cmml\" xref=\"S3.p2.12.m12.1.1.4.1\"></times><ci id=\"S3.p2.12.m12.1.1.4.2.cmml\" xref=\"S3.p2.12.m12.1.1.4.2\">Δ</ci><ci id=\"S3.p2.12.m12.1.1.4.3.cmml\" xref=\"S3.p2.12.m12.1.1.4.3\">𝐸</ci></apply></apply><apply id=\"S3.p2.12.m12.1.1c.cmml\" xref=\"S3.p2.12.m12.1.1\"><lt id=\"S3.p2.12.m12.1.1.5.cmml\" xref=\"S3.p2.12.m12.1.1.5\"></lt><share href=\"#S3.p2.12.m12.1.1.4.cmml\" id=\"S3.p2.12.m12.1.1d.cmml\" xref=\"S3.p2.12.m12.1.1\"></share><apply id=\"S3.p2.12.m12.1.1.6.cmml\" xref=\"S3.p2.12.m12.1.1.6\"><times id=\"S3.p2.12.m12.1.1.6.1.cmml\" xref=\"S3.p2.12.m12.1.1.6.1\"></times><cn type=\"float\" id=\"S3.p2.12.m12.1.1.6.2.cmml\" xref=\"S3.p2.12.m12.1.1.6.2\">0.2</cn><ci id=\"S3.p2.12.m12.1.1.6.3.cmml\" xref=\"S3.p2.12.m12.1.1.6.3\">GeV</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.12.m12.1c\">-0.3&lt;\\Delta E&lt;0.2~{}\\rm{GeV}</annotation></semantics></math> are retained for further analysis. For 7.2% of the events, there are multiple <math id=\"S3.p2.13.m13.1\" class=\"ltx_Math\" alttext=\"B^{0}\" display=\"inline\"><semantics id=\"S3.p2.13.m13.1a\"><msup id=\"S3.p2.13.m13.1.1\" xref=\"S3.p2.13.m13.1.1.cmml\"><mi id=\"S3.p2.13.m13.1.1.2\" xref=\"S3.p2.13.m13.1.1.2.cmml\">B</mi><mn id=\"S3.p2.13.m13.1.1.3\" xref=\"S3.p2.13.m13.1.1.3.cmml\">0</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.13.m13.1b\"><apply id=\"S3.p2.13.m13.1.1.cmml\" xref=\"S3.p2.13.m13.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p2.13.m13.1.1.1.cmml\" xref=\"S3.p2.13.m13.1.1\">superscript</csymbol><ci id=\"S3.p2.13.m13.1.1.2.cmml\" xref=\"S3.p2.13.m13.1.1.2\">𝐵</ci><cn type=\"integer\" id=\"S3.p2.13.m13.1.1.3.cmml\" xref=\"S3.p2.13.m13.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.13.m13.1c\">B^{0}</annotation></semantics></math> candidates in which case we choose the one that minimizes the deviation of the two <math id=\"S3.p2.14.m14.1\" class=\"ltx_Math\" alttext=\"\\pi^{0}\" display=\"inline\"><semantics id=\"S3.p2.14.m14.1a\"><msup id=\"S3.p2.14.m14.1.1\" xref=\"S3.p2.14.m14.1.1.cmml\"><mi id=\"S3.p2.14.m14.1.1.2\" xref=\"S3.p2.14.m14.1.1.2.cmml\">π</mi><mn id=\"S3.p2.14.m14.1.1.3\" xref=\"S3.p2.14.m14.1.1.3.cmml\">0</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.14.m14.1b\"><apply id=\"S3.p2.14.m14.1.1.cmml\" xref=\"S3.p2.14.m14.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p2.14.m14.1.1.1.cmml\" xref=\"S3.p2.14.m14.1.1\">superscript</csymbol><ci id=\"S3.p2.14.m14.1.1.2.cmml\" xref=\"S3.p2.14.m14.1.1.2\">𝜋</ci><cn type=\"integer\" id=\"S3.p2.14.m14.1.1.3.cmml\" xref=\"S3.p2.14.m14.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.14.m14.1c\">\\pi^{0}</annotation></semantics></math>’s reconstructed invariant masses from the world average [9]. This criterion selects the true <math id=\"S3.p2.15.m15.1\" class=\"ltx_Math\" alttext=\"B^{0}\" display=\"inline\"><semantics id=\"S3.p2.15.m15.1a\"><msup id=\"S3.p2.15.m15.1.1\" xref=\"S3.p2.15.m15.1.1.cmml\"><mi id=\"S3.p2.15.m15.1.1.2\" xref=\"S3.p2.15.m15.1.1.2.cmml\">B</mi><mn id=\"S3.p2.15.m15.1.1.3\" xref=\"S3.p2.15.m15.1.1.3.cmml\">0</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.p2.15.m15.1b\"><apply id=\"S3.p2.15.m15.1.1.cmml\" xref=\"S3.p2.15.m15.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p2.15.m15.1.1.1.cmml\" xref=\"S3.p2.15.m15.1.1\">superscript</csymbol><ci id=\"S3.p2.15.m15.1.1.2.cmml\" xref=\"S3.p2.15.m15.1.1.2\">𝐵</ci><cn type=\"integer\" id=\"S3.p2.15.m15.1.1.3.cmml\" xref=\"S3.p2.15.m15.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p2.15.m15.1c\">B^{0}</annotation></semantics></math> candidate in 90% of MC events.</p>\n",
      "</div>\n",
      "<div id=\"S3.p3\" class=\"ltx_para\">\n",
      "<p id=\"S3.p3.21\" class=\"ltx_p\">The dominant background is from <math id=\"S3.p3.1.m1.5\" class=\"ltx_Math\" alttext=\"e^{+}e^{-}\\rightarrow~{}q\\bar{q}~{}(q=u,d,s,c)\" display=\"inline\"><semantics id=\"S3.p3.1.m1.5a\"><mrow id=\"S3.p3.1.m1.5.5\" xref=\"S3.p3.1.m1.5.5.cmml\"><mrow id=\"S3.p3.1.m1.5.5.3\" xref=\"S3.p3.1.m1.5.5.3.cmml\"><msup id=\"S3.p3.1.m1.5.5.3.2\" xref=\"S3.p3.1.m1.5.5.3.2.cmml\"><mi id=\"S3.p3.1.m1.5.5.3.2.2\" xref=\"S3.p3.1.m1.5.5.3.2.2.cmml\">e</mi><mo id=\"S3.p3.1.m1.5.5.3.2.3\" xref=\"S3.p3.1.m1.5.5.3.2.3.cmml\">+</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p3.1.m1.5.5.3.1\" xref=\"S3.p3.1.m1.5.5.3.1.cmml\">​</mo><msup id=\"S3.p3.1.m1.5.5.3.3\" xref=\"S3.p3.1.m1.5.5.3.3.cmml\"><mi id=\"S3.p3.1.m1.5.5.3.3.2\" xref=\"S3.p3.1.m1.5.5.3.3.2.cmml\">e</mi><mo id=\"S3.p3.1.m1.5.5.3.3.3\" xref=\"S3.p3.1.m1.5.5.3.3.3.cmml\">−</mo></msup></mrow><mo rspace=\"0.608em\" stretchy=\"false\" id=\"S3.p3.1.m1.5.5.2\" xref=\"S3.p3.1.m1.5.5.2.cmml\">→</mo><mrow id=\"S3.p3.1.m1.5.5.1\" xref=\"S3.p3.1.m1.5.5.1.cmml\"><mi id=\"S3.p3.1.m1.5.5.1.3\" xref=\"S3.p3.1.m1.5.5.1.3.cmml\">q</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p3.1.m1.5.5.1.2\" xref=\"S3.p3.1.m1.5.5.1.2.cmml\">​</mo><mover accent=\"true\" id=\"S3.p3.1.m1.5.5.1.4\" xref=\"S3.p3.1.m1.5.5.1.4.cmml\"><mi id=\"S3.p3.1.m1.5.5.1.4.2\" xref=\"S3.p3.1.m1.5.5.1.4.2.cmml\">q</mi><mo id=\"S3.p3.1.m1.5.5.1.4.1\" xref=\"S3.p3.1.m1.5.5.1.4.1.cmml\">¯</mo></mover><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.p3.1.m1.5.5.1.2a\" xref=\"S3.p3.1.m1.5.5.1.2.cmml\">​</mo><mrow id=\"S3.p3.1.m1.5.5.1.1.1\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.p3.1.m1.5.5.1.1.1.2\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.cmml\">(</mo><mrow id=\"S3.p3.1.m1.5.5.1.1.1.1\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.cmml\"><mi id=\"S3.p3.1.m1.5.5.1.1.1.1.2\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.2.cmml\">q</mi><mo id=\"S3.p3.1.m1.5.5.1.1.1.1.1\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.1.cmml\">=</mo><mrow id=\"S3.p3.1.m1.5.5.1.1.1.1.3.2\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.3.1.cmml\"><mi id=\"S3.p3.1.m1.1.1\" xref=\"S3.p3.1.m1.1.1.cmml\">u</mi><mo id=\"S3.p3.1.m1.5.5.1.1.1.1.3.2.1\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.3.1.cmml\">,</mo><mi id=\"S3.p3.1.m1.2.2\" xref=\"S3.p3.1.m1.2.2.cmml\">d</mi><mo id=\"S3.p3.1.m1.5.5.1.1.1.1.3.2.2\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.3.1.cmml\">,</mo><mi id=\"S3.p3.1.m1.3.3\" xref=\"S3.p3.1.m1.3.3.cmml\">s</mi><mo id=\"S3.p3.1.m1.5.5.1.1.1.1.3.2.3\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.3.1.cmml\">,</mo><mi id=\"S3.p3.1.m1.4.4\" xref=\"S3.p3.1.m1.4.4.cmml\">c</mi></mrow></mrow><mo stretchy=\"false\" id=\"S3.p3.1.m1.5.5.1.1.1.3\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.p3.1.m1.5b\"><apply id=\"S3.p3.1.m1.5.5.cmml\" xref=\"S3.p3.1.m1.5.5\"><ci id=\"S3.p3.1.m1.5.5.2.cmml\" xref=\"S3.p3.1.m1.5.5.2\">→</ci><apply id=\"S3.p3.1.m1.5.5.3.cmml\" xref=\"S3.p3.1.m1.5.5.3\"><times id=\"S3.p3.1.m1.5.5.3.1.cmml\" xref=\"S3.p3.1.m1.5.5.3.1\"></times><apply id=\"S3.p3.1.m1.5.5.3.2.cmml\" xref=\"S3.p3.1.m1.5.5.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.p3.1.m1.5.5.3.2.1.cmml\" xref=\"S3.p3.1.m1.5.5.3.2\">superscript</csymbol><ci id=\"S3.p3.1.m1.5.5.3.2.2.cmml\" xref=\"S3.p3.1.m1.5.5.3.2.2\">𝑒</ci><plus id=\"S3.p3.1.m1.5.5.3.2.3.cmml\" xref=\"S3.p3.1.m1.5.5.3.2.3\"></plus></apply><apply id=\"S3.p3.1.m1.5.5.3.3.cmml\" xref=\"S3.p3.1.m1.5.5.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.p3.1.m1.5.5.3.3.1.cmml\" xref=\"S3.p3.1.m1.5.5.3.3\">superscript</csymbol><ci id=\"S3.p3.1.m1.5.5.3.3.2.cmml\" xref=\"S3.p3.1.m1.5.5.3.3.2\">𝑒</ci><minus id=\"S3.p3.1.m1.5.5.3.3.3.cmml\" xref=\"S3.p3.1.m1.5.5.3.3.3\"></minus></apply></apply><apply id=\"S3.p3.1.m1.5.5.1.cmml\" xref=\"S3.p3.1.m1.5.5.1\"><times id=\"S3.p3.1.m1.5.5.1.2.cmml\" xref=\"S3.p3.1.m1.5.5.1.2\"></times><ci id=\"S3.p3.1.m1.5.5.1.3.cmml\" xref=\"S3.p3.1.m1.5.5.1.3\">𝑞</ci><apply id=\"S3.p3.1.m1.5.5.1.4.cmml\" xref=\"S3.p3.1.m1.5.5.1.4\"><ci id=\"S3.p3.1.m1.5.5.1.4.1.cmml\" xref=\"S3.p3.1.m1.5.5.1.4.1\">¯</ci><ci id=\"S3.p3.1.m1.5.5.1.4.2.cmml\" xref=\"S3.p3.1.m1.5.5.1.4.2\">𝑞</ci></apply><apply id=\"S3.p3.1.m1.5.5.1.1.1.1.cmml\" xref=\"S3.p3.1.m1.5.5.1.1.1\"><eq id=\"S3.p3.1.m1.5.5.1.1.1.1.1.cmml\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.1\"></eq><ci id=\"S3.p3.1.m1.5.5.1.1.1.1.2.cmml\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.2\">𝑞</ci><list id=\"S3.p3.1.m1.5.5.1.1.1.1.3.1.cmml\" xref=\"S3.p3.1.m1.5.5.1.1.1.1.3.2\"><ci id=\"S3.p3.1.m1.1.1.cmml\" xref=\"S3.p3.1.m1.1.1\">𝑢</ci><ci id=\"S3.p3.1.m1.2.2.cmml\" xref=\"S3.p3.1.m1.2.2\">𝑑</ci><ci id=\"S3.p3.1.m1.3.3.cmml\" xref=\"S3.p3.1.m1.3.3\">𝑠</ci><ci id=\"S3.p3.1.m1.4.4.cmml\" xref=\"S3.p3.1.m1.4.4\">𝑐</ci></list></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p3.1.m1.5c\">e^{+}e^{-}\\rightarrow~{}q\\bar{q}~{}(q=u,d,s,c)</annotation></semantics></math> continuum process. To suppress this, we develop a Fisher discriminant (<math id=\"S3.p3.2.m2.1\" class=\"ltx_Math\" alttext=\"T_{c}\" display=\"inline\"><semantics id=\"S3.p3.2.m2.1a\"><msub id=\"S3.p3.2.m2.1.1\" xref=\"S3.p3.2.m2.1.1.cmml\"><mi id=\"S3.p3.2.m2.1.1.2\" xref=\"S3.p3.2.m2.1.1.2.cmml\">T</mi><mi id=\"S3.p3.2.m2.1.1.3\" xref=\"S3.p3.2.m2.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.p3.2.m2.1b\"><apply id=\"S3.p3.2.m2.1.1.cmml\" xref=\"S3.p3.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.p3.2.m2.1.1.1.cmml\" xref=\"S3.p3.2.m2.1.1\">subscript</csymbol><ci id=\"S3.p3.2.m2.1.1.2.cmml\" xref=\"S3.p3.2.m2.1.1.2\">𝑇</ci><ci id=\"S3.p3.2.m2.1.1.3.cmml\" xref=\"S3.p3.2.m2.1.1.3\">𝑐</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.p3.2.m2.1c\">T_{c}</annotation></semantics></math>) out of 16 modified Fox-Wolfram moments [16] combined with the cosine of the polar angle of the <math id=\"S3.p3.3.m3.1\" class=\"ltx_Math\" alttext=\"B\" displa\n",
      "\n",
      "文本已分成 2 块。\n",
      "分块结果:\n",
      "Chunk 1: page_content='\"> 5.24 GeV superscript 𝑐 2 M_{bc}>5.24~{}\\mathrm{GeV/}c^{2}  and  − 0.3 < Δ ​ E < 0.3 ​ GeV 0.3 Δ 𝐸 0.3 GeV -0.3<\\Delta E<0.3~{}\\rm{GeV} , while the signal-enhanced region is defined as  5.27 < M b ​ c < 5.29 ​ GeV / c 2 5.27 subscript 𝑀 𝑏 𝑐 5.29 GeV superscript 𝑐 2 5.27<M_{bc}<5.29~{}\\mathrm{GeV/}c^{2}  and  − 0.05 < Δ ​ E < 0.05 ​ GeV 0.05 Δ 𝐸 0.05 GeV -0.05<\\Delta E<0.05~{}\\rm{GeV} . When multiple  B 𝐵 B  candidates are present in an event, we choose the candidate with the best fit quality from the  B 𝐵 B  vertex fit. This is done for 19% of events and the selection efficiency is 92%. \n",
      " \n",
      " The dominant backgrounds are from  e + ​ e − → q ​ q ¯ ​ ( q = u , d , s , c ) → superscript 𝑒 superscript 𝑒 𝑞 ¯ 𝑞 𝑞 𝑢 𝑑 𝑠 𝑐 e^{+}e^{-}\\rightarrow q\\bar{q}~{}(q=u,d,s,c)  continuum process. The  B ​ B ¯ 𝐵 ¯ 𝐵 B\\bar{B}  events are spherical in shape whereas the particles from continuum events are collimated into two back-to-back jets. We make use of this difference in event topology by using a neural network [8] to combine several shape variables along with other properties of the event that distinguish  q ​ q ¯ 𝑞 ¯ 𝑞 q\\bar{q}  from  B ​ B ¯ 𝐵 ¯ 𝐵 B\\bar{B}  events. A requirement on the neural network output ( C N ​ N subscript 𝐶 𝑁 𝑁 C_{NN}   > 0.88 absent 0.88 >0.88 ) is applied to suppress continuum background. This selection requirement is optimized by maximizing a figure of merit defined as  N S N S + N B subscript 𝑁 𝑆 subscript 𝑁 𝑆 subscript 𝑁 𝐵 \\frac{N_{S}}{N_{S}+N_{B}} , where  N S subscript 𝑁 𝑆 N_{S}  ( N B subscript 𝑁 𝐵 N_{B} ) denotes the expected number of signal (background) events in the signal-enhanced region. Background contributions from  B 𝐵 B  decays mediated by the dominant  b → c → 𝑏 𝑐 b\\rightarrow c  transition are investigated with an MC sample of such decays. To suppress these backgrounds, candidates for which the invariant mass of the  K + ​ K − superscript 𝐾 superscript 𝐾 K^{+}K^{-}  or  K + ​ π − superscript 𝐾 superscript 𝜋 K^{+}\\pi^{-}  system lies in the range 185–188 MeV/ c 2 superscript 𝑐 2 c^{2}  are removed. This selection window corresponds to  ± 3.75 ​ σ plus-or-minus 3.75 𝜎 \\pm 3.75\\sigma  around the nominal  D 0 superscript 𝐷 0 D^{0}  mass [9], where  σ 𝜎 \\sigma  is the mass resolution. Backgrounds from charmless  B 𝐵 B  decays are studied with a large MC sample, where one of the  B 𝐵 B  mesons decays via a process with a known branching fraction. The study reveals that a few modes contribute in the  M b ​ c subscript 𝑀 𝑏 𝑐 M_{bc}  signal region with a corresponding  Δ ​ E Δ 𝐸 \\Delta E  peak, denoted collectively as the  “rare peaking”  background. These peaking backgrounds are due to  K − π 𝐾 𝜋 K-\\pi  misidentification, which consist of  B + → K + ​ K − ​ K + → superscript 𝐵 superscript 𝐾 superscript 𝐾 superscript 𝐾 B^{+}\\rightarrow K^{+}K^{-}K^{+} ,  B + → K + ​ π − ​ π + → superscript 𝐵 superscript 𝐾 superscript 𝜋 superscript 𝜋 B^{+}\\rightarrow K^{+}\\pi^{-}\\pi^{+} , and their intermediate resonant modes. Events that remain after removing the peaking components are called the  “rare combinatorial”  background.\n",
      " \n",
      " \n",
      " \n",
      " The signal yield is extracted by performing a two-dimensional unbinned extended maximum likelihood fit in  M b ​ c subscript 𝑀 𝑏 𝑐 M_{bc}  and  Δ ​ E Δ 𝐸 \\Delta E  with the likelihood defined as \n",
      " \n",
      " \n",
      " \n",
      " ℒ = e − ∑ j N j N ! ∏ i [ ∑ j N j 𝒫 j i ] , where 𝒫 j i = 1 2 ( 1 − q i . A C ​ P ) × 𝒫 j ( M b ​ c i , Δ E i ) , \\mathcal{L}=\\dfrac{e^{-\\sum_{j}N_{j}}}{N!}\\prod_{i}\\Big{[}\\sum_{j}N_{j}\\mathcal{P}_{j}^{i}\\Big{]},\\ \\ \\mathrm{where}\\ \\ \\ \\mathcal{P}_{j}^{i}=\\dfrac{1}{2}(1-q_{i}.A_{CP})\\times\\mathcal{P}_{j}(M_{bc}^{i},\\Delta E^{i}), \n",
      " \n",
      " (1) \n",
      " \n",
      " \n",
      " where  i 𝑖 i  denotes the event index,  N j subscript 𝑁 𝑗 N_{j}  is the yield for the component  j 𝑗 j ,  q i subscript 𝑞 𝑖 q_{i}  is the charge of  B 𝐵 B  candidates ( q i = ± 1 subscript 𝑞 𝑖 plus-or-minus 1 q_{i}=\\pm 1  for  B ± superscript 𝐵 plus-or-minus B^{\\pm} ), and  𝒫 j subscript 𝒫 𝑗 \\mathcal{P}_{j}  is the probability density function (PDF) corresponding to the component  j 𝑗 j . Figure 1 shows the fit results of first two  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}}  bins in the signal-enhanced region. The resulting branching fraction and  C ​ P 𝐶 𝑃 CP  asymmetry are [10] \n",
      " \n",
      " \n",
      " \n",
      " ℬ ​ ( B + → K + ​ K − ​ π + ) = ( 5.38 ± 0.40 ± 0.35 ) × 10 − 6 ℬ → superscript 𝐵 superscript 𝐾 superscript 𝐾 superscript 𝜋 plus-or-minus 5.38 0.40 0.35 superscript 10 6 \\mathcal{B}(B^{+}\\rightarrow K^{+}K^{-}\\pi^{+})=(5.38\\pm 0.40\\pm 0.35)\\times 10^{-6} \n",
      " \n",
      " (2) \n",
      " \n",
      " \n",
      " and \n",
      " \n",
      " \n",
      " \n",
      " A C ​ P = − 0.182 ± 0.071 ± 0.016 , subscript 𝐴 𝐶 𝑃 plus-or-minus 0.182 0.071 0.016 A_{CP}=-0.182\\pm 0.071\\pm 0.016, \n",
      " \n",
      " (3) \n",
      " \n",
      " \n",
      " where the quoted uncertainties are statistical and systematic, respectively. \n",
      " \n",
      " \n",
      " To investigate the localized  C ​ P 𝐶 𝑃 CP  asymmetry in the low  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}}  region, we determine the signal yield and  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  in bins of  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}} . The fitted results are shown in Table 1 and Fig. 2, where an excess of signal yield as well as a large  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  are seen in  M K + ​ K − < 1.5 ​ GeV / c 2 subscript 𝑀 superscript 𝐾 superscript 𝐾 1.5 GeV superscript 𝑐 2 M_{K^{+}K^{-}}<1.5~{}\\mathrm{GeV/}c^{2} , confirming the observations by BaBar and LHCb. We find strong evidence for a large  C ​ P 𝐶 𝑃 CP  asymmetry of  − 0.90 ± 0.17 ± 0.03 plus-or-minus 0.90 0.17 0.03 -0.90\\pm 0.17\\pm 0.03  with a significance of  4.8 ​ σ 4.8 𝜎 4.8\\sigma  for  M K + ​ K − < 1.1 ​ GeV / c 2 subscript 𝑀 superscript 𝐾 superscript 𝐾 1.1 GeV superscript 𝑐 2 M_{K^{+}K^{-}}<1.1~{}\\mathrm{GeV/}c^{2} . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Figure 1:  Signal-enhanced projections of the  M b ​ c subscript 𝑀 𝑏 𝑐 M_{bc} - Δ ​ E Δ 𝐸 \\Delta E  fit to data in the first (left) and second (right)  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}}  bins. Points with error bars are the data, red solid curves are the fit result, blue solid curves are the sum of the signal and the self cross-feed, cyan dotted curves are the continuum background, brown dash dotted curves are the generic  B 𝐵 B  backgrounds, and green dashed curves are the rare  B 𝐵 B  backgrounds. \n",
      " \n",
      " \n",
      " Table 1:  Signal yield, efficiency, differential branching fraction, and  𝒜 C ​ P subscript 𝒜 𝐶 𝑃 \\mathcal{A}_{CP}  for individual  M KK subscript 𝑀 KK M_{\\mathrm{{KK}}}  bins  \n",
      " \n",
      " \n",
      " \n",
      " M K + ​ K − subscript M superscript K superscript K \\textit{M}_{\\mathrm{{K^{+}K^{-}}}} \n",
      " N sig subscript 𝑁 sig N_{\\mathrm{{sig}}} \n",
      " Eff. (%) \n",
      " d ℬ / dM ( × 10 − 7 ) \\textit{d}\\mathrm{\\mathcal{B}/\\textit{dM}~{}(\\times 10^{-7}}) \n",
      " A C ​ P subscript 𝐴 𝐶 𝑃 {A}_{CP} \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " ( GeV / c 2 ) GeV superscript c 2 \\mathrm{(GeV/c^{2})} \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " 0.8–1.1 \n",
      "    59.8 ± 11.4 ± 2.6 plus-or-minus 59.8 11.4 2.6 59.8\\pm 11.4\\pm 2.6 \n",
      " \n",
      " 19.7 \n",
      " 14.0 ± 2.7 ± 0.8 plus-or-minus 14.0 2.7 0.8 14.0\\pm 2.7\\pm 0.8 \n",
      " − 0.90 ± 0.17 ± 0.03 plus-or-minus 0.90 0.17 0.03 -0.90\\pm 0.17\\pm 0.03 \n",
      " \n",
      " \n",
      " 1.1–1.5 \n",
      " 212.4 ± 21.3 ± 6.6 plus-or-minus 212.4 21.3 6.6 212.4\\pm 21.3\\pm 6.6 \n",
      " 19.3 \n",
      " 37.8 ± 3.8 ± 1.9 plus-or-minus 37.8 3.8 1.9 37.8\\pm 3.8\\pm 1.9 \n",
      " − 0.16 ± 0.10 ± 0.01 plus-or-minus 0.16 0.10 0.01 -0.16\\pm 0.10\\pm 0.01 \n",
      " \n",
      " \n",
      " 1.5–2.5 \n",
      "   113.5 ± 26.7 ± 18.0 plus-or-minus 113.5 26.7 18.0 113.5\\pm 26.7\\pm 18.0 \n",
      " \n",
      " 15.6 \n",
      " 10.0 ± 2.3 ± 1.6 plus-or-minus 10.0 2.3 1.6 10.0\\pm 2.3\\pm 1.6 \n",
      " − 0.15 ± 0.23 ± 0.03 plus-or-minus 0.15 0.23 0.03 -0.15\\pm 0.23\\pm 0.03 \n",
      " \n",
      " \n",
      " 2.5–3.5 \n",
      " 110.1 ± 17.6 ± 4.1 plus-or-minus 110.1 17.6 4.1 110.1\\pm 17.6\\pm 4.1 \n",
      " 15.1 \n",
      " 10.0 ± 1.6 ± 0.5 plus-or-minus 10.0 1.6 0.5 10.0\\pm 1.6\\pm 0.5 \n",
      " − 0.09 ± 0.16 ± 0.01 plus-or-minus 0.09 0.16 0.01 -0.09\\pm 0.16\\pm 0.01 \n",
      " \n",
      " \n",
      " 3.5–5.3 \n",
      "   172.6 ± 25.7 ± 6.87 plus-or-minus 172.6 25.7 6.87 172.6\\pm 25.7\\pm 6.87 \n",
      " \n",
      " 16.3 \n",
      "    8.1 ± 1.2 ± 0.5 plus-or-minus 8.1 1.2 0.5 8.1\\pm 1.2\\pm 0.5 \n",
      " \n",
      " − 0.05 ± 0.15 ± 0.00 plus-or-minus 0.05 0.15 0.00 -0.05\\pm 0.15\\pm 0.00 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Figure 2:  Measured differential branching fractions (left) and  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  (right) as a function of  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}} . Each point is obtained from a two-dimensional fit with systematic uncertainty included. Red squares with error bars in the left plot show the expected signal distribution for a three-body phase space MC sample. Note that the phase space hypothesis is rescaled to the experimentally observed total  B + → K + ​ K − ​ π + → superscript 𝐵 superscript 𝐾 superscript 𝐾 superscript 𝜋 B^{+}\\rightarrow K^{+}K^{-}\\pi^{+}  signal yield.' metadata={'Header 1': '#TITLE#'}\n",
      "Chunk 2: page_content='3  B 0 → π 0 ​ π 0 → superscript 𝐵 0 superscript 𝜋 0 superscript 𝜋 0 B^{0}\\rightarrow\\pi^{0}\\pi^{0}  decay \n",
      " \n",
      " One of the proposed techniques to measure  ϕ 2 subscript italic-ϕ 2 \\phi_{2}  is to perform an isospin analysis of the entire  B → π ​ π → 𝐵 𝜋 𝜋 B\\rightarrow\\pi\\pi  system [11]. This requires measurements of  ℬ ℬ \\mathcal{B}  and  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  for  B + → π + ​ π 0 → superscript 𝐵 superscript 𝜋 superscript 𝜋 0 B^{+}\\rightarrow\\pi^{+}\\pi^{0}  and  B 0 → π 0 ​ π 0 → superscript 𝐵 0 superscript 𝜋 0 superscript 𝜋 0 B^{0}\\rightarrow\\pi^{0}\\pi^{0}  decays, along with that of  ℬ ℬ \\mathcal{B}  and time-dependent  C ​ P 𝐶 𝑃 CP  asymmetry for the  B 0 → π + ​ π − → superscript 𝐵 0 superscript 𝜋 superscript 𝜋 B^{0}\\rightarrow\\pi^{+}\\pi^{-}  decay. One needs all these observables in order to determine  ϕ 2 subscript italic-ϕ 2 \\phi_{2}  as electroweak tree and loop processes contribute with different phases to  B → π ​ π → 𝐵 𝜋 𝜋 B\\rightarrow\\pi\\pi  decays. The  ℬ ℬ \\mathcal{B}  and  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  for  B 0 → π 0 ​ π 0 → superscript 𝐵 0 superscript 𝜋 0 superscript 𝜋 0 B^{0}\\rightarrow\\pi^{0}\\pi^{0}  are the least well determined among the  B → π ​ π → 𝐵 𝜋 𝜋 B\\rightarrow\\pi\\pi  decays. This decay is also important to probe the disagreement between quantum-chromodynamics-based factorization, which predicts  ℬ ℬ \\mathcal{B}  below  1 × 10 − 6 1 superscript 10 6 1\\times 10^{-6}  [12, 13], and previous measurements from Belle and BaBar of  ( 1.8 − 2.3 ) × 10 − 6 1.8 2.3 superscript 10 6 (1.8-2.3)\\times 10^{-6}  [14,15]. Our study is based on a data sample recorded at the  Υ ​ ( 4 ​ S ) Υ 4 𝑆 \\Upsilon(4S)  resonance with the Belle detector comprising of  752 × 10 6 752 superscript 10 6 752\\times 10^{6}   B ​ B ¯ 𝐵 ¯ 𝐵 B\\bar{B}  pairs, which corresponds to an integrated luminosity of 693  fb -1 , and an additional 83.35  fb -1  recorded 60 MeV below the  Υ ​ ( 4 ​ S ) Υ 4 𝑆 \\Upsilon(4S)  resonance. \n",
      " \n",
      " \n",
      " We reconstruct the signal  B 0 superscript 𝐵 0 B^{0}  candidate from a pair of  π 0 superscript 𝜋 0 \\pi^{0}  candidates, each subsequently decaying to two photons. In addition to photons reconstructed from clusters in the electromagnetic calorimeter (ECL) that do not match any charged track, photons that convert to  e + ​ e − superscript 𝑒 superscript 𝑒 e^{+}e^{-}  pairs in the silicon vertex detector (SVD) are recovered and reconstructed as  π 0 → γ ​ e + ​ e − → superscript 𝜋 0 𝛾 superscript 𝑒 superscript 𝑒 \\pi^{0}\\rightarrow\\gamma e^{+}e^{-} . This provides a 5.3% increase in detection efficiency. These photons must have an energy greater than 50 (100) MeV in the barrel (endcap) region of the ECL. The invariant mass of the two-photon combination\n",
      "must lie in the range  115 < m γ ​ γ < 152 ​ MeV / c 2 115 subscript m 𝛾 𝛾 152 MeV superscript 𝑐 2 \\mathrm{115}<\\mathrm{m_{\\gamma\\gamma}<152~{}MeV/}c^{2} , corresponding to  ± 2.6 ​ σ plus-or-minus 2.6 𝜎 \\pm 2.6\\sigma  around the nominal  π 0 superscript 𝜋 0 \\pi^{0}  mass [9]. As in the case of  B + → K + ​ K − ​ π + → superscript 𝐵 superscript 𝐾 superscript 𝐾 superscript 𝜋 B^{+}\\rightarrow K^{+}K^{-}\\pi^{+} , two kinematic variables  Δ ​ E Δ 𝐸 \\Delta E  and  M b ​ c subscript 𝑀 𝑏 𝑐 M_{bc}  are used to select the signal candidates. All candidates satisfying  M b ​ c > 5.26 ​ GeV / c 2 subscript 𝑀 𝑏 𝑐 5.26 GeV superscript 𝑐 2 M_{bc}>5.26~{}\\mathrm{GeV}/c^{2}  and  − 0.3 < Δ ​ E < 0.2 ​ GeV 0.3 Δ 𝐸 0.2 GeV -0.3<\\Delta E<0.2~{}\\rm{GeV}  are retained for further analysis. For 7.2% of the events, there are multiple  B 0 superscript 𝐵 0 B^{0}  candidates in which case we choose the one that minimizes the deviation of the two  π 0 superscript 𝜋 0 \\pi^{0} ’s reconstructed invariant masses from the world average [9]. This criterion selects the true  B 0 superscript 𝐵 0 B^{0}  candidate in 90% of MC events. \n",
      " \n",
      " \n",
      " The dominant background is from  e + ​ e − → q ​ q ¯ ​ ( q = u , d , s , c ) → superscript 𝑒 superscript 𝑒 𝑞 ¯ 𝑞 𝑞 𝑢 𝑑 𝑠 𝑐 e^{+}e^{-}\\rightarrow~{}q\\bar{q}~{}(q=u,d,s,c)  continuum process. To suppress this, we develop a Fisher discriminant ( T c subscript 𝑇 𝑐 T_{c} ) out of 16 modified Fox-Wolfram moments [16] combined with the cosine of the polar angle of the' metadata={'Header 2': '3 B0→π0\\u200bπ0→superscript𝐵0superscript𝜋0superscript𝜋0B^{0}\\\\rightarrow\\\\pi^{0}\\\\pi^{0} decay'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='white-space: pre-wrap; background-color: #f5f5f5; padding: 10px; border-radius: 5px;'><span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">&quot;&gt; 5.24 GeV superscript 𝑐 2 M_{bc}&gt;5.24~{}\\mathrm{GeV/}c^{2}  and  − 0.3 &lt; Δ ​ E &lt; 0.3 ​ GeV 0.3 Δ 𝐸 0.3 GeV -0.3&lt;\\Delta E&lt;0.3~{}\\rm{GeV} , while the signal-enhanced region is defined as  5.27 &lt; M b ​ c &lt; 5.29 ​ GeV / c 2 5.27 subscript 𝑀 𝑏 𝑐 5.29 GeV superscript 𝑐 2 5.27&lt;M_{bc}&lt;5.29~{}\\mathrm{GeV/}c^{2}  and  − 0.05 &lt; Δ ​ E &lt; 0.05 ​ GeV 0.05 Δ 𝐸 0.05 GeV -0.05&lt;\\Delta E&lt;0.05~{}\\rm{GeV} . When multiple  B 𝐵 B  candidates are present in an event, we choose the candidate with the best fit quality from the  B 𝐵 B  vertex fit. This is done for 19% of events and the selection efficiency is 92%. <br> <br> The dominant backgrounds are from  e + ​ e − → q ​ q ¯ ​ ( q = u , d , s , c ) → superscript 𝑒 superscript 𝑒 𝑞 ¯ 𝑞 𝑞 𝑢 𝑑 𝑠 𝑐 e^{+}e^{-}\\rightarrow q\\bar{q}~{}(q=u,d,s,c)  continuum process. The  B ​ B ¯ 𝐵 ¯ 𝐵 B\\bar{B}  events are spherical in shape whereas the particles from continuum events are collimated into two back-to-back jets. We make use of this difference in event topology by using a neural network [8] to combine several shape variables along with other properties of the event that distinguish  q ​ q ¯ 𝑞 ¯ 𝑞 q\\bar{q}  from  B ​ B ¯ 𝐵 ¯ 𝐵 B\\bar{B}  events. A requirement on the neural network output ( C N ​ N subscript 𝐶 𝑁 𝑁 C_{NN}   &gt; 0.88 absent 0.88 &gt;0.88 ) is applied to suppress continuum background. This selection requirement is optimized by maximizing a figure of merit defined as  N S N S + N B subscript 𝑁 𝑆 subscript 𝑁 𝑆 subscript 𝑁 𝐵 \\frac{N_{S}}{N_{S}+N_{B}} , where  N S subscript 𝑁 𝑆 N_{S}  ( N B subscript 𝑁 𝐵 N_{B} ) denotes the expected number of signal (background) events in the signal-enhanced region. Background contributions from  B 𝐵 B  decays mediated by the dominant  b → c → 𝑏 𝑐 b\\rightarrow c  transition are investigated with an MC sample of such decays. To suppress these backgrounds, candidates for which the invariant mass of the  K + ​ K − superscript 𝐾 superscript 𝐾 K^{+}K^{-}  or  K + ​ π − superscript 𝐾 superscript 𝜋 K^{+}\\pi^{-}  system lies in the range 185–188 MeV/ c 2 superscript 𝑐 2 c^{2}  are removed. This selection window corresponds to  ± 3.75 ​ σ plus-or-minus 3.75 𝜎 \\pm 3.75\\sigma  around the nominal  D 0 superscript 𝐷 0 D^{0}  mass [9], where  σ 𝜎 \\sigma  is the mass resolution. Backgrounds from charmless  B 𝐵 B  decays are studied with a large MC sample, where one of the  B 𝐵 B  mesons decays via a process with a known branching fraction. The study reveals that a few modes contribute in the  M b ​ c subscript 𝑀 𝑏 𝑐 M_{bc}  signal region with a corresponding  Δ ​ E Δ 𝐸 \\Delta E  peak, denoted collectively as the  “rare peaking”  background. These peaking backgrounds are due to  K − π 𝐾 𝜋 K-\\pi  misidentification, which consist of  B + → K + ​ K − ​ K + → superscript 𝐵 superscript 𝐾 superscript 𝐾 superscript 𝐾 B^{+}\\rightarrow K^{+}K^{-}K^{+} ,  B + → K + ​ π − ​ π + → superscript 𝐵 superscript 𝐾 superscript 𝜋 superscript 𝜋 B^{+}\\rightarrow K^{+}\\pi^{-}\\pi^{+} , and their intermediate resonant modes. Events that remain after removing the peaking components are called the  “rare combinatorial”  background.<br> <br> <br> <br> The signal yield is extracted by performing a two-dimensional unbinned extended maximum likelihood fit in  M b ​ c subscript 𝑀 𝑏 𝑐 M_{bc}  and  Δ ​ E Δ 𝐸 \\Delta E  with the likelihood defined as <br> <br> <br> <br> ℒ = e − ∑ j N j N ! ∏ i [ ∑ j N j 𝒫 j i ] , where 𝒫 j i = 1 2 ( 1 − q i . A C ​ P ) × 𝒫 j ( M b ​ c i , Δ E i ) , \\mathcal{L}=\\dfrac{e^{-\\sum_{j}N_{j}}}{N!}\\prod_{i}\\Big{[}\\sum_{j}N_{j}\\mathcal{P}_{j}^{i}\\Big{]},\\ \\ \\mathrm{where}\\ \\ \\ \\mathcal{P}_{j}^{i}=\\dfrac{1}{2}(1-q_{i}.A_{CP})\\times\\mathcal{P}_{j}(M_{bc}^{i},\\Delta E^{i}), <br> <br> (1) <br> <br> <br> where  i 𝑖 i  denotes the event index,  N j subscript 𝑁 𝑗 N_{j}  is the yield for the component  j 𝑗 j ,  q i subscript 𝑞 𝑖 q_{i}  is the charge of  B 𝐵 B  candidates ( q i = ± 1 subscript 𝑞 𝑖 plus-or-minus 1 q_{i}=\\pm 1  for  B ± superscript 𝐵 plus-or-minus B^{\\pm} ), and  𝒫 j subscript 𝒫 𝑗 \\mathcal{P}_{j}  is the probability density function (PDF) corresponding to the component  j 𝑗 j . Figure 1 shows the fit results of first two  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}}  bins in the signal-enhanced region. The resulting branching fraction and  C ​ P 𝐶 𝑃 CP  asymmetry are [10] <br> <br> <br> <br> ℬ ​ ( B + → K + ​ K − ​ π + ) = ( 5.38 ± 0.40 ± 0.35 ) × 10 − 6 ℬ → superscript 𝐵 superscript 𝐾 superscript 𝐾 superscript 𝜋 plus-or-minus 5.38 0.40 0.35 superscript 10 6 \\mathcal{B}(B^{+}\\rightarrow K^{+}K^{-}\\pi^{+})=(5.38\\pm 0.40\\pm 0.35)\\times 10^{-6} <br> <br> (2) <br> <br> <br> and <br> <br> <br> <br> A C ​ P = − 0.182 ± 0.071 ± 0.016 , subscript 𝐴 𝐶 𝑃 plus-or-minus 0.182 0.071 0.016 A_{CP}=-0.182\\pm 0.071\\pm 0.016, <br> <br> (3) <br> <br> <br> where the quoted uncertainties are statistical and systematic, respectively. <br> <br> <br> To investigate the localized  C ​ P 𝐶 𝑃 CP  asymmetry in the low  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}}  region, we determine the signal yield and  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  in bins of  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}} . The fitted results are shown in Table 1 and Fig. 2, where an excess of signal yield as well as a large  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  are seen in  M K + ​ K − &lt; 1.5 ​ GeV / c 2 subscript 𝑀 superscript 𝐾 superscript 𝐾 1.5 GeV superscript 𝑐 2 M_{K^{+}K^{-}}&lt;1.5~{}\\mathrm{GeV/}c^{2} , confirming the observations by BaBar and LHCb. We find strong evidence for a large  C ​ P 𝐶 𝑃 CP  asymmetry of  − 0.90 ± 0.17 ± 0.03 plus-or-minus 0.90 0.17 0.03 -0.90\\pm 0.17\\pm 0.03  with a significance of  4.8 ​ σ 4.8 𝜎 4.8\\sigma  for  M K + ​ K − &lt; 1.1 ​ GeV / c 2 subscript 𝑀 superscript 𝐾 superscript 𝐾 1.1 GeV superscript 𝑐 2 M_{K^{+}K^{-}}&lt;1.1~{}\\mathrm{GeV/}c^{2} . <br> <br> <br> <br> <br> <br> <br> <br> Figure 1:  Signal-enhanced projections of the  M b ​ c subscript 𝑀 𝑏 𝑐 M_{bc} - Δ ​ E Δ 𝐸 \\Delta E  fit to data in the first (left) and second (right)  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}}  bins. Points with error bars are the data, red solid curves are the fit result, blue solid curves are the sum of the signal and the self cross-feed, cyan dotted curves are the continuum background, brown dash dotted curves are the generic  B 𝐵 B  backgrounds, and green dashed curves are the rare  B 𝐵 B  backgrounds. <br> <br> <br> Table 1:  Signal yield, efficiency, differential branching fraction, and  𝒜 C ​ P subscript 𝒜 𝐶 𝑃 \\mathcal{A}_{CP}  for individual  M KK subscript 𝑀 KK M_{\\mathrm{{KK}}}  bins  <br> <br> <br> <br> M K + ​ K − subscript M superscript K superscript K \\textit{M}_{\\mathrm{{K^{+}K^{-}}}} <br> N sig subscript 𝑁 sig N_{\\mathrm{{sig}}} <br> Eff. (%) <br> d ℬ / dM ( × 10 − 7 ) \\textit{d}\\mathrm{\\mathcal{B}/\\textit{dM}~{}(\\times 10^{-7}}) <br> A C ​ P subscript 𝐴 𝐶 𝑃 {A}_{CP} <br> <br> <br> <br> <br> ( GeV / c 2 ) GeV superscript c 2 \\mathrm{(GeV/c^{2})} <br> <br> <br> <br> <br> <br> <br> 0.8–1.1 <br>    59.8 ± 11.4 ± 2.6 plus-or-minus 59.8 11.4 2.6 59.8\\pm 11.4\\pm 2.6 <br> <br> 19.7 <br> 14.0 ± 2.7 ± 0.8 plus-or-minus 14.0 2.7 0.8 14.0\\pm 2.7\\pm 0.8 <br> − 0.90 ± 0.17 ± 0.03 plus-or-minus 0.90 0.17 0.03 -0.90\\pm 0.17\\pm 0.03 <br> <br> <br> 1.1–1.5 <br> 212.4 ± 21.3 ± 6.6 plus-or-minus 212.4 21.3 6.6 212.4\\pm 21.3\\pm 6.6 <br> 19.3 <br> 37.8 ± 3.8 ± 1.9 plus-or-minus 37.8 3.8 1.9 37.8\\pm 3.8\\pm 1.9 <br> − 0.16 ± 0.10 ± 0.01 plus-or-minus 0.16 0.10 0.01 -0.16\\pm 0.10\\pm 0.01 <br> <br> <br> 1.5–2.5 <br>   113.5 ± 26.7 ± 18.0 plus-or-minus 113.5 26.7 18.0 113.5\\pm 26.7\\pm 18.0 <br> <br> 15.6 <br> 10.0 ± 2.3 ± 1.6 plus-or-minus 10.0 2.3 1.6 10.0\\pm 2.3\\pm 1.6 <br> − 0.15 ± 0.23 ± 0.03 plus-or-minus 0.15 0.23 0.03 -0.15\\pm 0.23\\pm 0.03 <br> <br> <br> 2.5–3.5 <br> 110.1 ± 17.6 ± 4.1 plus-or-minus 110.1 17.6 4.1 110.1\\pm 17.6\\pm 4.1 <br> 15.1 <br> 10.0 ± 1.6 ± 0.5 plus-or-minus 10.0 1.6 0.5 10.0\\pm 1.6\\pm 0.5 <br> − 0.09 ± 0.16 ± 0.01 plus-or-minus 0.09 0.16 0.01 -0.09\\pm 0.16\\pm 0.01 <br> <br> <br> 3.5–5.3 <br>   172.6 ± 25.7 ± 6.87 plus-or-minus 172.6 25.7 6.87 172.6\\pm 25.7\\pm 6.87 <br> <br> 16.3 <br>    8.1 ± 1.2 ± 0.5 plus-or-minus 8.1 1.2 0.5 8.1\\pm 1.2\\pm 0.5 <br> <br> − 0.05 ± 0.15 ± 0.00 plus-or-minus 0.05 0.15 0.00 -0.05\\pm 0.15\\pm 0.00 <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> Figure 2:  Measured differential branching fractions (left) and  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  (right) as a function of  M K + ​ K − subscript 𝑀 superscript 𝐾 superscript 𝐾 M_{K^{+}K^{-}} . Each point is obtained from a two-dimensional fit with systematic uncertainty included. Red squares with error bars in the left plot show the expected signal distribution for a three-body phase space MC sample. Note that the phase space hypothesis is rescaled to the experimentally observed total  B + → K + ​ K − ​ π + → superscript 𝐵 superscript 𝐾 superscript 𝐾 superscript 𝜋 B^{+}\\rightarrow K^{+}K^{-}\\pi^{+}  signal yield.</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">3  B 0 → π 0 ​ π 0 → superscript 𝐵 0 superscript 𝜋 0 superscript 𝜋 0 B^{0}\\rightarrow\\pi^{0}\\pi^{0}  decay <br> <br> One of the proposed techniques to measure  ϕ 2 subscript italic-ϕ 2 \\phi_{2}  is to perform an isospin analysis of the entire  B → π ​ π → 𝐵 𝜋 𝜋 B\\rightarrow\\pi\\pi  system [11]. This requires measurements of  ℬ ℬ \\mathcal{B}  and  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  for  B + → π + ​ π 0 → superscript 𝐵 superscript 𝜋 superscript 𝜋 0 B^{+}\\rightarrow\\pi^{+}\\pi^{0}  and  B 0 → π 0 ​ π 0 → superscript 𝐵 0 superscript 𝜋 0 superscript 𝜋 0 B^{0}\\rightarrow\\pi^{0}\\pi^{0}  decays, along with that of  ℬ ℬ \\mathcal{B}  and time-dependent  C ​ P 𝐶 𝑃 CP  asymmetry for the  B 0 → π + ​ π − → superscript 𝐵 0 superscript 𝜋 superscript 𝜋 B^{0}\\rightarrow\\pi^{+}\\pi^{-}  decay. One needs all these observables in order to determine  ϕ 2 subscript italic-ϕ 2 \\phi_{2}  as electroweak tree and loop processes contribute with different phases to  B → π ​ π → 𝐵 𝜋 𝜋 B\\rightarrow\\pi\\pi  decays. The  ℬ ℬ \\mathcal{B}  and  A C ​ P subscript 𝐴 𝐶 𝑃 A_{CP}  for  B 0 → π 0 ​ π 0 → superscript 𝐵 0 superscript 𝜋 0 superscript 𝜋 0 B^{0}\\rightarrow\\pi^{0}\\pi^{0}  are the least well determined among the  B → π ​ π → 𝐵 𝜋 𝜋 B\\rightarrow\\pi\\pi  decays. This decay is also important to probe the disagreement between quantum-chromodynamics-based factorization, which predicts  ℬ ℬ \\mathcal{B}  below  1 × 10 − 6 1 superscript 10 6 1\\times 10^{-6}  [12, 13], and previous measurements from Belle and BaBar of  ( 1.8 − 2.3 ) × 10 − 6 1.8 2.3 superscript 10 6 (1.8-2.3)\\times 10^{-6}  [14,15]. Our study is based on a data sample recorded at the  Υ ​ ( 4 ​ S ) Υ 4 𝑆 \\Upsilon(4S)  resonance with the Belle detector comprising of  752 × 10 6 752 superscript 10 6 752\\times 10^{6}   B ​ B ¯ 𝐵 ¯ 𝐵 B\\bar{B}  pairs, which corresponds to an integrated luminosity of 693  fb -1 , and an additional 83.35  fb -1  recorded 60 MeV below the  Υ ​ ( 4 ​ S ) Υ 4 𝑆 \\Upsilon(4S)  resonance. <br> <br> <br> We reconstruct the signal  B 0 superscript 𝐵 0 B^{0}  candidate from a pair of  π 0 superscript 𝜋 0 \\pi^{0}  candidates, each subsequently decaying to two photons. In addition to photons reconstructed from clusters in the electromagnetic calorimeter (ECL) that do not match any charged track, photons that convert to  e + ​ e − superscript 𝑒 superscript 𝑒 e^{+}e^{-}  pairs in the silicon vertex detector (SVD) are recovered and reconstructed as  π 0 → γ ​ e + ​ e − → superscript 𝜋 0 𝛾 superscript 𝑒 superscript 𝑒 \\pi^{0}\\rightarrow\\gamma e^{+}e^{-} . This provides a 5.3% increase in detection efficiency. These photons must have an energy greater than 50 (100) MeV in the barrel (endcap) region of the ECL. The invariant mass of the two-photon combination<br>must lie in the range  115 &lt; m γ ​ γ &lt; 152 ​ MeV / c 2 115 subscript m 𝛾 𝛾 152 MeV superscript 𝑐 2 \\mathrm{115}&lt;\\mathrm{m_{\\gamma\\gamma}&lt;152~{}MeV/}c^{2} , corresponding to  ± 2.6 ​ σ plus-or-minus 2.6 𝜎 \\pm 2.6\\sigma  around the nominal  π 0 superscript 𝜋 0 \\pi^{0}  mass [9]. As in the case of  B + → K + ​ K − ​ π + → superscript 𝐵 superscript 𝐾 superscript 𝐾 superscript 𝜋 B^{+}\\rightarrow K^{+}K^{-}\\pi^{+} , two kinematic variables  Δ ​ E Δ 𝐸 \\Delta E  and  M b ​ c subscript 𝑀 𝑏 𝑐 M_{bc}  are used to select the signal candidates. All candidates satisfying  M b ​ c &gt; 5.26 ​ GeV / c 2 subscript 𝑀 𝑏 𝑐 5.26 GeV superscript 𝑐 2 M_{bc}&gt;5.26~{}\\mathrm{GeV}/c^{2}  and  − 0.3 &lt; Δ ​ E &lt; 0.2 ​ GeV 0.3 Δ 𝐸 0.2 GeV -0.3&lt;\\Delta E&lt;0.2~{}\\rm{GeV}  are retained for further analysis. For 7.2% of the events, there are multiple  B 0 superscript 𝐵 0 B^{0}  candidates in which case we choose the one that minimizes the deviation of the two  π 0 superscript 𝜋 0 \\pi^{0} ’s reconstructed invariant masses from the world average [9]. This criterion selects the true  B 0 superscript 𝐵 0 B^{0}  candidate in 90% of MC events. <br> <br> <br> The dominant background is from  e + ​ e − → q ​ q ¯ ​ ( q = u , d , s , c ) → superscript 𝑒 superscript 𝑒 𝑞 ¯ 𝑞 𝑞 𝑢 𝑑 𝑠 𝑐 e^{+}e^{-}\\rightarrow~{}q\\bar{q}~{}(q=u,d,s,c)  continuum process. To suppress this, we develop a Fisher discriminant ( T c subscript 𝑇 𝑐 T_{c} ) out of 16 modified Fox-Wolfram moments [16] combined with the cosine of the polar angle of the</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 200000\n",
    "highlighter = TextHighlighter(\n",
    "    long_text=sample_text,\n",
    "    chunking_api=splitter.split_text,\n",
    "    max_length=max_len\n",
    ")\n",
    "\n",
    "# 显示高亮文本\n",
    "highlighter.display_highlighted_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaTex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/113.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/107.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/12.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/339.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/477.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/311.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/305.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/463.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/488.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/259.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/503.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/265.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/271.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/270.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/502.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/264.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/258.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/489.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/304.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/462.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/476.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/310.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/338.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/13.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/106.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/112.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/138.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/104.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/110.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/11.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/39.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/448.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/460.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/306.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/312.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/474.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/272.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/266.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/500.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/299.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/298.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/267.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/501.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/273.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/313.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/475.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/461.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/307.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/449.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/38.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/10.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/111.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/105.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/139.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/101.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/115.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/129.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/28.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/14.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/303.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/465.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/471.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/317.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/459.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/277.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/505.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/263.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/288.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/289.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/504.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/262.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/276.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/458.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/470.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/316.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/302.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/464.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/15.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/29.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/128.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/114.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/100.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/116.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/102.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/17.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/314.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/472.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/466.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/300.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/328.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/499.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/260.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/506.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/274.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/248.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/249.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/275.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/261.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/507.tex\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/latex/arxiv/498.tex\n",
      "100个文档平均运行时间:0.0005秒\n"
     ]
    }
   ],
   "source": [
    "type=\".tex\"\n",
    "paths,splitter = choose_type(type)\n",
    "file_paths = [os.path.join(paths, f) for f in os.listdir(paths) if f.endswith(type)]\n",
    "file_paths = file_paths[:100]\n",
    "\n",
    "total_time = 0\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\", encoding=\"latin1\") as f:\n",
    "        sample_text = f.read()\n",
    "\n",
    "    print(f\"\\n测试文件: {file_path}\")\n",
    "    start_time = time.time()\n",
    "    chunks = splitter.split_text(sample_text)\n",
    "    end_time = time.time()\n",
    "    total_time += end_time - start_time\n",
    "    \n",
    "average_time = total_time / len(file_paths)\n",
    "print(f\"100个文档平均运行时间:{average_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高亮展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机选取的文本片段起始索引: 22858, 长度: 20000\n",
      "选取的文本片段:\n",
      " indexed by subsets $\\sigma \\in \\binom{[e]}{s}$.\n",
      "Among the $L_{\\bf w}$ are the tropicalized linear spaces\n",
      "\\cite[Theorem 4.3.17]{MS}. Even more special are \n",
      "linear spaces spanned by $s$  points; cf.~\\cite{FR}. If $L_{\\bf w}$ is spanned by\n",
      "  $ x^1,  \\ldots, x^s$ in $\\RR^e/\\RR {\\bf 1}$ then its Pl\\\"ucker coordinate\n",
      "$w_\\sigma$  is the\n",
      "{\\em tropical determinant} of the  $s \\times s$-submatrix  indexed by $\\sigma$ of the\n",
      "$s \\times e$-matrix  $X = (x^1,\\ldots,x^s)$. Note that\n",
      "all tropical linear spaces $L_{\\bf w}$ are tropically convex.\n",
      "\n",
      "We are interested in the nearest point map\n",
      "$\\pi_{L_{\\bf w}}$ that takes a point $u$ to \n",
      "the largest point in $L_{\\bf w}$ dominated by $u$, as \n",
      "seen   in (\\ref{eq:tropproj}).\\end{comment}\n",
      "\n",
      "In order to consider a ``tropical principal component analysis'', as described at the beginning of this section, we must be able to project onto a tropical linear space. This projection operation is described by the \\emph{Red and Blue Rules}. From \\cite[Theorem 15]{JSY} we have:\n",
      "\n",
      "\\begin{theorem}[The Blue Rule]\n",
      "\\label{blue-rule}\n",
      "Let $p:[e]^d\\to \\overline \\RR$ be a tropical Pl\\\"ucker vector and $L_p$ its associated tropical linear space in $\\RR^e/\\RR {\\bf 1}$. Fix $u\\in\\RR^e/\\RR {\\bf 1}$, and define the point $w\\in\\RR^e/\\RR {\\bf 1}$ whose $i$th coordinate is\n",
      "\\begin{equation}\n",
      "\\label{eq:bluerule}\n",
      " \\quad w_i \\,\\, = \\,\\,\n",
      " {\\rm max}_\\tau \\,{\\rm min}_{j \\not\\in \\tau} \\bigl( u_j + p({\\tau \\cup \\{i\\}}) - p({\\tau \\cup \\{j\\}}) \\bigr)\n",
      " \\qquad {\\rm for} \\,\\,\\, i = 1,2,\\ldots, e\n",
      " \\end{equation}\n",
      "where $\\tau$ runs over all $(d-1)$-subsets of $[e]$ that do not contain $i$.\n",
      "\n",
      "Then $w\\in L_p$, and any other $x\\in L_p$ satisfies $d(u,x)\\geq d(u,w)$. In other words, $w$ attains the minimum distance of any point in $L_p$ to $u$.\n",
      "\\end{theorem}\n",
      "\n",
      "\\begin{theorem}[The Red Rule]\n",
      "\\label{red-rule}\n",
      "Let $p:[e]^d\\to \\overline \\RR$ be a tropical Pl\\\"ucker vector and $L_p$ its associated tropical linear space in $\\RR^e/\\RR {\\bf 1}$. Fix $u\\in\\RR^e/\\RR {\\bf 1}$. Let $v$ be the all-zeros vector. For every $(d+1)$-sized subset $\\tau$ of $[e]$, compute $\\max p({\\tau-\\tau_i}) + u_{\\tau_i}$. If this maximum is unique, attained with index $\\tau_i$, then let $\\gamma_{\\tau,\\tau_i}$ be the positive difference between the second maximum and this maximum, and set $v_{\\tau_i}=\\max(v_{\\tau_i}, \\gamma_{\\tau,\\tau_i})$.\n",
      "\n",
      "Then $v$ gives the difference between $u$ and a closest point of $L_p$. In particular, if $w$ is the point in $L_p$ returned by the Blue Rule, we have \n",
      "\\[u = w + v.\\]\n",
      "\\end{theorem}\n",
      "\n",
      "We write $\\pi_{L_p}$ as the projection function which takes a point $u\\in \\RR^e/\\RR {\\bf 1}$ and returns the nearest point $w\\in L_p$ given by the Blue Rule.\n",
      "\n",
      "\\begin{example}\n",
      "\\label{red-blue-rule-example}\n",
      "Let $A$ be the matrix of Example \\ref{tropical-linear-space-example}, with $p$ and $L_p$ its associated tropical Pl\\\"ucker vector and Stiefel tropical linear space. Let $u$ be the point $(1, -2, 3)\\in\\RR^e/\\RR {\\bf 1}$. \n",
      "\n",
      "The Blue Rule constructs a point $w\\in\\RR^3/\\RR{\\bf 1}$ whose first coordinate is\n",
      "\\[\\max(\\min(u_1+p(\\{1,2\\})-p(\\{1,2\\}), u_3+p(\\{1,2\\})-p(\\{1,3\\})),\\]\n",
      "\\[\\min(u_1+p(\\{1,3\\})-p(\\{1,3\\}),u_2+p(\\{1,3\\})-p(\\{2,3\\}))).\\]\n",
      "Substituting in, we get the first coordinate of $w$ as \n",
      "\\[w_1 = \\max(\\min(1 + 2 -2, 3 + 2 - 4), \\min(1 + 4 - 4, -2 + 4 - 3)) = \\max(1, -1) = 1.\\]\n",
      "Similarly, we get $w_2 = -2$ and $w_3 = 0$. So the Blue Rule outputs the vector $(1, -2, 2)$.\n",
      "\n",
      "The Red Rule constructs a vector $v$ as follows. First, we begin with $v = (0, 0, 0)$. Next we take the set $\\tau = [e]$ and compute $\\max(p(\\{2,3\\})+u_1, p(\\{1,3\\})+u_2, p(\\{1,2\\})+u_3) = \\max(3 + 1, 4 -2, 2 + 3) = 5.$ So the Red Rule redefines $v_3 = 5 - 4 = 1$, and hence outputs the vector $v=(0,0,1)$. Now Theorem \\ref{red-rule} states that $u = w + v$, which is easily verified to hold.\n",
      "\\end{example}\n",
      "\n",
      "\\begin{definition}\n",
      "Let $v = (v_1,\\ldots, v_e)$ be a real vector, and define the tropical linear functional $\\bigoplus (-v_i)\\otimes x_i$. Let $\\mathcal H$ be the tropical solution set of this linear functional: that is, $\\mathcal H$ consists of all $x\\in\\RR^e/\\RR {\\bf 1}$ such that the maximum of $\\bigoplus( -v_i)\\otimes x_i$ is attained at least twice. We call any $\\mathcal H$ obtained in this way a \\emph{tropical hyperplane}.\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{remark}\n",
      "\\label{tropical-hyperplane-valuated-matroid}\n",
      "Let $A$ be a tropical matrix of dimensions $(e-1)\\times e$. Then the Stiefel tropical linear space of $A$ is a tropical hyperplane. Furthermore, any tropical hyperplane is the Stiefel tropical linear space of such a tropical matrix $A$.\n",
      "\\end{remark}\n",
      "\\begin{comment}\n",
      "\\begin{definition}\n",
      "Let $p:[e]^{e-1}\\to\\overline \\RR$ be a Stiefel tropical linear space. Then we call the tropical linear space $\\mathcal H$ corresponding to $p$ a \\emph{tropical hyperplane}.\n",
      "\\end{definition}\n",
      "\\begin{lemma}\n",
      "\\label{tropical-hyperplane-valuated-matroid}\n",
      "Let $v=(v_1,\\ldots, v_e)\\in\\mathbb \\overline \\RR^e$ be any vector. Then there exists a tropical Pl\\\"ucker vector $p:[e]^{e-1}\\to\\overline \\RR$ defined by\n",
      "\\[p([e]-\\{i\\})=v_i.\\]\n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "We check only the exchange relation. Let $\\sigma$ be an $(e-2)$-sized subset of $[e]$, and let $\\tau$ be $[e]$, the only $e$-sized subset of $[e]$. We need that the maximum\n",
      "\\[\\max_{i\\in [e]}(p(\\sigma\\cup\\{i\\}) + p([e]-\\{i\\}))\\]\n",
      "is attained at least twice. But if $i$ is a choice of index which attains a maximum, then $[e]-\\sigma-\\{i\\}$ attains the same value. So the maximum is indeed attained at least twice.\n",
      "\\end{proof}\n",
      "\n",
      "In tropical geometry, tropical hyperplanes are often introduced in a different form.\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{tropical-hyperplane}\n",
      "Let $\\mathcal H$ be a tropical hyperplane in $\\TP^{e-1}$ and $p$ the tropical Pl\\\"ucker vector corresponding to $\\mathcal H$. Write $v_i:=p([e]-\\{i\\})$. Then $\\mathcal H$ is the tropical solution set of the tropical linear functional $\\bigoplus (-v_i)\\otimes x_i$.\n",
      "\\end{lemma}\n",
      "\\end{comment}\n",
      "\n",
      "\\subsection{A tropical interpretation for phylogenetic trees}\n",
      "\\label{tropical-interpretation-for-phylogenetics}\n",
      "\n",
      "In this section we describe some of the tropical aspects underlying the study of phylogenetic trees. Our treatment of this subject largely follows \\cite[Section 4.3]{MS}.\n",
      "\n",
      "\\begin{definition}\n",
      "A \\emph{dissimilarity map} $d$ is a function $d:[m]\\times [m]\\to \\mathbb R_{\\geq 0}$ such that $d(i,i)=0$ and $d(i,j)=d(j,i)\\geq 0$ for each $i,j\\in [m]$. If, furthermore, we have that $d(i,j)\\leq d(i,k)+d(k,j)$ for all $i,j,k\\in [m]$, we call $d$ a \\emph{metric}. Note that for convenience we often write $d_{ij}$ for the term $d(i,j)$.\n",
      "\n",
      "We can represent a dissimilarity map $d$ by an $m\\times m$ matrix $D$ whose $(i,j)$th entry is $d_{ij}$. Because $D$ is clearly symmetric and all diagonal entries are trivial, there is a natural embedding of $d$ into the tropical space $\\mathbb R^{e} = R^{\\binom m 2}$.\n",
      "\\end{definition}\n",
      "\n",
      "In fact, the condition of being a metric is intrinsically tropical.\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{metric-condition}\n",
      "Let $d:[m]\\times [m]\\to\\mathbb R_{\\geq 0}$ be a dissimilarity metric and $D$ its corresponding matrix. Then $d$ is a metric iff $-D\\odot -D = -D$.\n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "The $(i,j)$th entry of $-D\\odot -D$ is equal to \n",
      "\\[\\bigoplus_{k=1}^m -d_{ik}-d_{kj} = \\max_{k\\in [m]}(-d_{ik}-d_{kj}).\\]\n",
      "In particular, we note that the $(i,j)$th entry of $-D\\odot -D$ is at least as large as $-d_{ij}=-d_{ii}-d_{ij}$. Now a simple negation of the definition shows that $d$ is a metric iff $-d_{ij}\\geq \\max_{k\\in [m]}(-d_{ik}-d_{kj})$. \n",
      "\\end{proof}\n",
      "\n",
      "\\begin{definition}\n",
      "Let $T=(V,E)$ be a tree with $m$ labeled leaves and no vertices of degree two. We call such a tree a \\emph{phylogenetic tree}.\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{definition}\n",
      "Let $T$ be a phylogenetic tree with $m$ leaves labeled with the elements of $[m]$, and assign a length $\\ell_e\\in \\mathbb R$ to each edge $e$ of $T$. Let $d:[m]\\times [m]\\to \\mathbb R$ be defined so that $d_{ij}$ is the total length of the unique path from leaf $i$ to leaf $j$. We call a function $d$ obtained in this way a \\emph{tree distance}. If, furthermore, each entry of the distance matrix $D$ is nonnegative, then $d$ is in fact a metric. We call such a $d$ a \\emph{tree metric}. As before, we can embed $D$ into $\\mathbb R^{e}$. \n",
      "\\end{definition}\n",
      "\n",
      "Of course, any tree distance differs from a tree metric by some scalar multiple of ${\\bf 1}$. Hence the sets of tree distances and tree metrics coincide in $\\mathbb R/{\\bf 1}\\RR$. \n",
      "\n",
      "\\begin{definition}\n",
      "Let $d:[m]\\times [m]\\to \\mathbb R_{\\geq 0}$ be a metric which satisfies the following strengthening of the triangle inequality for each choice of $i,j,k\\in [m]$:\n",
      "\\[d(i,k)\\leq \\max(d(i,j),d(j,k)).\\]\n",
      "We call such a metric an \\emph{ultrametric}. Let $\\mathcal U_m$ denote the collection of all ultrametrics in $\\mathbb R^e/{\\bf 1}\\mathbb R$.\n",
      "\\end{definition}\n",
      "\n",
      "It is well-known that all ultrametrics are tree metrics. In fact, all ultrametrics are derived from \\emph{equidistant trees}, where all leaves have the same distance to some distinguished root vertex. Furthermore, the tree metric of an equidistant tree is an ultrametric; hence ultrametrics and equidistant trees convey equivalent information.\n",
      "\n",
      "Let $L_m$ denote the subspace of $\\mathbb R^e$ defined by the linear equations $x_{ij} - x_{ik} + x_{jk}=0$ for $1\\leq i < j <k \\leq m$. The tropicalization $\\Trop(L_m)\\subseteq \\RR^e/\\RR {\\bf 1}$ is the tropical linear space consisting of points $(v_{12},v_{13},\\ldots, v_{m-1,m})$ such that $\\max(v_{ij},v_{ik},v_{jk})$ is obtained at least twice for all triples $i,j,k\\in [m]$.\n",
      "\n",
      "\\begin{remark}\n",
      "Experts in tropical geometry will note that the tropical linear space $\\Trop(L_m)$ corresponds to the graphic matroid of the complete graph $K_m$.\n",
      "\\end{remark}\n",
      "\n",
      "\\begin{theorem}\n",
      "\\label{ultrametrics}\n",
      "The image of $\\mathcal U_m$ in the tropical projective torus $\\RR^e/\\RR {\\bf 1}$ coincides with $\\Trop(L_m)$.\n",
      "\\end{theorem}\n",
      "\\begin{proof}\n",
      "Let $(v_{12},v_{13},\\ldots, v_{m-1,m})\\in \\Trop(L_m)$. We may assume that each coordinate is nonnegative, so that this point corresponds to the image of a dissimilarity map $d$. To see that $d$ is in fact an ultrametric, fix $i,j,k\\in [m]$. We know that $\\max(d_{ij},d_{ik}, d_{kj})$ is attained at least twice, by the definition of $\\Trop(L_m)$. If $d_{ij}$ is one of these maximums then it must be equal to $\\max(d_{ik},d_{kj})$. If $d(i,j)$ is not one of these maximums then it must be strictly less than $\\max(d_{ik},d_{kj})$. Either way, we have that $d_{ij}\\leq \\max(d_{ik},d_{kj})$. This shows that $d$ is in fact an ultrametric, so that $\\Trop(L_m)\\subseteq \\mathcal U_m$.\n",
      "\n",
      "Let $\\bar D\\in \\mathcal U_m$. Then there exists some lifted ultrametric $d$ with associated matrix $D$. Fix a choice of $i,j,k\\in [m]$, and without loss of generality let $i,j$ such that $d_{ij}=\\max(d_{ij},d_{ik},d_{kj})$. Because $d$ is an ultrametric, we have that $d_{ij}\\leq \\max(d_{ik},d_{kj})$.  Hence in fact $d_{ij}=\\max(d_{ik},d_{kj})$, and the maximum of $\\max(d_{ij},d_{ik},d_{kj})$ is attained at least twice. Thus $\\Trop(L_m)\\supseteq \\mathcal U_m$.\n",
      "\\end{proof}\n",
      "\n",
      "In words, Theorem \\ref{ultrametrics} states that the image of the space of labeled rooted trees is a tropical linear space. The set of equidistant trees thus has an intrinsic tropical structure.\n",
      "\n",
      "\\begin{comment}\n",
      "\n",
      "Tree metrics can be characterized in the following way:\n",
      "\n",
      "\\begin{lemma}[Four-point condition]\n",
      "\\label{four-point-condition}\n",
      "A metric $d$ on the set $[n]$ is a tree metric iff, for any four elements $u,v,x,y\\in [n]$, the maximum of $d(u,v)+d(x,y), d(u,x)+d(v,y)$, and $d(u,y)+d(x,v)$ is attained at least twice.\n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "See \\cite[Theorem 2.36]{PS}.\n",
      "\\end{proof}\n",
      "\n",
      "\\begin{remark}\n",
      "Let $T\\in\\mathbb R^e$ be a tree metric. Lemma \\ref{four-point-condition} implies that $T + \\lambda \\cdot {\\bf 1}$ is also a tree metric for any nonnegative choice of $\\lambda$. \n",
      "\\end{remark}\n",
      "\n",
      "\\begin{definition}\n",
      "Let $Q = (q_{ij})$ be an $n\\times n$ symmetric matrix with zeros on the diagonal whose off-diagonal entries are variables. For each collection of four elements $u,v,x,y\\in[n]$, define the quadratic tropical polynomial\n",
      "\\[g_{uvxy}(Q) = q_{uv}\\otimes q_{xy} \\oplus q_{ux}\\otimes q_{vy} \\oplus q_{uy}\\otimes q_{vx}.\\]\n",
      "Thus $g_{uvxy}$ defines a tropical hypersurface $\\mathcal T(g_{uvxy})$ in $\\mathbb R^{\\binom n 2}/{\\bf 1}\\RR$. \n",
      "\\end{definition}\n",
      "\n",
      "\\begin{remark}\n",
      "In fact, the intersection of the $\\binom n 4$ tropical hypersurfaces $\\mathcal T(g_{uvxy})$ is a tropical linear space, known as the \\emph{tropical Grassmannian} $\\mathcal T(Gr(2,n))$. Note that $g_{uvxy}(Q)$ is the tropicalization of the Plucker relation $p_{uv}p_{xy}-p_{ux}p_{vy}-p_{uy}p_{vx}$.\n",
      "\\end{remark}\n",
      "\n",
      "\\begin{theorem}\n",
      "Let $\\mathcal T$ be the collection of all tree metrics, identified with its image in $\\mathbb R^{\\binom n 2}/{\\bf 1}\\RR$. Then $\\mathcal T$ equals the tropical Grassmannian $\\mathcal T(Gr(2,n))$.\n",
      "\\end{theorem}\n",
      "\\begin{proof}\n",
      "We show one containment. (For a full proof, see \\cite{MS}[Theorem 4.3.5].) Fix $T\\in \\mathcal T$. We may assume that all coordinates of $T$ are nonnegative, so that $T$ is in fact a tree metric. Now Lemma \\ref{four-point-condition} states that $T$ is a tree metric if and only if, for all $1\\leq u<v<x<y\\leq n$, the maximum of $d(u,v)+d(x,y), d(u,x)+d(v,y)$, and $d(u,y)+d(x,v)$ is attained at least twice. This is precisely what it means to be contained in intersection of the tropical hypersurfaces $\\mathcal T(g_{uvxy})=\\mathcal T(Gr(2,n))$.\n",
      "\\end{proof}\n",
      "\n",
      "ULTRAMETRICS\n",
      "\n",
      "Let $e:=\\binom m 2$, and let $L_m$ denote the $(m-1)$-dimensional subspace of $\\mathbb R^e$ defined by the linear equations $x_{ij} - x_{ik} + x_{jk}=0$ for $1\\leq i < j <k \\leq m$. \n",
      "\n",
      "\\end{comment}\n",
      "\n",
      "\\section{Tropical PCA as a Stiefel tropical linear space}\\label{trop:lin}\n",
      "\n",
      "As noted in the introduction, one can interpret ordinary $(s-1)$th principal component analysis as a method of dimensionality reduction, replacing data points with their projections onto the translate of some particularly well-fitting linear space of dimension $s-1$. Classically, this translation of a well-fitted linear space can be described by an $(s\\times e)$-dimensional matrix, whose first $(s-1)$ rows are the basis vectors of the linear space, and whose last row is a translation vector from the origin.\n",
      "\n",
      "In analogy with the classical case, our approach to an $(s-1)$th tropical principal component analysis is to replace data points with their tropical projections onto the best-fit Stiefel tropical linear space of dimension $(s-1)$, defined by a tropical matrix of size $s\\times e$.\n",
      "\n",
      "\\subsection{Best-fit tropical hyperplanes}\n",
      "\n",
      "We begin our discussion of tropical principal component analysis by\n",
      "considering a specific case: reducing by one the dimension\n",
      "of a collection of $e$ datapoints in $\\RR^e/\\RR {\\bf 1}$. In other words, we\n",
      "seek the $(e-1)$th order tropical PCA, or a \\emph{best-fit tropical hyperplane}, for a collection of $e$ data points in $\\RR^e/\\RR {\\bf 1}$.\n",
      "\n",
      "\\begin{comment}It is well-known that any tropical hyperplane is a \\emph{Stiefel tropical linear space}: that is, any tropical hyperplane in $\\TP^{e-1}$ is spanned by $e-1$ points.\\cite{FR} In particular, it suffices to consider the valuated matroids corresponding to all $e\\times e$ matrices $A$ with entries in $\\overline \\RR$.\n",
      "\n",
      "% The special case of this theorem when $L_{\\bf w}$ has the form \n",
      "% ${\\rm Trop}(M)$, for some rank $r$ matroid $M$ on $[e]$, \n",
      "% was proved by Ardila in \\cite[Theorem 1]{Ard}.\n",
      "% Matroids correspond to the case when each tropical Pl\\\"ucker coordinate $w_\\sigma$ is\n",
      "% either $0$ or $-\\infty$.\n",
      "The application that motivated Ardila's study\n",
      "was the ultrametric tree space $\\mathcal{U}_m$.\n",
      "Here the nearest-point map computes the largest ultrametric\n",
      "dominated by a given dissimilarity map, a problem of importance\n",
      "in phylogenetics. An efficient algorithm for this problem was\n",
      "given by Chepoi and Fichet \\cite{CF}. This was recently\n",
      "revisited by Apostolico {\\it et al.}~in~\\cite{ACDP}.\n",
      "\n",
      "Returning to ideas for geometric statistics,\n",
      "the Blue Rule may serve as a subroutine for\n",
      "the numerical computation of regression planes.\n",
      "Let $d_1,\\ldots, d_n$ be data points in $\\RR^{e}/\\RR {\\bf 1}$,\n",
      "lying in a tropically convex \n",
      "subset $\\mathcal{P}$ of interest, such as\n",
      "$\\mathcal{P} = \\mathcal{U}_m$.\n",
      "The tropical regression plane of dimension $s-1$ is a\n",
      "solution to the optimization problem\n",
      "\\begin{equation}\n",
      "\\label{eq:regression}\n",
      "\\argmin_{L_{\\bf w}} \\sum_{i=1}^n  d_{\\rm tr} (d_i, L_{\\bf w}).\n",
      "\\end{equation}\n",
      "Here ${\\bf w}$ runs over all points in the Dressian ${\\rm Dr}(s,e)$,\n",
      "or in the tropical Grassmannian ${\\rm Gr}(s,e)$.\n",
      "\n",
      "\\begin{remark}\\label{bernds}\n",
      "Maybe we can first compute the set of Fermat-Weber points of a sample\n",
      "$\\{d_1, \\ldots , d_n\\}$ and let ${\\bf \\bar{x}} = (\\bar{x}^1, \\ldots\n",
      "\\bar{x}^e)$ be a Fermat-Weber point of the sample.    Then compute\n",
      "the tropical hyperplane \n",
      "$L_{{\\bf w}}$ which contains the  set of Fermat-Weber points of the\n",
      "sample by the considering the following tropical hyper plane\n",
      "\\[\n",
      "a_1 \\odot x_i \\oplus \\cdots \\oplus a_e \\odot x_e = \\max \\{a_1 + x_1, \\ldots ,\n",
      "a_e + x_e\\},\n",
      "\\]\n",
      "and setting $a_i = -\\bar{x}^i$.  \n",
      "Is it the\n",
      "optimal solution of our optimization \n",
      "problem?  If not then how far off?  \n",
      "\\end{remark}\n",
      "\n",
      "The distance of a nearest hyperplane to a set of $e$ data points is related to the \\emph{tropical volume}, from \\cite[Definition 2]{DGJ}:\n",
      "\n",
      "\\leoncomment{In the below writeup I used the min-plus convention, sorry. In a second pass I can edit things to use max-plus instead.}\\end{comment}\n",
      "\n",
      "We require the following definition, from \\cite{DGJ}.\n",
      "\n",
      "\\begin{definition}\n",
      "Let $A$ be an $e\\times e$ matrix with entries in $\\overline \\RR$ whose rows correspond to $e$ points in $\\RR^e/\\RR {\\bf 1}$. The \\emph{tropical volume} of $A$ is given by the expression\n",
      "\\[\\tvol A := \\bigoplus_{\\sigma\\in S_e} \\sum a_{i,\\sigma(i)} - \\bigoplus_{\\tau\\in S_e-\\sigma_{opt}}\\sum a_{i,\\tau(i)},\\]\n",
      "where $\\sigma_{opt}$ is an optimal permutation attaining the tropical determinant in the first tropical sum.\n",
      "\\end{definition}\n",
      "\n",
      "Recall that a square tropical matrix $A$ is \\emph{tropically singular} if two distinct permutations attain the tropical determinant. The following, from \\cite[Lemma 5.1]{RGST}, is one of the earliest results in tropical geometry:\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{common-hyperplane}\n",
      "Let $A$ be an $e\\times e$ tropical matrix whose rows represent $e$ points of $\\RR^e/\\RR {\\bf 1}$. Then $A$ is tropically singular iff those $k$ points lie on a tropical hyperplane in $\\RR^e/\\RR {\\bf 1}$. In particular, $\\tvol(A)=0$ iff the $e$ points lie on a common tropical hyperplane.\n",
      "\\end{lemma}\n",
      "\n",
      "Of course, if our collection of $e$ datapoints $D^{(i)}$ lie on a common hyperplane, then this hyperplane is our $(e-1)$th tropical PCA. This fact hints at some relationship between the tropical volume and the best fit hyperplane. In fact, this relationship is quite strong.\n",
      "\n",
      "\\begin{theorem}\n",
      "\\label{tropical-volume}\n",
      "Let $D^{(1)},\\ldots, D^{(e)}$ be a collection of $e$ points in $\\RR^e/\\RR {\\bf 1}$. Then the best-fit hyperplane attains a distance from the $e$ points equal to their tropical volume, and one such best-fit hyperplane is spanned by a choice of $e-1$ of the points.\n",
      "\\end{theorem}\n",
      "\n",
      "To prove this theorem, we first show that the tropical volume is an upper bound on the minimal distance of a best-fit tropical hyperplane.\n",
      "\n",
      "\\begin{lemma}\\label{upper-bound}\n",
      "Let $D^{(1)},\\ldots, D^{(e)}$ be a collection of $e$ points in $\\RR^e/\\RR {\\bf 1}$, and let $A$ be the matrix whose $i,j$th entry is $D^{(i)}_j$. Then there exists a hyperplane of distance $\\tvol A$ from the data points, spanned by some choice of $e-1$ of the points.\n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "Suppose that all $e$ data points can be spanned by a single hyperplane. Then Lemma \\ref{common-hyperplane} tells us that this best-fit hyperplane is of distance $\\tvol A = 0$ from the data points.\n",
      "\n",
      "Now suppose that the $e$ data points do not lie on the same hyperplane. %Because we are working in $\\TP^{e-1}$, we are free to choose the last coordinate of each data point to be 0. \n",
      "Without loss of generality, we may assume that the data points $D^{(1)},\\ldo\n",
      "\n",
      "文本已分成 185 块。\n",
      "分块结果:\n",
      "Chunk 1:  indexed by subsets \\sigma \\in \\binom{[e]}{s}.\n",
      "Among the L_{\\bf w}\n",
      "Chunk 2:  are the tropicalized linear spaces\n",
      "\\cite[Theorem 4.3.17]{MS}. Even more special are \n",
      "linear spaces spanned by \n",
      "Chunk 3: s  points; cf.~\\cite{FR}. If L_{\\bf w} is spanned by\n",
      "   x^1,  \\ldots, x^s in \\RR^e/\\RR {\\bf 1} then its Pl\\\"ucker coordinate\n",
      "w_\\sigma  is the\n",
      "{\\em tropical determinant} of the  s \\times s-submatrix  indexed by \\sigma of the\n",
      "s \\times e-matrix  X = (x^1,\\ldots,x^s). Note that\n",
      "all tropical linear spaces L_{\\bf w}\n",
      "Chunk 4:  are tropically convex.\n",
      "\n",
      "We are interested in the nearest point map\n",
      "\n",
      "Chunk 5: \\pi_{L_{\\bf w}} that takes a point u to \n",
      "the largest point in L_{\\bf w} dominated by u\n",
      "Chunk 6: , as \n",
      "seen   in (\\ref{eq:tropproj}).\\end{comment}\n",
      "\n",
      "In order to consider a ``tropical principal component analysis'', as described at the beginning of this section, we must be able to project onto a tropical linear space. This projection operation is described by the \\emph{Red and Blue Rules}. From \\cite[Theorem 15]{JSY} we have:\n",
      "\n",
      "\\begin{theorem}[The Blue Rule]\n",
      "\\label{blue-rule}\n",
      "Let \n",
      "Chunk 7: p:[e]^d\\to \\overline \\RR be a tropical Pl\\\"ucker vector and L_p its associated tropical linear space in \\RR^e/\\RR {\\bf 1}. Fix u\\in\\RR^e/\\RR {\\bf 1}, and define the point w\\in\\RR^e/\\RR {\\bf 1} whose i\n",
      "Chunk 8: th coordinate is\n",
      "\\begin{equation}\n",
      "\\label{eq:bluerule}\n",
      " \\quad w_i \\,\\, = \\,\\,\n",
      " {\\rm max}_\\tau \\,{\\rm min}_{j \\not\\in \\tau} \\bigl( u_j + p({\\tau \\cup \\{i\\}}) - p({\\tau \\cup \\{j\\}}) \\bigr)\n",
      " \\qquad {\\rm for} \\,\\,\\, i = 1,2,\\ldots, e\n",
      " \\end{equation}\n",
      "where \n",
      "Chunk 9: \\tau runs over all (d-1)-subsets of [e] that do not contain i.\n",
      "\n",
      "Then w\\in L_p, and any other x\\in L_p satisfies d(u,x)\\geq d(u,w). In other words, w attains the minimum distance of any point in L_p to u\n",
      "Chunk 10: .\n",
      "\\end{theorem}\n",
      "\n",
      "\\begin{theorem}[The Red Rule]\n",
      "\\label{red-rule}\n",
      "Let \n",
      "Chunk 11: p:[e]^d\\to \\overline \\RR be a tropical Pl\\\"ucker vector and L_p its associated tropical linear space in \\RR^e/\\RR {\\bf 1}. Fix u\\in\\RR^e/\\RR {\\bf 1}. Let v be the all-zeros vector. For every (d+1)-sized subset \\tau of [e], compute \\max p({\\tau-\\tau_i}) + u_{\\tau_i}. If this maximum is unique, attained with index \\tau_i, then let \\gamma_{\\tau,\\tau_i}\n",
      "Chunk 12:  be the positive difference between the second maximum and this maximum, and set \n",
      "Chunk 13: v_{\\tau_i}=\\max(v_{\\tau_i}, \\gamma_{\\tau,\\tau_i}).\n",
      "\n",
      "Then v gives the difference between u and a closest point of L_p. In particular, if w is the point in L_p\n",
      "Chunk 14:  returned by the Blue Rule, we have \n",
      "\\[u = w + v.\\]\n",
      "\\end{theorem}\n",
      "\n",
      "We write \n",
      "Chunk 15: \\pi_{L_p} as the projection function which takes a point u\\in \\RR^e/\\RR {\\bf 1} and returns the nearest point w\\in L_p\n",
      "Chunk 16:  given by the Blue Rule.\n",
      "\n",
      "\\begin{example}\n",
      "\\label{red-blue-rule-example}\n",
      "Let \n",
      "Chunk 17: A\n",
      "Chunk 18:  be the matrix of Example \\ref{tropical-linear-space-example}, with \n",
      "Chunk 19: p and L_p\n",
      "Chunk 20:  its associated tropical Pl\\\"ucker vector and Stiefel tropical linear space. Let \n",
      "Chunk 21: u be the point (1, -2, 3)\\in\\RR^e/\\RR {\\bf 1}. \n",
      "\n",
      "The Blue Rule constructs a point w\\in\\RR^3/\\RR{\\bf 1}\n",
      "Chunk 22:  whose first coordinate is\n",
      "\\[\\max(\\min(u_1+p(\\{1,2\\})-p(\\{1,2\\}), u_3+p(\\{1,2\\})-p(\\{1,3\\})),\\]\n",
      "\\[\\min(u_1+p(\\{1,3\\})-p(\\{1,3\\}),u_2+p(\\{1,3\\})-p(\\{2,3\\}))).\\]\n",
      "Substituting in, we get the first coordinate of \n",
      "Chunk 23: w\n",
      "Chunk 24:  as \n",
      "\\[w_1 = \\max(\\min(1 + 2 -2, 3 + 2 - 4), \\min(1 + 4 - 4, -2 + 4 - 3)) = \\max(1, -1) = 1.\\]\n",
      "Similarly, we get \n",
      "Chunk 25: w_2 = -2 and w_3 = 0. So the Blue Rule outputs the vector (1, -2, 2).\n",
      "\n",
      "The Red Rule constructs a vector v as follows. First, we begin with v = (0, 0, 0). Next we take the set \\tau = [e] and compute \n",
      "Chunk 26: \\max(p(\\{2,3\\})+u_1, p(\\{1,3\\})+u_2, p(\\{1,2\\})+u_3) = \\max(3 + 1, 4 -2, 2 + 3) = 5.\n",
      "Chunk 27:  So the Red Rule redefines v_3 = 5 - 4 = 1, and hence outputs the vector v=(0,0,1). Now Theorem \\ref{red-rule} states that u = w + v\n",
      "Chunk 28: , which is easily verified to hold.\n",
      "\\end{example}\n",
      "\n",
      "\\begin{definition}\n",
      "Let \n",
      "Chunk 29: v = (v_1,\\ldots, v_e)\n",
      "Chunk 30:  be a real vector, and define the tropical linear functional \n",
      "Chunk 31: \\bigoplus (-v_i)\\otimes x_i. Let \\mathcal H\n",
      "Chunk 32:  be the tropical solution set of this linear functional: that is, \n",
      "Chunk 33: \\mathcal H consists of all x\\in\\RR^e/\\RR {\\bf 1} such that the maximum of \\bigoplus( -v_i)\\otimes x_i is attained at least twice. We call any \\mathcal H\n",
      "Chunk 34:  obtained in this way a \\emph{tropical hyperplane}.\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{remark}\n",
      "\\label{tropical-hyperplane-valuated-matroid}\n",
      "Let \n",
      "Chunk 35: A be a tropical matrix of dimensions (e-1)\\times e. Then the Stiefel tropical linear space of A\n",
      "Chunk 36:  is a tropical hyperplane. Furthermore, any tropical hyperplane is the Stiefel tropical linear space of such a tropical matrix \n",
      "Chunk 37: A\n",
      "Chunk 38: .\n",
      "\\end{remark}\n",
      "\\begin{comment}\n",
      "\\begin{definition}\n",
      "Let \n",
      "Chunk 39: p:[e]^{e-1}\\to\\overline \\RR\n",
      "Chunk 40:  be a Stiefel tropical linear space. Then we call the tropical linear space \n",
      "Chunk 41: \\mathcal H corresponding to p\n",
      "Chunk 42:  a \\emph{tropical hyperplane}.\n",
      "\\end{definition}\n",
      "\\begin{lemma}\n",
      "\\label{tropical-hyperplane-valuated-matroid}\n",
      "Let \n",
      "Chunk 43: v=(v_1,\\ldots, v_e)\\in\\mathbb \\overline \\RR^e\n",
      "Chunk 44:  be any vector. Then there exists a tropical Pl\\\"ucker vector \n",
      "Chunk 45: p:[e]^{e-1}\\to\\overline \\RR\n",
      "Chunk 46:  defined by\n",
      "\\[p([e]-\\{i\\})=v_i.\\]\n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "We check only the exchange relation. Let \n",
      "Chunk 47: \\sigma be an (e-2)-sized subset of [e], and let \\tau be [e], the only e-sized subset of [e]\n",
      "Chunk 48: . We need that the maximum\n",
      "\\[\\max_{i\\in [e]}(p(\\sigma\\cup\\{i\\}) + p([e]-\\{i\\}))\\]\n",
      "is attained at least twice. But if \n",
      "Chunk 49: i\n",
      "Chunk 50:  is a choice of index which attains a maximum, then \n",
      "Chunk 51: [e]-\\sigma-\\{i\\}\n",
      "Chunk 52:  attains the same value. So the maximum is indeed attained at least twice.\n",
      "\\end{proof}\n",
      "\n",
      "In tropical geometry, tropical hyperplanes are often introduced in a different form.\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{tropical-hyperplane}\n",
      "Let \n",
      "Chunk 53: \\mathcal H be a tropical hyperplane in \\TP^{e-1} and p the tropical Pl\\\"ucker vector corresponding to \\mathcal H. Write v_i:=p([e]-\\{i\\}). Then \\mathcal H\n",
      "Chunk 54:  is the tropical solution set of the tropical linear functional \n",
      "Chunk 55: \\bigoplus (-v_i)\\otimes x_i\n",
      "Chunk 56: .\n",
      "\\end{lemma}\n",
      "\\end{comment}\n",
      "\n",
      "\\subsection{A tropical interpretation for phylogenetic trees}\n",
      "\\label{tropical-interpretation-for-phylogenetics}\n",
      "\n",
      "In this section we describe some of the tropical aspects underlying the study of phylogenetic trees. Our treatment of this subject largely follows \\cite[Section 4.3]{MS}.\n",
      "\n",
      "\\begin{definition}\n",
      "A \\emph{dissimilarity map} \n",
      "Chunk 57: d is a function d:[m]\\times [m]\\to \\mathbb R_{\\geq 0} such that d(i,i)=0 and d(i,j)=d(j,i)\\geq 0 for each i,j\\in [m]. If, furthermore, we have that d(i,j)\\leq d(i,k)+d(k,j) for all i,j,k\\in [m], we call d\n",
      "Chunk 58:  a \\emph{metric}. Note that for convenience we often write \n",
      "Chunk 59: d_{ij} for the term d(i,j).\n",
      "\n",
      "We can represent a dissimilarity map d by an m\\times m matrix D whose (i,j)th entry is d_{ij}. Because D\n",
      "Chunk 60:  is clearly symmetric and all diagonal entries are trivial, there is a natural embedding of \n",
      "Chunk 61: d into the tropical space \\mathbb R^{e} = R^{\\binom m 2}\n",
      "Chunk 62: .\n",
      "\\end{definition}\n",
      "\n",
      "In fact, the condition of being a metric is intrinsically tropical.\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{metric-condition}\n",
      "Let \n",
      "Chunk 63: d:[m]\\times [m]\\to\\mathbb R_{\\geq 0} be a dissimilarity metric and D its corresponding matrix. Then d is a metric iff -D\\odot -D = -D.\n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "The (i,j)th entry of -D\\odot -D\n",
      "Chunk 64:  is equal to \n",
      "\\[\\bigoplus_{k=1}^m -d_{ik}-d_{kj} = \\max_{k\\in [m]}(-d_{ik}-d_{kj}).\\]\n",
      "In particular, we note that the \n",
      "Chunk 65: (i,j)th entry of -D\\odot -D is at least as large as -d_{ij}=-d_{ii}-d_{ij}\n",
      "Chunk 66: . Now a simple negation of the definition shows that \n",
      "Chunk 67: d is a metric iff -d_{ij}\\geq \\max_{k\\in [m]}(-d_{ik}-d_{kj}). \n",
      "\\end{proof}\n",
      "\n",
      "\\begin{definition}\n",
      "Let T=(V,E) be a tree with m\n",
      "Chunk 68:  labeled leaves and no vertices of degree two. We call such a tree a \\emph{phylogenetic tree}.\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{definition}\n",
      "Let \n",
      "Chunk 69: T be a phylogenetic tree with m leaves labeled with the elements of [m], and assign a length \\ell_e\\in \\mathbb R to each edge e of T. Let d:[m]\\times [m]\\to \\mathbb R be defined so that d_{ij}\n",
      "Chunk 70:  is the total length of the unique path from leaf \n",
      "Chunk 71: i to leaf j. We call a function d\n",
      "Chunk 72:  obtained in this way a \\emph{tree distance}. If, furthermore, each entry of the distance matrix \n",
      "Chunk 73: D is nonnegative, then d is in fact a metric. We call such a d a \\emph{tree metric}. As before, we can embed D into \\mathbb R^{e}\n",
      "Chunk 74: . \n",
      "\\end{definition}\n",
      "\n",
      "Of course, any tree distance differs from a tree metric by some scalar multiple of \n",
      "Chunk 75: {\\bf 1}\n",
      "Chunk 76: . Hence the sets of tree distances and tree metrics coincide in \n",
      "Chunk 77: \\mathbb R/{\\bf 1}\\RR. \n",
      "\n",
      "\\begin{definition}\n",
      "Let d:[m]\\times [m]\\to \\mathbb R_{\\geq 0}\n",
      "Chunk 78:  be a metric which satisfies the following strengthening of the triangle inequality for each choice of \n",
      "Chunk 79: i,j,k\\in [m]\n",
      "Chunk 80: :\n",
      "\\[d(i,k)\\leq \\max(d(i,j),d(j,k)).\\]\n",
      "We call such a metric an \\emph{ultrametric}. Let \n",
      "Chunk 81: \\mathcal U_m denote the collection of all ultrametrics in \\mathbb R^e/{\\bf 1}\\mathbb R\n",
      "Chunk 82: .\n",
      "\\end{definition}\n",
      "\n",
      "It is well-known that all ultrametrics are tree metrics. In fact, all ultrametrics are derived from \\emph{equidistant trees}, where all leaves have the same distance to some distinguished root vertex. Furthermore, the tree metric of an equidistant tree is an ultrametric; hence ultrametrics and equidistant trees convey equivalent information.\n",
      "\n",
      "Let \n",
      "Chunk 83: L_m denote the subspace of \\mathbb R^e defined by the linear equations x_{ij} - x_{ik} + x_{jk}=0 for 1\\leq i < j <k \\leq m. The tropicalization \\Trop(L_m)\\subseteq \\RR^e/\\RR {\\bf 1}\n",
      "Chunk 84:  is the tropical linear space consisting of points \n",
      "Chunk 85: (v_{12},v_{13},\\ldots, v_{m-1,m}) such that \\max(v_{ij},v_{ik},v_{jk}) is obtained at least twice for all triples i,j,k\\in [m]\n",
      "Chunk 86: .\n",
      "\n",
      "\\begin{remark}\n",
      "Experts in tropical geometry will note that the tropical linear space \n",
      "Chunk 87: \\Trop(L_m)\n",
      "Chunk 88:  corresponds to the graphic matroid of the complete graph \n",
      "Chunk 89: K_m\n",
      "Chunk 90: .\n",
      "\\end{remark}\n",
      "\n",
      "\\begin{theorem}\n",
      "\\label{ultrametrics}\n",
      "The image of \n",
      "Chunk 91: \\mathcal U_m in the tropical projective torus \\RR^e/\\RR {\\bf 1} coincides with \\Trop(L_m).\n",
      "\\end{theorem}\n",
      "\\begin{proof}\n",
      "Let (v_{12},v_{13},\\ldots, v_{m-1,m})\\in \\Trop(L_m)\n",
      "Chunk 92: . We may assume that each coordinate is nonnegative, so that this point corresponds to the image of a dissimilarity map \n",
      "Chunk 93: d. To see that d is in fact an ultrametric, fix i,j,k\\in [m]. We know that \\max(d_{ij},d_{ik}, d_{kj})\n",
      "Chunk 94:  is attained at least twice, by the definition of \n",
      "Chunk 95: \\Trop(L_m). If d_{ij}\n",
      "Chunk 96:  is one of these maximums then it must be equal to \n",
      "Chunk 97: \\max(d_{ik},d_{kj}). If d(i,j)\n",
      "Chunk 98:  is not one of these maximums then it must be strictly less than \n",
      "Chunk 99: \\max(d_{ik},d_{kj}). Either way, we have that d_{ij}\\leq \\max(d_{ik},d_{kj}). This shows that d is in fact an ultrametric, so that \\Trop(L_m)\\subseteq \\mathcal U_m.\n",
      "\n",
      "Let \\bar D\\in \\mathcal U_m. Then there exists some lifted ultrametric d with associated matrix D. Fix a choice of i,j,k\\in [m], and without loss of generality let i,j such that d_{ij}=\\max(d_{ij},d_{ik},d_{kj}). Because d is an ultrametric, we have that d_{ij}\\leq \\max(d_{ik},d_{kj}).  Hence in fact d_{ij}=\\max(d_{ik},d_{kj}), and the maximum of \\max(d_{ij},d_{ik},d_{kj}) is attained at least twice. Thus \\Trop(L_m)\\supseteq \\mathcal U_m\n",
      "Chunk 100: .\n",
      "\\end{proof}\n",
      "\n",
      "In words, Theorem \\ref{ultrametrics} states that the image of the space of labeled rooted trees is a tropical linear space. The set of equidistant trees thus has an intrinsic tropical structure.\n",
      "\n",
      "\\begin{comment}\n",
      "\n",
      "Tree metrics can be characterized in the following way:\n",
      "\n",
      "\\begin{lemma}[Four-point condition]\n",
      "\\label{four-point-condition}\n",
      "A metric \n",
      "Chunk 101: d on the set [n] is a tree metric iff, for any four elements u,v,x,y\\in [n], the maximum of d(u,v)+d(x,y), d(u,x)+d(v,y), and d(u,y)+d(x,v)\n",
      "Chunk 102:  is attained at least twice.\n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "See \\cite[Theorem 2.36]{PS}.\n",
      "\\end{proof}\n",
      "\n",
      "\\begin{remark}\n",
      "Let \n",
      "Chunk 103: T\\in\\mathbb R^e\n",
      "Chunk 104:  be a tree metric. Lemma \\ref{four-point-condition} implies that \n",
      "Chunk 105: T + \\lambda \\cdot {\\bf 1}\n",
      "Chunk 106:  is also a tree metric for any nonnegative choice of \n",
      "Chunk 107: \\lambda. \n",
      "\\end{remark}\n",
      "\n",
      "\\begin{definition}\n",
      "Let Q = (q_{ij}) be an n\\times n\n",
      "Chunk 108:  symmetric matrix with zeros on the diagonal whose off-diagonal entries are variables. For each collection of four elements \n",
      "Chunk 109: u,v,x,y\\in[n]\n",
      "Chunk 110: , define the quadratic tropical polynomial\n",
      "\\[g_{uvxy}(Q) = q_{uv}\\otimes q_{xy} \\oplus q_{ux}\\otimes q_{vy} \\oplus q_{uy}\\otimes q_{vx}.\\]\n",
      "Thus \n",
      "Chunk 111: g_{uvxy} defines a tropical hypersurface \\mathcal T(g_{uvxy}) in \\mathbb R^{\\binom n 2}/{\\bf 1}\\RR\n",
      "Chunk 112: . \n",
      "\\end{definition}\n",
      "\n",
      "\\begin{remark}\n",
      "In fact, the intersection of the \n",
      "Chunk 113: \\binom n 4 tropical hypersurfaces \\mathcal T(g_{uvxy})\n",
      "Chunk 114:  is a tropical linear space, known as the \\emph{tropical Grassmannian} \n",
      "Chunk 115: \\mathcal T(Gr(2,n)). Note that g_{uvxy}(Q) is the tropicalization of the Plucker relation p_{uv}p_{xy}-p_{ux}p_{vy}-p_{uy}p_{vx}.\n",
      "\\end{remark}\n",
      "\n",
      "\\begin{theorem}\n",
      "Let \\mathcal T\n",
      "Chunk 116:  be the collection of all tree metrics, identified with its image in \n",
      "Chunk 117: \\mathbb R^{\\binom n 2}/{\\bf 1}\\RR. Then \\mathcal T equals the tropical Grassmannian \\mathcal T(Gr(2,n))\n",
      "Chunk 118: .\n",
      "\\end{theorem}\n",
      "\\begin{proof}\n",
      "We show one containment. (For a full proof, see \\cite{MS}[Theorem 4.3.5].) Fix \n",
      "Chunk 119: T\\in \\mathcal T. We may assume that all coordinates of T are nonnegative, so that T\n",
      "Chunk 120:  is in fact a tree metric. Now Lemma \\ref{four-point-condition} states that \n",
      "Chunk 121: T is a tree metric if and only if, for all 1\\leq u<v<x<y\\leq n, the maximum of d(u,v)+d(x,y), d(u,x)+d(v,y), and d(u,y)+d(x,v)\n",
      "Chunk 122:  is attained at least twice. This is precisely what it means to be contained in intersection of the tropical hypersurfaces \n",
      "Chunk 123: \\mathcal T(g_{uvxy})=\\mathcal T(Gr(2,n)).\n",
      "\\end{proof}\n",
      "\n",
      "ULTRAMETRICS\n",
      "\n",
      "Let e:=\\binom m 2, and let L_m denote the (m-1)-dimensional subspace of \\mathbb R^e defined by the linear equations x_{ij} - x_{ik} + x_{jk}=0 for 1\\leq i < j <k \\leq m\n",
      "Chunk 124: . \n",
      "\n",
      "\\end{comment}\n",
      "\n",
      "\\section{Tropical PCA as a Stiefel tropical linear space}\\label{trop:lin}\n",
      "\n",
      "As noted in the introduction, one can interpret ordinary \n",
      "Chunk 125: (s-1)\n",
      "Chunk 126: th principal component analysis as a method of dimensionality reduction, replacing data points with their projections onto the translate of some particularly well-fitting linear space of dimension \n",
      "Chunk 127: s-1\n",
      "Chunk 128: . Classically, this translation of a well-fitted linear space can be described by an \n",
      "Chunk 129: (s\\times e)-dimensional matrix, whose first (s-1)\n",
      "Chunk 130:  rows are the basis vectors of the linear space, and whose last row is a translation vector from the origin.\n",
      "\n",
      "In analogy with the classical case, our approach to an \n",
      "Chunk 131: (s-1)\n",
      "Chunk 132: th tropical principal component analysis is to replace data points with their tropical projections onto the best-fit Stiefel tropical linear space of dimension \n",
      "Chunk 133: (s-1), defined by a tropical matrix of size s\\times e\n",
      "Chunk 134: .\n",
      "\n",
      "\\subsection{Best-fit tropical hyperplanes}\n",
      "\n",
      "We begin our discussion of tropical principal component analysis by\n",
      "considering a specific case: reducing by one the dimension\n",
      "of a collection of \n",
      "Chunk 135: e datapoints in \\RR^e/\\RR {\\bf 1}. In other words, we\n",
      "seek the (e-1)\n",
      "Chunk 136: th order tropical PCA, or a \\emph{best-fit tropical hyperplane}, for a collection of \n",
      "Chunk 137: e data points in \\RR^e/\\RR {\\bf 1}\n",
      "Chunk 138: .\n",
      "\n",
      "\\begin{comment}It is well-known that any tropical hyperplane is a \\emph{Stiefel tropical linear space}: that is, any tropical hyperplane in \n",
      "Chunk 139: \\TP^{e-1} is spanned by e-1\n",
      "Chunk 140:  points.\\cite{FR} In particular, it suffices to consider the valuated matroids corresponding to all \n",
      "Chunk 141: e\\times e matrices A with entries in \\overline \\RR.\n",
      "\n",
      "% The special case of this theorem when L_{\\bf w} has the form \n",
      "% {\\rm Trop}(M), for some rank r matroid M on [e]\n",
      "Chunk 142: , \n",
      "% was proved by Ardila in \\cite[Theorem 1]{Ard}.\n",
      "% Matroids correspond to the case when each tropical Pl\\\"ucker coordinate \n",
      "Chunk 143: w_\\sigma is\n",
      "% either 0 or -\\infty\n",
      "Chunk 144: .\n",
      "The application that motivated Ardila's study\n",
      "was the ultrametric tree space \n",
      "Chunk 145: \\mathcal{U}_m\n",
      "Chunk 146: .\n",
      "Here the nearest-point map computes the largest ultrametric\n",
      "dominated by a given dissimilarity map, a problem of importance\n",
      "in phylogenetics. An efficient algorithm for this problem was\n",
      "given by Chepoi and Fichet \\cite{CF}. This was recently\n",
      "revisited by Apostolico {\\it et al.}~in~\\cite{ACDP}.\n",
      "\n",
      "Returning to ideas for geometric statistics,\n",
      "the Blue Rule may serve as a subroutine for\n",
      "the numerical computation of regression planes.\n",
      "Let \n",
      "Chunk 147: d_1,\\ldots, d_n be data points in \\RR^{e}/\\RR {\\bf 1},\n",
      "lying in a tropically convex \n",
      "subset \\mathcal{P} of interest, such as\n",
      "\\mathcal{P} = \\mathcal{U}_m.\n",
      "The tropical regression plane of dimension s-1\n",
      "Chunk 148:  is a\n",
      "solution to the optimization problem\n",
      "\\begin{equation}\n",
      "\\label{eq:regression}\n",
      "\\argmin_{L_{\\bf w}} \\sum_{i=1}^n  d_{\\rm tr} (d_i, L_{\\bf w}).\n",
      "\\end{equation}\n",
      "Here \n",
      "Chunk 149: {\\bf w} runs over all points in the Dressian {\\rm Dr}(s,e),\n",
      "or in the tropical Grassmannian {\\rm Gr}(s,e)\n",
      "Chunk 150: .\n",
      "\n",
      "\\begin{remark}\\label{bernds}\n",
      "Maybe we can first compute the set of Fermat-Weber points of a sample\n",
      "\n",
      "Chunk 151: \\{d_1, \\ldots , d_n\\} and let {\\bf \\bar{x}} = (\\bar{x}^1, \\ldots\n",
      "\\bar{x}^e)\n",
      "Chunk 152:  be a Fermat-Weber point of the sample.    Then compute\n",
      "the tropical hyperplane \n",
      "\n",
      "Chunk 153: L_{{\\bf w}}\n",
      "Chunk 154:  which contains the  set of Fermat-Weber points of the\n",
      "sample by the considering the following tropical hyper plane\n",
      "\\[\n",
      "a_1 \\odot x_i \\oplus \\cdots \\oplus a_e \\odot x_e = \\max \\{a_1 + x_1, \\ldots ,\n",
      "a_e + x_e\\},\n",
      "\\]\n",
      "and setting \n",
      "Chunk 155: a_i = -\\bar{x}^i\n",
      "Chunk 156: .  \n",
      "Is it the\n",
      "optimal solution of our optimization \n",
      "problem?  If not then how far off?  \n",
      "\\end{remark}\n",
      "\n",
      "The distance of a nearest hyperplane to a set of \n",
      "Chunk 157: e\n",
      "Chunk 158:  data points is related to the \\emph{tropical volume}, from \\cite[Definition 2]{DGJ}:\n",
      "\n",
      "\\leoncomment{In the below writeup I used the min-plus convention, sorry. In a second pass I can edit things to use max-plus instead.}\\end{comment}\n",
      "\n",
      "We require the following definition, from \\cite{DGJ}.\n",
      "\n",
      "\\begin{definition}\n",
      "Let \n",
      "Chunk 159: A be an e\\times e matrix with entries in \\overline \\RR whose rows correspond to e points in \\RR^e/\\RR {\\bf 1}. The \\emph{tropical volume} of A\n",
      "Chunk 160:  is given by the expression\n",
      "\\[\\tvol A := \\bigoplus_{\\sigma\\in S_e} \\sum a_{i,\\sigma(i)} - \\bigoplus_{\\tau\\in S_e-\\sigma_{opt}}\\sum a_{i,\\tau(i)},\\]\n",
      "where \n",
      "Chunk 161: \\sigma_{opt}\n",
      "Chunk 162:  is an optimal permutation attaining the tropical determinant in the first tropical sum.\n",
      "\\end{definition}\n",
      "\n",
      "Recall that a square tropical matrix \n",
      "Chunk 163: A\n",
      "Chunk 164:  is \\emph{tropically singular} if two distinct permutations attain the tropical determinant. The following, from \\cite[Lemma 5.1]{RGST}, is one of the earliest results in tropical geometry:\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{common-hyperplane}\n",
      "Let \n",
      "Chunk 165: A be an e\\times e tropical matrix whose rows represent e points of \\RR^e/\\RR {\\bf 1}. Then A is tropically singular iff those k points lie on a tropical hyperplane in \\RR^e/\\RR {\\bf 1}. In particular, \\tvol(A)=0 iff the e\n",
      "Chunk 166:  points lie on a common tropical hyperplane.\n",
      "\\end{lemma}\n",
      "\n",
      "Of course, if our collection of \n",
      "Chunk 167: e datapoints D^{(i)}\n",
      "Chunk 168:  lie on a common hyperplane, then this hyperplane is our \n",
      "Chunk 169: (e-1)\n",
      "Chunk 170: th tropical PCA. This fact hints at some relationship between the tropical volume and the best fit hyperplane. In fact, this relationship is quite strong.\n",
      "\n",
      "\\begin{theorem}\n",
      "\\label{tropical-volume}\n",
      "Let \n",
      "Chunk 171: D^{(1)},\\ldots, D^{(e)} be a collection of e points in \\RR^e/\\RR {\\bf 1}\n",
      "Chunk 172: . Then the best-fit hyperplane attains a distance from the \n",
      "Chunk 173: e\n",
      "Chunk 174:  points equal to their tropical volume, and one such best-fit hyperplane is spanned by a choice of \n",
      "Chunk 175: e-1\n",
      "Chunk 176:  of the points.\n",
      "\\end{theorem}\n",
      "\n",
      "To prove this theorem, we first show that the tropical volume is an upper bound on the minimal distance of a best-fit tropical hyperplane.\n",
      "\n",
      "\\begin{lemma}\\label{upper-bound}\n",
      "Let \n",
      "Chunk 177: D^{(1)},\\ldots, D^{(e)} be a collection of e points in \\RR^e/\\RR {\\bf 1}, and let A be the matrix whose i,jth entry is D^{(i)}_j. Then there exists a hyperplane of distance \\tvol A from the data points, spanned by some choice of e-1\n",
      "Chunk 178:  of the points.\n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "Suppose that all \n",
      "Chunk 179: e\n",
      "Chunk 180:  data points can be spanned by a single hyperplane. Then Lemma \\ref{common-hyperplane} tells us that this best-fit hyperplane is of distance \n",
      "Chunk 181: \\tvol A = 0 from the data points.\n",
      "\n",
      "Now suppose that the e\n",
      "Chunk 182:  data points do not lie on the same hyperplane. %Because we are working in \n",
      "Chunk 183: \\TP^{e-1}\n",
      "Chunk 184: , we are free to choose the last coordinate of each data point to be 0. \n",
      "Without loss of generality, we may assume that the data points \n",
      "Chunk 185: D^{(1)},\\ldo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='white-space: pre-wrap; background-color: #f5f5f5; padding: 10px; border-radius: 5px;'><span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> indexed by subsets \\sigma \\in \\binom{[e]}{s}.<br>Among the L_{\\bf w}</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> are the tropicalized linear spaces<br>\\cite[Theorem 4.3.17]{MS}. Even more special are <br>linear spaces spanned by </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">s  points; cf.~\\cite{FR}. If L_{\\bf w} is spanned by<br>   x^1,  \\ldots, x^s in \\RR^e/\\RR {\\bf 1} then its Pl\\&quot;ucker coordinate<br>w_\\sigma  is the<br>{\\em tropical determinant} of the  s \\times s-submatrix  indexed by \\sigma of the<br>s \\times e-matrix  X = (x^1,\\ldots,x^s). Note that<br>all tropical linear spaces L_{\\bf w}</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> are tropically convex.<br><br>We are interested in the nearest point map<br></span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\pi_{L_{\\bf w}} that takes a point u to <br>the largest point in L_{\\bf w} dominated by u</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">, as <br>seen   in (\\ref{eq:tropproj}).\\end{comment}<br><br>In order to consider a ``tropical principal component analysis&#x27;&#x27;, as described at the beginning of this section, we must be able to project onto a tropical linear space. This projection operation is described by the \\emph{Red and Blue Rules}. From \\cite[Theorem 15]{JSY} we have:<br><br>\\begin{theorem}[The Blue Rule]<br>\\label{blue-rule}<br>Let </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">p:[e]^d\\to \\overline \\RR be a tropical Pl\\&quot;ucker vector and L_p its associated tropical linear space in \\RR^e/\\RR {\\bf 1}. Fix u\\in\\RR^e/\\RR {\\bf 1}, and define the point w\\in\\RR^e/\\RR {\\bf 1} whose i</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">th coordinate is<br>\\begin{equation}<br>\\label{eq:bluerule}<br> \\quad w_i \\,\\, = \\,\\,<br> {\\rm max}_\\tau \\,{\\rm min}_{j \\not\\in \\tau} \\bigl( u_j + p({\\tau \\cup \\{i\\}}) - p({\\tau \\cup \\{j\\}}) \\bigr)<br> \\qquad {\\rm for} \\,\\,\\, i = 1,2,\\ldots, e<br> \\end{equation}<br>where </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\tau runs over all (d-1)-subsets of [e] that do not contain i.<br><br>Then w\\in L_p, and any other x\\in L_p satisfies d(u,x)\\geq d(u,w). In other words, w attains the minimum distance of any point in L_p to u</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>\\end{theorem}<br><br>\\begin{theorem}[The Red Rule]<br>\\label{red-rule}<br>Let </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">p:[e]^d\\to \\overline \\RR be a tropical Pl\\&quot;ucker vector and L_p its associated tropical linear space in \\RR^e/\\RR {\\bf 1}. Fix u\\in\\RR^e/\\RR {\\bf 1}. Let v be the all-zeros vector. For every (d+1)-sized subset \\tau of [e], compute \\max p({\\tau-\\tau_i}) + u_{\\tau_i}. If this maximum is unique, attained with index \\tau_i, then let \\gamma_{\\tau,\\tau_i}</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be the positive difference between the second maximum and this maximum, and set </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">v_{\\tau_i}=\\max(v_{\\tau_i}, \\gamma_{\\tau,\\tau_i}).<br><br>Then v gives the difference between u and a closest point of L_p. In particular, if w is the point in L_p</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> returned by the Blue Rule, we have <br>\\[u = w + v.\\]<br>\\end{theorem}<br><br>We write </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\pi_{L_p} as the projection function which takes a point u\\in \\RR^e/\\RR {\\bf 1} and returns the nearest point w\\in L_p</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> given by the Blue Rule.<br><br>\\begin{example}<br>\\label{red-blue-rule-example}<br>Let </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">A</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be the matrix of Example \\ref{tropical-linear-space-example}, with </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">p and L_p</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> its associated tropical Pl\\&quot;ucker vector and Stiefel tropical linear space. Let </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">u be the point (1, -2, 3)\\in\\RR^e/\\RR {\\bf 1}. <br><br>The Blue Rule constructs a point w\\in\\RR^3/\\RR{\\bf 1}</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> whose first coordinate is<br>\\[\\max(\\min(u_1+p(\\{1,2\\})-p(\\{1,2\\}), u_3+p(\\{1,2\\})-p(\\{1,3\\})),\\]<br>\\[\\min(u_1+p(\\{1,3\\})-p(\\{1,3\\}),u_2+p(\\{1,3\\})-p(\\{2,3\\}))).\\]<br>Substituting in, we get the first coordinate of </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">w</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> as <br>\\[w_1 = \\max(\\min(1 + 2 -2, 3 + 2 - 4), \\min(1 + 4 - 4, -2 + 4 - 3)) = \\max(1, -1) = 1.\\]<br>Similarly, we get </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">w_2 = -2 and w_3 = 0. So the Blue Rule outputs the vector (1, -2, 2).<br><br>The Red Rule constructs a vector v as follows. First, we begin with v = (0, 0, 0). Next we take the set \\tau = [e] and compute </span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\max(p(\\{2,3\\})+u_1, p(\\{1,3\\})+u_2, p(\\{1,2\\})+u_3) = \\max(3 + 1, 4 -2, 2 + 3) = 5.</span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> So the Red Rule redefines v_3 = 5 - 4 = 1, and hence outputs the vector v=(0,0,1). Now Theorem \\ref{red-rule} states that u = w + v</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">, which is easily verified to hold.<br>\\end{example}<br><br>\\begin{definition}<br>Let </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">v = (v_1,\\ldots, v_e)</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be a real vector, and define the tropical linear functional </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\bigoplus (-v_i)\\otimes x_i. Let \\mathcal H</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be the tropical solution set of this linear functional: that is, </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathcal H consists of all x\\in\\RR^e/\\RR {\\bf 1} such that the maximum of \\bigoplus( -v_i)\\otimes x_i is attained at least twice. We call any \\mathcal H</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> obtained in this way a \\emph{tropical hyperplane}.<br>\\end{definition}<br><br>\\begin{remark}<br>\\label{tropical-hyperplane-valuated-matroid}<br>Let </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">A be a tropical matrix of dimensions (e-1)\\times e. Then the Stiefel tropical linear space of A</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is a tropical hyperplane. Furthermore, any tropical hyperplane is the Stiefel tropical linear space of such a tropical matrix </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">A</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>\\end{remark}<br>\\begin{comment}<br>\\begin{definition}<br>Let </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">p:[e]^{e-1}\\to\\overline \\RR</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be a Stiefel tropical linear space. Then we call the tropical linear space </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathcal H corresponding to p</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> a \\emph{tropical hyperplane}.<br>\\end{definition}<br>\\begin{lemma}<br>\\label{tropical-hyperplane-valuated-matroid}<br>Let </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">v=(v_1,\\ldots, v_e)\\in\\mathbb \\overline \\RR^e</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be any vector. Then there exists a tropical Pl\\&quot;ucker vector </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">p:[e]^{e-1}\\to\\overline \\RR</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> defined by<br>\\[p([e]-\\{i\\})=v_i.\\]<br>\\end{lemma}<br>\\begin{proof}<br>We check only the exchange relation. Let </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\sigma be an (e-2)-sized subset of [e], and let \\tau be [e], the only e-sized subset of [e]</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. We need that the maximum<br>\\[\\max_{i\\in [e]}(p(\\sigma\\cup\\{i\\}) + p([e]-\\{i\\}))\\]<br>is attained at least twice. But if </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">i</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is a choice of index which attains a maximum, then </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">[e]-\\sigma-\\{i\\}</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> attains the same value. So the maximum is indeed attained at least twice.<br>\\end{proof}<br><br>In tropical geometry, tropical hyperplanes are often introduced in a different form.<br><br>\\begin{lemma}<br>\\label{tropical-hyperplane}<br>Let </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathcal H be a tropical hyperplane in \\TP^{e-1} and p the tropical Pl\\&quot;ucker vector corresponding to \\mathcal H. Write v_i:=p([e]-\\{i\\}). Then \\mathcal H</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is the tropical solution set of the tropical linear functional </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\bigoplus (-v_i)\\otimes x_i</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>\\end{lemma}<br>\\end{comment}<br><br>\\subsection{A tropical interpretation for phylogenetic trees}<br>\\label{tropical-interpretation-for-phylogenetics}<br><br>In this section we describe some of the tropical aspects underlying the study of phylogenetic trees. Our treatment of this subject largely follows \\cite[Section 4.3]{MS}.<br><br>\\begin{definition}<br>A \\emph{dissimilarity map} </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">d is a function d:[m]\\times [m]\\to \\mathbb R_{\\geq 0} such that d(i,i)=0 and d(i,j)=d(j,i)\\geq 0 for each i,j\\in [m]. If, furthermore, we have that d(i,j)\\leq d(i,k)+d(k,j) for all i,j,k\\in [m], we call d</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> a \\emph{metric}. Note that for convenience we often write </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">d_{ij} for the term d(i,j).<br><br>We can represent a dissimilarity map d by an m\\times m matrix D whose (i,j)th entry is d_{ij}. Because D</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is clearly symmetric and all diagonal entries are trivial, there is a natural embedding of </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">d into the tropical space \\mathbb R^{e} = R^{\\binom m 2}</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>\\end{definition}<br><br>In fact, the condition of being a metric is intrinsically tropical.<br><br>\\begin{lemma}<br>\\label{metric-condition}<br>Let </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">d:[m]\\times [m]\\to\\mathbb R_{\\geq 0} be a dissimilarity metric and D its corresponding matrix. Then d is a metric iff -D\\odot -D = -D.<br>\\end{lemma}<br>\\begin{proof}<br>The (i,j)th entry of -D\\odot -D</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is equal to <br>\\[\\bigoplus_{k=1}^m -d_{ik}-d_{kj} = \\max_{k\\in [m]}(-d_{ik}-d_{kj}).\\]<br>In particular, we note that the </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">(i,j)th entry of -D\\odot -D is at least as large as -d_{ij}=-d_{ii}-d_{ij}</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. Now a simple negation of the definition shows that </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">d is a metric iff -d_{ij}\\geq \\max_{k\\in [m]}(-d_{ik}-d_{kj}). <br>\\end{proof}<br><br>\\begin{definition}<br>Let T=(V,E) be a tree with m</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> labeled leaves and no vertices of degree two. We call such a tree a \\emph{phylogenetic tree}.<br>\\end{definition}<br><br>\\begin{definition}<br>Let </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">T be a phylogenetic tree with m leaves labeled with the elements of [m], and assign a length \\ell_e\\in \\mathbb R to each edge e of T. Let d:[m]\\times [m]\\to \\mathbb R be defined so that d_{ij}</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is the total length of the unique path from leaf </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">i to leaf j. We call a function d</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> obtained in this way a \\emph{tree distance}. If, furthermore, each entry of the distance matrix </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">D is nonnegative, then d is in fact a metric. We call such a d a \\emph{tree metric}. As before, we can embed D into \\mathbb R^{e}</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. <br>\\end{definition}<br><br>Of course, any tree distance differs from a tree metric by some scalar multiple of </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">{\\bf 1}</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. Hence the sets of tree distances and tree metrics coincide in </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathbb R/{\\bf 1}\\RR. <br><br>\\begin{definition}<br>Let d:[m]\\times [m]\\to \\mathbb R_{\\geq 0}</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be a metric which satisfies the following strengthening of the triangle inequality for each choice of </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">i,j,k\\in [m]</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\">:<br>\\[d(i,k)\\leq \\max(d(i,j),d(j,k)).\\]<br>We call such a metric an \\emph{ultrametric}. Let </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathcal U_m denote the collection of all ultrametrics in \\mathbb R^e/{\\bf 1}\\mathbb R</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>\\end{definition}<br><br>It is well-known that all ultrametrics are tree metrics. In fact, all ultrametrics are derived from \\emph{equidistant trees}, where all leaves have the same distance to some distinguished root vertex. Furthermore, the tree metric of an equidistant tree is an ultrametric; hence ultrametrics and equidistant trees convey equivalent information.<br><br>Let </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">L_m denote the subspace of \\mathbb R^e defined by the linear equations x_{ij} - x_{ik} + x_{jk}=0 for 1\\leq i &lt; j &lt;k \\leq m. The tropicalization \\Trop(L_m)\\subseteq \\RR^e/\\RR {\\bf 1}</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is the tropical linear space consisting of points </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">(v_{12},v_{13},\\ldots, v_{m-1,m}) such that \\max(v_{ij},v_{ik},v_{jk}) is obtained at least twice for all triples i,j,k\\in [m]</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br><br>\\begin{remark}<br>Experts in tropical geometry will note that the tropical linear space </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\Trop(L_m)</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> corresponds to the graphic matroid of the complete graph </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">K_m</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>\\end{remark}<br><br>\\begin{theorem}<br>\\label{ultrametrics}<br>The image of </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathcal U_m in the tropical projective torus \\RR^e/\\RR {\\bf 1} coincides with \\Trop(L_m).<br>\\end{theorem}<br>\\begin{proof}<br>Let (v_{12},v_{13},\\ldots, v_{m-1,m})\\in \\Trop(L_m)</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. We may assume that each coordinate is nonnegative, so that this point corresponds to the image of a dissimilarity map </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">d. To see that d is in fact an ultrametric, fix i,j,k\\in [m]. We know that \\max(d_{ij},d_{ik}, d_{kj})</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is attained at least twice, by the definition of </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\Trop(L_m). If d_{ij}</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is one of these maximums then it must be equal to </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\max(d_{ik},d_{kj}). If d(i,j)</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is not one of these maximums then it must be strictly less than </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\max(d_{ik},d_{kj}). Either way, we have that d_{ij}\\leq \\max(d_{ik},d_{kj}). This shows that d is in fact an ultrametric, so that \\Trop(L_m)\\subseteq \\mathcal U_m.<br><br>Let \\bar D\\in \\mathcal U_m. Then there exists some lifted ultrametric d with associated matrix D. Fix a choice of i,j,k\\in [m], and without loss of generality let i,j such that d_{ij}=\\max(d_{ij},d_{ik},d_{kj}). Because d is an ultrametric, we have that d_{ij}\\leq \\max(d_{ik},d_{kj}).  Hence in fact d_{ij}=\\max(d_{ik},d_{kj}), and the maximum of \\max(d_{ij},d_{ik},d_{kj}) is attained at least twice. Thus \\Trop(L_m)\\supseteq \\mathcal U_m</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>\\end{proof}<br><br>In words, Theorem \\ref{ultrametrics} states that the image of the space of labeled rooted trees is a tropical linear space. The set of equidistant trees thus has an intrinsic tropical structure.<br><br>\\begin{comment}<br><br>Tree metrics can be characterized in the following way:<br><br>\\begin{lemma}[Four-point condition]<br>\\label{four-point-condition}<br>A metric </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">d on the set [n] is a tree metric iff, for any four elements u,v,x,y\\in [n], the maximum of d(u,v)+d(x,y), d(u,x)+d(v,y), and d(u,y)+d(x,v)</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is attained at least twice.<br>\\end{lemma}<br>\\begin{proof}<br>See \\cite[Theorem 2.36]{PS}.<br>\\end{proof}<br><br>\\begin{remark}<br>Let </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">T\\in\\mathbb R^e</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be a tree metric. Lemma \\ref{four-point-condition} implies that </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">T + \\lambda \\cdot {\\bf 1}</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is also a tree metric for any nonnegative choice of </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\lambda. <br>\\end{remark}<br><br>\\begin{definition}<br>Let Q = (q_{ij}) be an n\\times n</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> symmetric matrix with zeros on the diagonal whose off-diagonal entries are variables. For each collection of four elements </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">u,v,x,y\\in[n]</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\">, define the quadratic tropical polynomial<br>\\[g_{uvxy}(Q) = q_{uv}\\otimes q_{xy} \\oplus q_{ux}\\otimes q_{vy} \\oplus q_{uy}\\otimes q_{vx}.\\]<br>Thus </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">g_{uvxy} defines a tropical hypersurface \\mathcal T(g_{uvxy}) in \\mathbb R^{\\binom n 2}/{\\bf 1}\\RR</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. <br>\\end{definition}<br><br>\\begin{remark}<br>In fact, the intersection of the </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\binom n 4 tropical hypersurfaces \\mathcal T(g_{uvxy})</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is a tropical linear space, known as the \\emph{tropical Grassmannian} </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathcal T(Gr(2,n)). Note that g_{uvxy}(Q) is the tropicalization of the Plucker relation p_{uv}p_{xy}-p_{ux}p_{vy}-p_{uy}p_{vx}.<br>\\end{remark}<br><br>\\begin{theorem}<br>Let \\mathcal T</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be the collection of all tree metrics, identified with its image in </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathbb R^{\\binom n 2}/{\\bf 1}\\RR. Then \\mathcal T equals the tropical Grassmannian \\mathcal T(Gr(2,n))</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>\\end{theorem}<br>\\begin{proof}<br>We show one containment. (For a full proof, see \\cite{MS}[Theorem 4.3.5].) Fix </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">T\\in \\mathcal T. We may assume that all coordinates of T are nonnegative, so that T</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is in fact a tree metric. Now Lemma \\ref{four-point-condition} states that </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">T is a tree metric if and only if, for all 1\\leq u&lt;v&lt;x&lt;y\\leq n, the maximum of d(u,v)+d(x,y), d(u,x)+d(v,y), and d(u,y)+d(x,v)</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is attained at least twice. This is precisely what it means to be contained in intersection of the tropical hypersurfaces </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathcal T(g_{uvxy})=\\mathcal T(Gr(2,n)).<br>\\end{proof}<br><br>ULTRAMETRICS<br><br>Let e:=\\binom m 2, and let L_m denote the (m-1)-dimensional subspace of \\mathbb R^e defined by the linear equations x_{ij} - x_{ik} + x_{jk}=0 for 1\\leq i &lt; j &lt;k \\leq m</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. <br><br>\\end{comment}<br><br>\\section{Tropical PCA as a Stiefel tropical linear space}\\label{trop:lin}<br><br>As noted in the introduction, one can interpret ordinary </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">(s-1)</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">th principal component analysis as a method of dimensionality reduction, replacing data points with their projections onto the translate of some particularly well-fitting linear space of dimension </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">s-1</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. Classically, this translation of a well-fitted linear space can be described by an </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">(s\\times e)-dimensional matrix, whose first (s-1)</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> rows are the basis vectors of the linear space, and whose last row is a translation vector from the origin.<br><br>In analogy with the classical case, our approach to an </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">(s-1)</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">th tropical principal component analysis is to replace data points with their tropical projections onto the best-fit Stiefel tropical linear space of dimension </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">(s-1), defined by a tropical matrix of size s\\times e</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br><br>\\subsection{Best-fit tropical hyperplanes}<br><br>We begin our discussion of tropical principal component analysis by<br>considering a specific case: reducing by one the dimension<br>of a collection of </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">e datapoints in \\RR^e/\\RR {\\bf 1}. In other words, we<br>seek the (e-1)</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">th order tropical PCA, or a \\emph{best-fit tropical hyperplane}, for a collection of </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">e data points in \\RR^e/\\RR {\\bf 1}</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br><br>\\begin{comment}It is well-known that any tropical hyperplane is a \\emph{Stiefel tropical linear space}: that is, any tropical hyperplane in </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\TP^{e-1} is spanned by e-1</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> points.\\cite{FR} In particular, it suffices to consider the valuated matroids corresponding to all </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">e\\times e matrices A with entries in \\overline \\RR.<br><br>% The special case of this theorem when L_{\\bf w} has the form <br>% {\\rm Trop}(M), for some rank r matroid M on [e]</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">, <br>% was proved by Ardila in \\cite[Theorem 1]{Ard}.<br>% Matroids correspond to the case when each tropical Pl\\&quot;ucker coordinate </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">w_\\sigma is<br>% either 0 or -\\infty</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>The application that motivated Ardila&#x27;s study<br>was the ultrametric tree space </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\mathcal{U}_m</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br>Here the nearest-point map computes the largest ultrametric<br>dominated by a given dissimilarity map, a problem of importance<br>in phylogenetics. An efficient algorithm for this problem was<br>given by Chepoi and Fichet \\cite{CF}. This was recently<br>revisited by Apostolico {\\it et al.}~in~\\cite{ACDP}.<br><br>Returning to ideas for geometric statistics,<br>the Blue Rule may serve as a subroutine for<br>the numerical computation of regression planes.<br>Let </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">d_1,\\ldots, d_n be data points in \\RR^{e}/\\RR {\\bf 1},<br>lying in a tropically convex <br>subset \\mathcal{P} of interest, such as<br>\\mathcal{P} = \\mathcal{U}_m.<br>The tropical regression plane of dimension s-1</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is a<br>solution to the optimization problem<br>\\begin{equation}<br>\\label{eq:regression}<br>\\argmin_{L_{\\bf w}} \\sum_{i=1}^n  d_{\\rm tr} (d_i, L_{\\bf w}).<br>\\end{equation}<br>Here </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">{\\bf w} runs over all points in the Dressian {\\rm Dr}(s,e),<br>or in the tropical Grassmannian {\\rm Gr}(s,e)</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.<br><br>\\begin{remark}\\label{bernds}<br>Maybe we can first compute the set of Fermat-Weber points of a sample<br></span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\{d_1, \\ldots , d_n\\} and let {\\bf \\bar{x}} = (\\bar{x}^1, \\ldots<br>\\bar{x}^e)</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> be a Fermat-Weber point of the sample.    Then compute<br>the tropical hyperplane <br></span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">L_{{\\bf w}}</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> which contains the  set of Fermat-Weber points of the<br>sample by the considering the following tropical hyper plane<br>\\[<br>a_1 \\odot x_i \\oplus \\cdots \\oplus a_e \\odot x_e = \\max \\{a_1 + x_1, \\ldots ,<br>a_e + x_e\\},<br>\\]<br>and setting </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">a_i = -\\bar{x}^i</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\">.  <br>Is it the<br>optimal solution of our optimization <br>problem?  If not then how far off?  <br>\\end{remark}<br><br>The distance of a nearest hyperplane to a set of </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">e</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> data points is related to the \\emph{tropical volume}, from \\cite[Definition 2]{DGJ}:<br><br>\\leoncomment{In the below writeup I used the min-plus convention, sorry. In a second pass I can edit things to use max-plus instead.}\\end{comment}<br><br>We require the following definition, from \\cite{DGJ}.<br><br>\\begin{definition}<br>Let </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">A be an e\\times e matrix with entries in \\overline \\RR whose rows correspond to e points in \\RR^e/\\RR {\\bf 1}. The \\emph{tropical volume} of A</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is given by the expression<br>\\[\\tvol A := \\bigoplus_{\\sigma\\in S_e} \\sum a_{i,\\sigma(i)} - \\bigoplus_{\\tau\\in S_e-\\sigma_{opt}}\\sum a_{i,\\tau(i)},\\]<br>where </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\sigma_{opt}</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is an optimal permutation attaining the tropical determinant in the first tropical sum.<br>\\end{definition}<br><br>Recall that a square tropical matrix </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">A</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> is \\emph{tropically singular} if two distinct permutations attain the tropical determinant. The following, from \\cite[Lemma 5.1]{RGST}, is one of the earliest results in tropical geometry:<br><br>\\begin{lemma}<br>\\label{common-hyperplane}<br>Let </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">A be an e\\times e tropical matrix whose rows represent e points of \\RR^e/\\RR {\\bf 1}. Then A is tropically singular iff those k points lie on a tropical hyperplane in \\RR^e/\\RR {\\bf 1}. In particular, \\tvol(A)=0 iff the e</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> points lie on a common tropical hyperplane.<br>\\end{lemma}<br><br>Of course, if our collection of </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">e datapoints D^{(i)}</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> lie on a common hyperplane, then this hyperplane is our </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">(e-1)</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\">th tropical PCA. This fact hints at some relationship between the tropical volume and the best fit hyperplane. In fact, this relationship is quite strong.<br><br>\\begin{theorem}<br>\\label{tropical-volume}<br>Let </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">D^{(1)},\\ldots, D^{(e)} be a collection of e points in \\RR^e/\\RR {\\bf 1}</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">. Then the best-fit hyperplane attains a distance from the </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">e</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> points equal to their tropical volume, and one such best-fit hyperplane is spanned by a choice of </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">e-1</span> <span style=\"background-color: lightsalmon; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> of the points.<br>\\end{theorem}<br><br>To prove this theorem, we first show that the tropical volume is an upper bound on the minimal distance of a best-fit tropical hyperplane.<br><br>\\begin{lemma}\\label{upper-bound}<br>Let </span> <span style=\"background-color: lightseagreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\">D^{(1)},\\ldots, D^{(e)} be a collection of e points in \\RR^e/\\RR {\\bf 1}, and let A be the matrix whose i,jth entry is D^{(i)}_j. Then there exists a hyperplane of distance \\tvol A from the data points, spanned by some choice of e-1</span> <span style=\"background-color: lightsteelblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> of the points.<br>\\end{lemma}<br>\\begin{proof}<br>Suppose that all </span> <span style=\"background-color: lightgoldenrodyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">e</span> <span style=\"background-color: lightcyan; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> data points can be spanned by a single hyperplane. Then Lemma \\ref{common-hyperplane} tells us that this best-fit hyperplane is of distance </span> <span style=\"background-color: lightblue; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\tvol A = 0 from the data points.<br><br>Now suppose that the e</span> <span style=\"background-color: lightgreen; padding: 2px 4px; border-radius: 3px; margin: 1px;\"> data points do not lie on the same hyperplane. %Because we are working in </span> <span style=\"background-color: lightpink; padding: 2px 4px; border-radius: 3px; margin: 1px;\">\\TP^{e-1}</span> <span style=\"background-color: lightyellow; padding: 2px 4px; border-radius: 3px; margin: 1px;\">, we are free to choose the last coordinate of each data point to be 0. <br>Without loss of generality, we may assume that the data points </span> <span style=\"background-color: lightcoral; padding: 2px 4px; border-radius: 3px; margin: 1px;\">D^{(1)},\\ldo</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机选取的文本片段起始索引: 86108, 长度: 20000\n",
      "选取的文本片段:\n",
      "\\\\\n",
      "     \\hline\n",
      "     $D^{(3)}$ & 1 & 7.331937 & 7.331937\\\\\n",
      "     \\hline\\hline\n",
      "     \\end{tabular}\n",
      "}{%\n",
      "  \\caption{Vectorized Distance Matrices}%\n",
      "  \\label{table 2}\n",
      "}\n",
      "\\end{floatrow}\n",
      "\\end{figure}\n",
      "\n",
      "Using our optimization problem formulation from Proposition \\ref{polytope_projection}, we obtain $D^{(1)},D^{(2)},D^{(3)}$ for this example. These points are ultrametrics, and they are described in Figure \\ref{figure 2} and Table \\ref{table 2}. In fact, in this case the best-fit tropical polytope contains all the equidistant trees, so that the sum of distances is zero.\n",
      "\\end{example}\n",
      "\n",
      "\\subsection{Approximative algorithms}\n",
      "For larger datasets, we turn to the approximative Algorithms \\ref{tropical-linear-space-algorithm} and \\ref{al1}. We implemented both algorithms in {\\tt R}.\\footnote{Our software for all computations can be downloaded at \\url{http://polytopes.net/computations/tropicalPCA/}.} We then generated a random sample from {\\tt Mesquite} \\cite{Mesq} and applied our algorithms on this dataset. The sample was constructed as follows:\n",
      "\\begin{algorithm}[Generating the simulation dataset]\\label{sim_data} \n",
      "                                \\qquad  \\qquad \\qquad \n",
      "\n",
      "\\begin{enumerate}\n",
      "\\item Generate 250 gene trees with 8 leaves from the coalescent model under a fixed species tree with depth equal to 10 \n",
      "\\item Transform the gene trees to be ultrametrics.\n",
      "\\item Compute approximate second order tropical principal components via the Algorithms \\ref{tropical-linear-space-algorithm} and \\ref{al1}.\n",
      "\\end{enumerate}\n",
      "\\end{algorithm}\n",
      "\n",
      "We applied both methods of tropical principal component analysis to a set of random trees generated by Algorithm \\ref{sim_data}. In analogy with \\cite{NTWY}, we define summary statistics to describe the fit of a Stiefel tropical linear space\n",
      "or a tropical polytope to a given data\n",
      "set. If $L_p$ is a Stiefel tropical linear space, we define its distance to the datapoints $d(L_p)$ as\n",
      "\\[d(L_p) = \\sum_i d(D^{(i)}, L_p),\\]\n",
      "and a tropical proportion of variance statistic\n",
      "\\[r(L_p) = \\frac{\\sum_i d_{tr}(\\bar \\pi, \\pi_{L_p}(D^{(i)}))}{\\sum_i d_{tr}(D^{(i)}, \\pi_{L_p}(D^{(i)})+\\sum_i d_{tr}(\\bar \\pi, \\pi_{L_p}(D^{(i)}))}\\]\n",
      "where $\\bar \\pi$ denotes a Fermat-Weber point of the projections of the datapoints, as in \\cite{LY}. These statistics are defined analogously for a tropical polytope $\\mathcal P$. The statistic $r(L_p)$ can be interpreted as the proportion of variance explained by $L_p$; in order to remain consistent with the tropical metric, we sum distances rather than squared distances. \n",
      "\n",
      "For the polytopal approach, as noted above, the projections will remain ultrametrics. We therefore analyze the topologies of these projections, and compare them with the topology of the species tree.\n",
      "\n",
      "\\subsection{Approximation results}\n",
      "\n",
      "We applied Algorithm \\ref{tropical-linear-space-algorithm} to find an approximate 2-dimensional best-fit Stiefel tropical linear space with a convergence threshold of 100 iterations. The summary statistics for this run were: $d(L_p)=363.0378$ and $r(L_p) = 0.322$.\n",
      "\n",
      "We also applied a variant of Algorithm \\ref{al1} to find an approximate best-fit tropical polytope with three vertices, in which we enumerated through all $\\binom{250}{3}$ different choices. The summary statistics were: $d(\\mathcal P) = 360.6831$ and \n",
      "$r(\\mathcal P) = 0.265$. We note that the overall sum of distances is similar between the two methods, but that the best-fit Stiefel tropical linear space explains a slightly higher proportion of variance. \n",
      "\n",
      "For the tropical polytope method, we recall that projections of equidistant trees will remain ultrametrics. We present common topologies of the projections as well as the species tree topology\n",
      "in Figure \\ref{sim_topology}.\\footnote{Tree topologies of all projected\n",
      "points can be found in the supplement \n",
      "at \\url{http://polytopes.net/computations/tropicalPCA/}.}\n",
      "We observe that these topologies of\n",
      "projected trees are broadly consistent with the topology of the species tree\n",
      "under which these gene trees were generated: taxa $g$ and $c$ group\n",
      "together, as do taxa $h$ and $f$, and the four taxa $a$, $b$, $d$, $e$ also group together. We can view our best-fit tropical polytope as preserving these features of the species tree, meaning that this tropical polytope retains information after projection.\n",
      "\\begin{figure}[!ht]\n",
      "\\centering\n",
      "\\includegraphics[width=0.6\\linewidth]{proj_sim_250_8_topology.pdf}\n",
      "  \\caption{Topology frequencies after projections: the parenthesized numbers are frequencies, and the last tree gives the species tree topology.}\n",
      "  \\label{sim_topology}\n",
      "\\end{figure}\n",
      "\n",
      "\\section{Apicomplexa genome}\\label{apicomplexa}\n",
      "\n",
      "We also applied our tropical principal component algorithms to a set of trees constructed from 252\n",
      "orthologous sequences on eight species of protozoa in the\n",
      "Apicomplexa phylum by \\cite{KWK}.   \n",
      "This dataset was also analyzed by Weyenberg et.~al; one can find\n",
      "more details, such as the gene sequences, in \\cite{KDETrees}.\n",
      "Because ordinary PCA is sensitive to outliers, we removed 16 outlier gene trees identified by \\cite{KDETrees} before fitting the tropical principal components.\n",
      "\n",
      "To find an approximate best-fit 2-dimensional Stiefel tropical linear space, we applied Algorithm \\ref{tropical-linear-space-algorithm} with a convergence threshold of 100 iterations. Due to the stochastic nature of the algorithm, we executed the algorithm three times. %Tree topologies from a representative execution are presented in Table \\ref{apicomplexa-tropical-linear-space}. We note the close similarity between the most common tree topology with the generally-accepted phylogeny of these eight taxa \\cite[Figure 1]{KWK}. \n",
      "The summary statistics remained consistent between these runs. For one representative execution, these statistics were: $d(L_p) = 145.38$ and $r(L_p)=0.616$. % In general, furthermore, projected topologies were largely congruent with the generally accepted phylogeny: the two Plasmodium species (Pv and Pf) group together, as do Ta and Bb; and Tt is isolated on a deep branch.\n",
      "\n",
      "\\begin{comment}\\begin{table}\n",
      "\\centering\n",
      "\\caption{Tree topologies and frequency counts for  apicomplexa gene tree projections}\n",
      "\\label{apicomplexa-tropical-linear-space}\n",
      "\\begin{tabular}{|c|c|c|c|}\n",
      "\\hline\n",
      "Count & Topology & Count & Topology\\\\\\hline\n",
      "112 & \\includegraphics[height=1.65in]{apicomplexa-tree3.png} & 31 & \\includegraphics[height=1.65in]{apicomplexa-tree13.png}\\\\ \\hline\n",
      "30 & \\includegraphics[height=1.65in]{apicomplexa-tree9.png} & 26 & \\includegraphics[height=1.65in]{apicomplexa-tree1.png}\\\\ \\hline\n",
      "20 & \\includegraphics[height=1.65in]{apicomplexa-tree4.png} & 17 & \\includegraphics[height=1.65in]{apicomplexa-tree24.png}\\\\ \\hline\n",
      "8 & \\includegraphics[height=1.65in]{apicomplexa-tree14.png} & 7 & \\includegraphics[height=1.65in]{apicomplexa-tree43.png}\\\\ \\hline\n",
      "1 & \\includegraphics[height=1.65in]{apicomplexa-tree202.png} & \\ & \\ \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "Comparing the tropical PC's on tropical topology and tropical linear space, we can see that the most three frequent topologies of the projected trees for the apicomplexa data (which totally covered more than two thirds of all trees) are the same for these two methods.\n",
      "\\end{comment}\n",
      "\n",
      "% \\subsection{African coelacanths genome and transcriptome data}\n",
      "% We also have applied tropical PCAs to the dataset consisting of 1,290\n",
      "% nuclear genes encoding 690,838 amino acid residues obtained from\n",
      "% genome and transcriptome data in \\cite{LSZ}. \n",
      "% It has been much work on the phylogenetic relations between\n",
      "% coelacanths, lungfishes and tetrapods, but still it is not clear\n",
      "% despite several studies \\cite{Hedges2009}. \n",
      "\n",
      "% Most morphological and paleontological studies support the hypothesis that lungfishes are closer to tetrapods than they are to coelacanths \\cite[Fig. 1, Tree 1]{LSZ}.\n",
      "% However, some research supports the hypothesis that coelacanths are\n",
      "% closer to tetrapods \\cite[Fig. 1, Tree 2]{LSZ}. \n",
      "% Others support the hypothesis that coelacanths and lungfishes form a\n",
      "% sister clade \\cite[Fig. 1, Tree 3]{LSZ}, or that tetrapods,\n",
      "% lungfishes, and coelacanths cannot be resolved \\cite[Fig.1, Tree\n",
      "% 4]{LSZ}. \n",
      "\n",
      "% We have used the gene trees used for the PCA under the BHV metric over\n",
      "% the tree space by Nye.~et.~al and detailed information such as tree\n",
      "% reconstruction method used as well as species names can be found in \\cite{NTWY}.\n",
      "% %\\begin{small}\n",
      "\n",
      "\n",
      "\n",
      "We also applied a variant of Algorithm \\ref{al1} to find a well-fitted tropical polytope with three vertices, enumerating through all ${\\binom{252}{3}}$ possibilities. The summary statistics for this run were: $d(\\mathcal P)= 147.0568$ and $r(\\mathcal P) = 0.612$. We note that these summary statistics are relatively consistent with the summary statistics obtained from the Stiefel tropical linear space algorithm. \n",
      "\n",
      "The tree topologies are presented in Figure \\ref{api_topology}. In general, the projected topologies were largely congruent with the generally accepted phylogeny: the two Plasmodium species (Pv and Pf) group together, as do the four species Ta, Bb, Tg, and Et, and Tt is isolated on a deep branch. \n",
      "\n",
      "\\cite[Theorem 23]{DS} tells us the tropical convex hull of the rows and columns of a matrix are equal. This allows us to visualize our best-fit tropical polytope in the two-dimensional plane $\\mathbb R^3/\\RR {\\bf 1}$ as the tropical convex hull of 28 points. These 28 points divide the polytope into different cells, as described in \\cite[Example 9]{JSY}. We plot this polytope, along with its cells and the projections of our data points, in Figure \\ref{api_triangle}.  We note that the different topologies seem to divide the tropical polytope PCA into several regions of positive area. \n",
      "\n",
      "% \\begin{figure}[!ht]\n",
      "% \\centering\n",
      "% \\includegraphics[width=0.5\\linewidth]{proj_api_topology.pdf}\n",
      "%   \\caption{Projected topology frequencies from the Apicomplexa dataset: parenthesized numbers give the frequencies of each topology.}\n",
      "%   \\label{api_topology}\n",
      "% \\end{figure}\n",
      "\n",
      "% \\begin{figure}[!ht]\n",
      "% \\centering\n",
      "% \\includegraphics[width=0.5\\linewidth]{api_approx_visual.pdf}\n",
      "%   \\caption{Projected points onto the tropical triangle}\n",
      "%   \\label{api_triangle}\n",
      "% \\end{figure}\n",
      "\n",
      "\\begin{figure}[!ht]\n",
      "\\includegraphics[width=1\\linewidth]{proj_api_topology.png}\n",
      "\\caption{Projected topology frequencies from the Apicomplexa dataset: parenthesized numbers give the frequencies of each topology.}%\n",
      "\\label{api_topology}\n",
      "\\end{figure}\n",
      "\\begin{figure}[!ht]\n",
      "\n",
      "  \\includegraphics[width=0.6\\linewidth]{api_approx_visual.pdf}\n",
      "{%\n",
      "\\caption{Projected points in the tropical polytope PCA, colored as in Figure \\ref{api_topology}.}%\n",
      "\\label{api_triangle}\n",
      "}\n",
      "\\end{figure}\n",
      "\n",
      "\n",
      "\n",
      "\\section{Discussion}\\label{discuss}\n",
      "\n",
      "In recent decades, the field of phylogenetics has found\n",
      "applications in the analysis on genomic scale data.  In particular, phylogenetic methods have been used to analyze the relationship between species and populations, as well as the evolutionary processes of \n",
      "speciation and molecular evolution.  As the cost of generating genomic data continues to decrease, the sheer volume of genomic data demands new analysis techniques. Motivated by this problem from systematic biology, we introduced in this paper a tropical analogue to principal component analysis in the tropical projective torus.\n",
      "\n",
      "Compared to the classical case, there is still much to be understood about these tropical principal component analyses.  \n",
      "For example, there is a nested structure to the classical principal components: the zeroth order PCA is contained by the\n",
      "first order PCA; the first order PCA is\n",
      "contained by the second order PCA; and so on.  It is unclear whether a similar relationship holds in the tropical analogue, either as a Stiefel tropical linear space or as a tropical polytope. We found examples of best-fit Stiefel tropical linear spaces which do not contain tropical Fermat-Weber points, such as Example \\ref{fermat-weber-noncontainment}. In each such case, however, there existed another tropical linear space of equally good fit that did contain a tropical Fermat-Weber point. Because these best-fit tropical structures are in general not unique, it is possible that one could define the principal components so that this containment property holds. Future work could explore this question further.\n",
      "\n",
      "We also introduced some approximative methods to compute the\n",
      "second order tropical PCA as a Stiefel tropical linear space and tropical polytope. Both algorithms rely on the uniform sampling of three random points from\n",
      "the dataset.  However, uniform sampling may not be the most efficient approach to\n",
      "finding a well-fitted solution. One might explore improvements to\n",
      "these algorithms using different sampling methods, such as the\n",
      "Metropolis-Hasting algorithm or Gibb sampling. \\cite{ZYCH}\n",
      "\n",
      " In \\cite{NTWY}, the authors\n",
      "considered the Billera-Holmes-Vogtman (BHV) \\cite{BHV} metric on the\n",
      "tree space and defined the $(s-1)$st order PCA as the {locus of the weighted Fr\\'echet \n",
      "  mean} of $s$ distinct points in the tree space. %  This object is ``similar'' to\n",
      "% a convex hull of $s$ distinct points in the tree space. \n",
      "Nye et.~al did not use a\n",
      "convex hull of $s$ distinct points in the tree space under the BHV\n",
      "metric because Lin et.~al showed in \\cite{LSTY} that the dimension of\n",
      "the convex hull under the BHV metric can be\n",
      "arbitrary high. In contrast, our methods for tropical principal\n",
      "component analysis are well-behaved with respect to dimension: the\n",
      "Stiefel tropical linear space given by an $(s\\times e)$-dimensional\n",
      "matrix will be of dimension $s-1$, and the tropical convex hull of $s$\n",
      "points has dimension at most $s-1$ as well. \\cite[Theorem 5.3.23]{MS} In\n",
      "statistics, we often use different metrics to analyze\n",
      "empirical data sets.  Our methods provide a new approach to analyzing phylogenetic tree datasets which may be particularly suitable in certain situations. \n",
      "\n",
      "In this work, we also found an exact solution for the best-fit tropical hyperplane of $e$ points in $\\RR^e/\\RR {\\bf 1}$. The general problem of constructing a best-fit Stiefel tropical linear space of given dimension remains unsolved. In addition, given $n$ points in a tropical linear space $L_p$, we noted that the Stiefel tropical linear space defined by these $n$ points may not be contained in $L_p$. Understanding the conditions under which containment holds could enable further application of these techniques to phylogenetics.\n",
      "\n",
      "\\smallskip\n",
      "\n",
      "\\noindent {\\bf Acknowledgements.}\\smallskip \\\\\n",
      "R.~Y.~was supported by Research Initiation Proposals from the Naval\n",
      "Postgraduate School.  L.~Z.~was supported by an NSF Graduate Research Fellowship. X.~Z.~was supported\n",
      "by travel funding from the Department of Statistics at the University\n",
      "of Kentucky.\n",
      "\n",
      "The authors thank Bernd Sturmfels (UC Berkeley and MPI Leipzig) for many helpful conversations. The authors also thank Daniel Howe (University of Kentucky) for his input on apicomplexa tree topologies.\n",
      "\n",
      "\n",
      "%\\begin{small}\n",
      "\\begin{thebibliography}{10}\n",
      "\n",
      "%\\setlength{\\itemsep}{-0.7mm}\n",
      "\n",
      "\\bibitem{AGNS}\n",
      "M.~Akian, S.~Gaubert, N.~Viorel and I.~Singer:\n",
      "{\\em Best approximation in max-plus semimodules},\n",
      "Linear Algebra Appl.~{\\bf 435} (2011) 3261--3296. \n",
      "\n",
      "\\bibitem{ACDP}\n",
      "A.~Apostolico, M.~Comin, A.~Dress and L.~Parida:\n",
      "{\\em Ultrametric networks: a new tool for phylogenetic analysis},\n",
      "Algorithms for Molecular Biology {\\bf 8} (2013) 7.\n",
      "\n",
      "% \\bibitem{Ard}\n",
      "% F.~Ardila: {\\em Subdominant matroid ultrametrics},\n",
      "% {Annals~of  Combinatorics} {\\bf 8} (2004) 379--389.\n",
      "\n",
      "% \\bibitem{ABY}\n",
      "% F.~Ardila, T.~Baker and R.~Yatchak:\n",
      "% {\\em Moving robots efficiently using the combinatorics of CAT(0) cubical complexes},\n",
      "%  SIAM J.~Discrete Math.~{\\bf 28} (2014) 986--1007.\n",
      " \n",
      "% \\bibitem{AK}\n",
      "% F.~Ardila and C.~Klivans:\n",
      "%  {\\em The Bergman complex of a matroid and phylogenetic trees},\n",
      "%   Journal of Combinatorial Theory Ser.~B {\\bf 96} (2006) 38--49.\n",
      " \n",
      "%  \\bibitem{Ber} M.~Berger:\n",
      "% {\\em A Panoramic View of Riemannian Geometry},\n",
      "% Springer Verlag, Berlin, 2003.\n",
      " \n",
      "\\bibitem{BHV} L.~Billera, S.~Holmes and K.~Vogtman:\n",
      "{\\em Geometry of the space of phylogenetic trees},\n",
      " Advances~in Applied~Mathematics {\\bf 27} (2001) 733--767.\n",
      "\n",
      "% \\bibitem{Bow} B.~Bowditch: {\\em Some results on the geometry of convex hulls\n",
      "% in manifolds of pinched negative curvature},\n",
      "% Comment.~Math.~Helvetici {\\bf 69} (1994) 49--81.\n",
      "\n",
      "\\bibitem{CF} V.~Chepoi and B.~Fichet:\n",
      "{\\em $\\ell_\\infty$ approximation via subdominants},\n",
      "{Journal of Mathematical Psychology} {\\bf 44} (2000) 600--616.\n",
      " \n",
      "\\bibitem{CGQ} G.~Cohen, S.~Gaubert and J.P.~Quadrat:\n",
      "{\\em Duality and separation theorems in idempotent semimodules},\n",
      "Linear Algebra Appl.~{\\bf 379} (2004) 395--422.\n",
      "\n",
      "\\bibitem{DGJ} J.~Depersin, S.~Gaubert and M.~Joswig:\n",
      "{\\em A tropical isoperimetric inequality},   {\\tt arXiv: 1611. 04148}\n",
      "\n",
      "% \\bibitem{Fei} E.~Feichtner:\n",
      "%  {\\em Complexes of trees and nested set complexes},\n",
      "%   Pacific Journal of Mathematics {\\bf 227} (2006) 271--286.\n",
      "   \n",
      "\\bibitem{FR} A.~Fink and F.~Rinc\\'on:\n",
      "{\\em Stiefel tropical linear spaces},\n",
      "J.~Combin.~Theory~A {\\bf 135} (2015) 291--331.\n",
      "\n",
      "% \\bibitem{FMPV}   P.T.~Fletcher, J.~Moeller, J.M.~Phillips and S.~Venkatasubramanian:\n",
      "% {\\em Computing hulls, centerpoints and VC dimension in positive definite spaces},\n",
      "% presented at Algorithms and Data Structures Symposium, New York, 2011;\n",
      "% original version at {\\tt arXiv:0912.1580}.\n",
      "\n",
      "\n",
      "\\bibitem{polymake} E.~Gawrilow and M.~Joswig: {\\em polymake: a\n",
      "  framework for analyzing convex polytopes}, in Polytopes: â¬âcombinatorics\n",
      "  and computation, 43--73, DMV Seminar 29,\n",
      "  Birkh\\\"auser, Basel, 2000. \n",
      "\n",
      "% \\bibitem{Hedges2009} S.~B.~ Hedges: {\\em Vertebrates (Vertebrata)}\n",
      "%   from the book {\\em The Timeline of Life} edited by S. B. Hedges and\n",
      "%   S. Kumar.  (2009) 309--314. Oxford University Press.  \n",
      "\n",
      "% \\bibitem{openbook} T.~Hotz, S.~Huckemann, H.~Le,\n",
      "% J.~Marron, J.~Mattingly, E.~Miller, J.~Nolen, M.~Owen,\n",
      "% V.~Patrangenaru and S.~Skwerer:\n",
      "% {\\em Sticky central limit theorems on open books},\n",
      "% Annals~of Applied Probability {\\bf 6} (2013) 2238--2258.\n",
      "\n",
      "\\bibitem{bigM} Griva Igor, Nash Stephan G., Sofer Ariela: Linear and Nonlinear Optimization (2nd ed.). Society for Industrial Mathematics.\n",
      "\n",
      "\\bibitem{DS} M.~Develin and B.~Sturmfels: {\\em Tropical convexity}, Doc. Math. {\\bf 9} (2004), 1--27.\n",
      "\n",
      "\\bibitem{Mesq} Maddison, W. P. and D.R. Maddison. 2017. Mesquite: a modular system for\n",
      "     evolutionary analysis.  Version 3.31  http://mesquiteproject.org.\n",
      "\n",
      "\\bibitem{KWK} C.~Kuo,  J.~P.~Wares, and J.~C.~Kissinger:\n",
      "{\\em  The Apicomplexan whole-genome phylogeny: An analysis of\n",
      "  incongruence among gene trees}, Mol. Biol. Evol. {\\bf 25} (2008)\n",
      "2689--2698.\n",
      "\n",
      "% \\bibitem{LSZ} D.~Liang, X.~X.~Shen, and P.~Zhang: \n",
      "% {\\em One thousand two hundred ninety nuclear genes from a genome-wide\n",
      "%   survey support lungfishes as the sister group of tetrapods},\n",
      "% Mol. Biol. Evol. {\\bf 30} (2013) 1803--1807.\n",
      "\n",
      "\\bibitem{J}\n",
      "M.~Joswig: {\\em Essentials of Tropical Combinatorics}, in preparation, {\\url http://page.math.tu-berlin.de/~joswig/etc/index.html}, accessed in 2017.\n",
      "\n",
      "\\bibitem{JSY}\n",
      "M.~Joswig, B.~Sturmfels and J.~Yu:\n",
      "{\\em Affine buildings and tropical convexity},\n",
      " Albanian J.~Math. {\\bf 1} (2007) 187--211.\n",
      " \n",
      "\\bibitem{Lenstra} H.~W.~Lenstra:\n",
      "{\\em Integer Programming with a fixed number of\n",
      "  Variables}. Mathematics of Operations Research. {\\bf 8} (1983)\n",
      "538--548.\n",
      "\n",
      "\\bibitem{LSTY} B.~Lin, B.~Sturmfels, X.~Tang, and R.~Yoshida:\n",
      "{\\em Convexity in Tree Spaces}. SIAM Discrete Math. {\\bf 3} (2017) 2015--2038.\n",
      "\n",
      "\\bibitem{LY} B.~Lin and R.~Yoshida: {\\em Tropical Fermat-Weber\n",
      "    points}.  (2016) {\\url https://arxiv.org/abs/1604.04674}.\n",
      "\\bibitem{MS}  D.~Maclagan and B.~Sturmfels:\n",
      "{\\em  Introduction to Tropical Geometry},\n",
      "Graduate Studies in Mathematics, 161, American Mathematical Society, \n",
      "Providence, RI, 2015. \n",
      "\n",
      "% \\bibitem{Mil} E.~Miller: {\\em Fruit flies and moduli: Interactions between biology\n",
      "% and mathematics}, Notices of the American Mathematical Society\n",
      "% {\\bf 62}(10) (2015) 1178--1183.\n",
      "\n",
      "% \\bibitem{MOP} E.~Miller, M.~Owen and S.~Provan:\n",
      "% {\\em Polyhedral computational geometry for averaging\n",
      "% metric phylogenetic trees}, \n",
      "% Adva\n",
      "\n",
      "文本已分成 46 块。\n",
      "分块结果:\n",
      "Chunk 1: \\\\\n",
      "     \\hline\n",
      "     D^{(3)}\n",
      "Chunk 2:  & 1 & 7.331937 & 7.331937\\\\\n",
      "     \\hline\\hline\n",
      "     \\end{tabular}\n",
      "}{%\n",
      "  \\caption{Vectorized Distance Matrices}%\n",
      "  \\label{table 2}\n",
      "}\n",
      "\\end{floatrow}\n",
      "\\end{figure}\n",
      "\n",
      "Using our optimization problem formulation from Proposition \\ref{polytope_projection}, we obtain \n",
      "Chunk 3: D^{(1)},D^{(2)},D^{(3)}\n",
      "Chunk 4:  for this example. These points are ultrametrics, and they are described in Figure \\ref{figure 2} and Table \\ref{table 2}. In fact, in this case the best-fit tropical polytope contains all the equidistant trees, so that the sum of distances is zero.\n",
      "\\end{example}\n",
      "\n",
      "\\subsection{Approximative algorithms}\n",
      "For larger datasets, we turn to the approximative Algorithms \\ref{tropical-linear-space-algorithm} and \\ref{al1}. We implemented both algorithms in {\\tt R}.\\footnote{Our software for all computations can be downloaded at \\url{http://polytopes.net/computations/tropicalPCA/}.} We then generated a random sample from {\\tt Mesquite} \\cite{Mesq} and applied our algorithms on this dataset. The sample was constructed as follows:\n",
      "\\begin{algorithm}[Generating the simulation dataset]\\label{sim_data} \n",
      "                                \\qquad  \\qquad \\qquad \n",
      "\n",
      "\\begin{enumerate}\n",
      "\\item Generate 250 gene trees with 8 leaves from the coalescent model under a fixed species tree with depth equal to 10 \n",
      "\\item Transform the gene trees to be ultrametrics.\n",
      "\\item Compute approximate second order tropical principal components via the Algorithms \\ref{tropical-linear-space-algorithm} and \\ref{al1}.\n",
      "\\end{enumerate}\n",
      "\\end{algorithm}\n",
      "\n",
      "We applied both methods of tropical principal component analysis to a set of random trees generated by Algorithm \\ref{sim_data}. In analogy with \\cite{NTWY}, we define summary statistics to describe the fit of a Stiefel tropical linear space\n",
      "or a tropical polytope to a given data\n",
      "set. If \n",
      "Chunk 5: L_p\n",
      "Chunk 6:  is a Stiefel tropical linear space, we define its distance to the datapoints \n",
      "Chunk 7: d(L_p)\n",
      "Chunk 8:  as\n",
      "\\[d(L_p) = \\sum_i d(D^{(i)}, L_p),\\]\n",
      "and a tropical proportion of variance statistic\n",
      "\\[r(L_p) = \\frac{\\sum_i d_{tr}(\\bar \\pi, \\pi_{L_p}(D^{(i)}))}{\\sum_i d_{tr}(D^{(i)}, \\pi_{L_p}(D^{(i)})+\\sum_i d_{tr}(\\bar \\pi, \\pi_{L_p}(D^{(i)}))}\\]\n",
      "where \n",
      "Chunk 9: \\bar \\pi\n",
      "Chunk 10:  denotes a Fermat-Weber point of the projections of the datapoints, as in \\cite{LY}. These statistics are defined analogously for a tropical polytope \n",
      "Chunk 11: \\mathcal P. The statistic r(L_p)\n",
      "Chunk 12:  can be interpreted as the proportion of variance explained by \n",
      "Chunk 13: L_p\n",
      "Chunk 14: ; in order to remain consistent with the tropical metric, we sum distances rather than squared distances. \n",
      "\n",
      "For the polytopal approach, as noted above, the projections will remain ultrametrics. We therefore analyze the topologies of these projections, and compare them with the topology of the species tree.\n",
      "\n",
      "\\subsection{Approximation results}\n",
      "\n",
      "We applied Algorithm \\ref{tropical-linear-space-algorithm} to find an approximate 2-dimensional best-fit Stiefel tropical linear space with a convergence threshold of 100 iterations. The summary statistics for this run were: \n",
      "Chunk 15: d(L_p)=363.0378 and r(L_p) = 0.322\n",
      "Chunk 16: .\n",
      "\n",
      "We also applied a variant of Algorithm \\ref{al1} to find an approximate best-fit tropical polytope with three vertices, in which we enumerated through all \n",
      "Chunk 17: \\binom{250}{3} different choices. The summary statistics were: d(\\mathcal P) = 360.6831 and \n",
      "r(\\mathcal P) = 0.265\n",
      "Chunk 18: . We note that the overall sum of distances is similar between the two methods, but that the best-fit Stiefel tropical linear space explains a slightly higher proportion of variance. \n",
      "\n",
      "For the tropical polytope method, we recall that projections of equidistant trees will remain ultrametrics. We present common topologies of the projections as well as the species tree topology\n",
      "in Figure \\ref{sim_topology}.\\footnote{Tree topologies of all projected\n",
      "points can be found in the supplement \n",
      "at \\url{http://polytopes.net/computations/tropicalPCA/}.}\n",
      "We observe that these topologies of\n",
      "projected trees are broadly consistent with the topology of the species tree\n",
      "under which these gene trees were generated: taxa \n",
      "Chunk 19: g and c group\n",
      "together, as do taxa h and f, and the four taxa a, b, d, e\n",
      "Chunk 20:  also group together. We can view our best-fit tropical polytope as preserving these features of the species tree, meaning that this tropical polytope retains information after projection.\n",
      "\\begin{figure}[!ht]\n",
      "\\centering\n",
      "\\includegraphics[width=0.6\\linewidth]{proj_sim_250_8_topology.pdf}\n",
      "  \\caption{Topology frequencies after projections: the parenthesized numbers are frequencies, and the last tree gives the species tree topology.}\n",
      "  \\label{sim_topology}\n",
      "\\end{figure}\n",
      "\n",
      "\\section{Apicomplexa genome}\\label{apicomplexa}\n",
      "\n",
      "We also applied our tropical principal component algorithms to a set of trees constructed from 252\n",
      "orthologous sequences on eight species of protozoa in the\n",
      "Apicomplexa phylum by \\cite{KWK}.   \n",
      "This dataset was also analyzed by Weyenberg et.~al; one can find\n",
      "more details, such as the gene sequences, in \\cite{KDETrees}.\n",
      "Because ordinary PCA is sensitive to outliers, we removed 16 outlier gene trees identified by \\cite{KDETrees} before fitting the tropical principal components.\n",
      "\n",
      "To find an approximate best-fit 2-dimensional Stiefel tropical linear space, we applied Algorithm \\ref{tropical-linear-space-algorithm} with a convergence threshold of 100 iterations. Due to the stochastic nature of the algorithm, we executed the algorithm three times. %Tree topologies from a representative execution are presented in Table \\ref{apicomplexa-tropical-linear-space}. We note the close similarity between the most common tree topology with the generally-accepted phylogeny of these eight taxa \\cite[Figure 1]{KWK}. \n",
      "The summary statistics remained consistent between these runs. For one representative execution, these statistics were: \n",
      "Chunk 21: d(L_p) = 145.38 and r(L_p)=0.616\n",
      "Chunk 22: . % In general, furthermore, projected topologies were largely congruent with the generally accepted phylogeny: the two Plasmodium species (Pv and Pf) group together, as do Ta and Bb; and Tt is isolated on a deep branch.\n",
      "\n",
      "\\begin{comment}\\begin{table}\n",
      "\\centering\n",
      "\\caption{Tree topologies and frequency counts for  apicomplexa gene tree projections}\n",
      "\\label{apicomplexa-tropical-linear-space}\n",
      "\\begin{tabular}{|c|c|c|c|}\n",
      "\\hline\n",
      "Count & Topology & Count & Topology\\\\\\hline\n",
      "112 & \\includegraphics[height=1.65in]{apicomplexa-tree3.png} & 31 & \\includegraphics[height=1.65in]{apicomplexa-tree13.png}\\\\ \\hline\n",
      "30 & \\includegraphics[height=1.65in]{apicomplexa-tree9.png} & 26 & \\includegraphics[height=1.65in]{apicomplexa-tree1.png}\\\\ \\hline\n",
      "20 & \\includegraphics[height=1.65in]{apicomplexa-tree4.png} & 17 & \\includegraphics[height=1.65in]{apicomplexa-tree24.png}\\\\ \\hline\n",
      "8 & \\includegraphics[height=1.65in]{apicomplexa-tree14.png} & 7 & \\includegraphics[height=1.65in]{apicomplexa-tree43.png}\\\\ \\hline\n",
      "1 & \\includegraphics[height=1.65in]{apicomplexa-tree202.png} & \\ & \\ \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "Comparing the tropical PC's on tropical topology and tropical linear space, we can see that the most three frequent topologies of the projected trees for the apicomplexa data (which totally covered more than two thirds of all trees) are the same for these two methods.\n",
      "\\end{comment}\n",
      "\n",
      "% \\subsection{African coelacanths genome and transcriptome data}\n",
      "% We also have applied tropical PCAs to the dataset consisting of 1,290\n",
      "% nuclear genes encoding 690,838 amino acid residues obtained from\n",
      "% genome and transcriptome data in \\cite{LSZ}. \n",
      "% It has been much work on the phylogenetic relations between\n",
      "% coelacanths, lungfishes and tetrapods, but still it is not clear\n",
      "% despite several studies \\cite{Hedges2009}. \n",
      "\n",
      "% Most morphological and paleontological studies support the hypothesis that lungfishes are closer to tetrapods than they are to coelacanths \\cite[Fig. 1, Tree 1]{LSZ}.\n",
      "% However, some research supports the hypothesis that coelacanths are\n",
      "% closer to tetrapods \\cite[Fig. 1, Tree 2]{LSZ}. \n",
      "% Others support the hypothesis that coelacanths and lungfishes form a\n",
      "% sister clade \\cite[Fig. 1, Tree 3]{LSZ}, or that tetrapods,\n",
      "% lungfishes, and coelacanths cannot be resolved \\cite[Fig.1, Tree\n",
      "% 4]{LSZ}. \n",
      "\n",
      "% We have used the gene trees used for the PCA under the BHV metric over\n",
      "% the tree space by Nye.~et.~al and detailed information such as tree\n",
      "% reconstruction method used as well as species names can be found in \\cite{NTWY}.\n",
      "% %\\begin{small}\n",
      "\n",
      "\n",
      "\n",
      "We also applied a variant of Algorithm \\ref{al1} to find a well-fitted tropical polytope with three vertices, enumerating through all \n",
      "Chunk 23: {\\binom{252}{3}}\n",
      "Chunk 24:  possibilities. The summary statistics for this run were: \n",
      "Chunk 25: d(\\mathcal P)= 147.0568 and r(\\mathcal P) = 0.612\n",
      "Chunk 26: . We note that these summary statistics are relatively consistent with the summary statistics obtained from the Stiefel tropical linear space algorithm. \n",
      "\n",
      "The tree topologies are presented in Figure \\ref{api_topology}. In general, the projected topologies were largely congruent with the generally accepted phylogeny: the two Plasmodium species (Pv and Pf) group together, as do the four species Ta, Bb, Tg, and Et, and Tt is isolated on a deep branch. \n",
      "\n",
      "\\cite[Theorem 23]{DS} tells us the tropical convex hull of the rows and columns of a matrix are equal. This allows us to visualize our best-fit tropical polytope in the two-dimensional plane \n",
      "Chunk 27: \\mathbb R^3/\\RR {\\bf 1}\n",
      "Chunk 28:  as the tropical convex hull of 28 points. These 28 points divide the polytope into different cells, as described in \\cite[Example 9]{JSY}. We plot this polytope, along with its cells and the projections of our data points, in Figure \\ref{api_triangle}.  We note that the different topologies seem to divide the tropical polytope PCA into several regions of positive area. \n",
      "\n",
      "% \\begin{figure}[!ht]\n",
      "% \\centering\n",
      "% \\includegraphics[width=0.5\\linewidth]{proj_api_topology.pdf}\n",
      "%   \\caption{Projected topology frequencies from the Apicomplexa dataset: parenthesized numbers give the frequencies of each topology.}\n",
      "%   \\label{api_topology}\n",
      "% \\end{figure}\n",
      "\n",
      "% \\begin{figure}[!ht]\n",
      "% \\centering\n",
      "% \\includegraphics[width=0.5\\linewidth]{api_approx_visual.pdf}\n",
      "%   \\caption{Projected points onto the tropical triangle}\n",
      "%   \\label{api_triangle}\n",
      "% \\end{figure}\n",
      "\n",
      "\\begin{figure}[!ht]\n",
      "\\includegraphics[width=1\\linewidth]{proj_api_topology.png}\n",
      "\\caption{Projected topology frequencies from the Apicomplexa dataset: parenthesized numbers give the frequencies of each topology.}%\n",
      "\\label{api_topology}\n",
      "\\end{figure}\n",
      "\\begin{figure}[!ht]\n",
      "\n",
      "  \\includegraphics[width=0.6\\linewidth]{api_approx_visual.pdf}\n",
      "{%\n",
      "\\caption{Projected points in the tropical polytope PCA, colored as in Figure \\ref{api_topology}.}%\n",
      "\\label{api_triangle}\n",
      "}\n",
      "\\end{figure}\n",
      "\n",
      "\n",
      "\n",
      "\\section{Discussion}\\label{discuss}\n",
      "\n",
      "In recent decades, the field of phylogenetics has found\n",
      "applications in the analysis on genomic scale data.  In particular, phylogenetic methods have been used to analyze the relationship between species and populations, as well as the evolutionary processes of \n",
      "speciation and molecular evolution.  As the cost of generating genomic data continues to decrease, the sheer volume of genomic data demands new analysis techniques. Motivated by this problem from systematic biology, we introduced in this paper a tropical analogue to principal component analysis in the tropical projective torus.\n",
      "\n",
      "Compared to the classical case, there is still much to be understood about these tropical principal component analyses.  \n",
      "For example, there is a nested structure to the classical principal components: the zeroth order PCA is contained by the\n",
      "first order PCA; the first order PCA is\n",
      "contained by the second order PCA; and so on.  It is unclear whether a similar relationship holds in the tropical analogue, either as a Stiefel tropical linear space or as a tropical polytope. We found examples of best-fit Stiefel tropical linear spaces which do not contain tropical Fermat-Weber points, such as Example \\ref{fermat-weber-noncontainment}. In each such case, however, there existed another tropical linear space of equally good fit that did contain a tropical Fermat-Weber point. Because these best-fit tropical structures are in general not unique, it is possible that one could define the principal components so that this containment property holds. Future work could explore this question further.\n",
      "\n",
      "We also introduced some approximative methods to compute the\n",
      "second order tropical PCA as a Stiefel tropical linear space and tropical polytope. Both algorithms rely on the uniform sampling of three random points from\n",
      "the dataset.  However, uniform sampling may not be the most efficient approach to\n",
      "finding a well-fitted solution. One might explore improvements to\n",
      "these algorithms using different sampling methods, such as the\n",
      "Metropolis-Hasting algorithm or Gibb sampling. \\cite{ZYCH}\n",
      "\n",
      " In \\cite{NTWY}, the authors\n",
      "considered the Billera-Holmes-Vogtman (BHV) \\cite{BHV} metric on the\n",
      "tree space and defined the \n",
      "Chunk 29: (s-1)\n",
      "Chunk 30: st order PCA as the {locus of the weighted Fr\\'echet \n",
      "  mean} of \n",
      "Chunk 31: s\n",
      "Chunk 32:  distinct points in the tree space. %  This object is ``similar'' to\n",
      "% a convex hull of \n",
      "Chunk 33: s\n",
      "Chunk 34:  distinct points in the tree space. \n",
      "Nye et.~al did not use a\n",
      "convex hull of \n",
      "Chunk 35: s\n",
      "Chunk 36:  distinct points in the tree space under the BHV\n",
      "metric because Lin et.~al showed in \\cite{LSTY} that the dimension of\n",
      "the convex hull under the BHV metric can be\n",
      "arbitrary high. In contrast, our methods for tropical principal\n",
      "component analysis are well-behaved with respect to dimension: the\n",
      "Stiefel tropical linear space given by an \n",
      "Chunk 37: (s\\times e)-dimensional\n",
      "matrix will be of dimension s-1, and the tropical convex hull of s\n",
      "points has dimension at most s-1\n",
      "Chunk 38:  as well. \\cite[Theorem 5.3.23]{MS} In\n",
      "statistics, we often use different metrics to analyze\n",
      "empirical data sets.  Our methods provide a new approach to analyzing phylogenetic tree datasets which may be particularly suitable in certain situations. \n",
      "\n",
      "In this work, we also found an exact solution for the best-fit tropical hyperplane of \n",
      "Chunk 39: e points in \\RR^e/\\RR {\\bf 1}\n",
      "Chunk 40: . The general problem of constructing a best-fit Stiefel tropical linear space of given dimension remains unsolved. In addition, given \n",
      "Chunk 41: n points in a tropical linear space L_p\n",
      "Chunk 42: , we noted that the Stiefel tropical linear space defined by these \n",
      "Chunk 43: n points may not be contained in L_p\n",
      "Chunk 44: . Understanding the conditions under which containment holds could enable further application of these techniques to phylogenetics.\n",
      "\n",
      "\\smallskip\n",
      "\n",
      "\\noindent {\\bf Acknowledgements.}\\smallskip \\\\\n",
      "R.~Y.~was supported by Research Initiation Proposals from the Naval\n",
      "Postgraduate School.  L.~Z.~was supported by an NSF Graduate Research Fellowship. X.~Z.~was supported\n",
      "by travel funding from the Department of Statistics at the University\n",
      "of Kentucky.\n",
      "\n",
      "The authors thank Bernd Sturmfels (UC Berkeley and MPI Leipzig) for many helpful conversations. The authors also thank Daniel Howe (University of Kentucky) for his input on apicomplexa tree topologies.\n",
      "\n",
      "\n",
      "%\\begin{small}\n",
      "\\begin{thebibliography}{10}\n",
      "\n",
      "%\\setlength{\\itemsep}{-0.7mm}\n",
      "\n",
      "\\bibitem{AGNS}\n",
      "M.~Akian, S.~Gaubert, N.~Viorel and I.~Singer:\n",
      "{\\em Best approximation in max-plus semimodules},\n",
      "Linear Algebra Appl.~{\\bf 435} (2011) 3261--3296. \n",
      "\n",
      "\\bibitem{ACDP}\n",
      "A.~Apostolico, M.~Comin, A.~Dress and L.~Parida:\n",
      "{\\em Ultrametric networks: a new tool for phylogenetic analysis},\n",
      "Algorithms for Molecular Biology {\\bf 8} (2013) 7.\n",
      "\n",
      "% \\bibitem{Ard}\n",
      "% F.~Ardila: {\\em Subdominant matroid ultrametrics},\n",
      "% {Annals~of  Combinatorics} {\\bf 8} (2004) 379--389.\n",
      "\n",
      "% \\bibitem{ABY}\n",
      "% F.~Ardila, T.~Baker and R.~Yatchak:\n",
      "% {\\em Moving robots efficiently using the combinatorics of CAT(0) cubical complexes},\n",
      "%  SIAM J.~Discrete Math.~{\\bf 28} (2014) 986--1007.\n",
      " \n",
      "% \\bibitem{AK}\n",
      "% F.~Ardila and C.~Klivans:\n",
      "%  {\\em The Bergman complex of a matroid and phylogenetic trees},\n",
      "%   Journal of Combinatorial Theory Ser.~B {\\bf 96} (2006) 38--49.\n",
      " \n",
      "%  \\bibitem{Ber} M.~Berger:\n",
      "% {\\em A Panoramic View of Riemannian Geometry},\n",
      "% Springer Verlag, Berlin, 2003.\n",
      " \n",
      "\\bibitem{BHV} L.~Billera, S.~Holmes and K.~Vogtman:\n",
      "{\\em Geometry of the space of phylogenetic trees},\n",
      " Advances~in Applied~Mathematics {\\bf 27} (2001) 733--767.\n",
      "\n",
      "% \\bibitem{Bow} B.~Bowditch: {\\em Some results on the geometry of convex hulls\n",
      "% in manifolds of pinched negative curvature},\n",
      "% Comment.~Math.~Helvetici {\\bf 69} (1994) 49--81.\n",
      "\n",
      "\\bibitem{CF} V.~Chepoi and B.~Fichet:\n",
      "{\\em \n",
      "Chunk 45: \\ell_\\infty\n",
      "Chunk 46:  approximation via subdominants},\n",
      "{Journal of Mathematical Psychology} {\\bf 44} (2000) 600--616.\n",
      " \n",
      "\\bibitem{CGQ} G.~Cohen, S.~Gaubert and J.P.~Quadrat:\n",
      "{\\em Duality and separation theorems in idempotent semimodules},\n",
      "Linear Algebra Appl.~{\\bf 379} (2004) 395--422.\n",
      "\n",
      "\\bibitem{DGJ} J.~Depersin, S.~Gaubert and M.~Joswig:\n",
      "{\\em A tropical isoperimetric inequality},   {\\tt arXiv: 1611. 04148}\n",
      "\n",
      "% \\bibitem{Fei} E.~Feichtner:\n",
      "%  {\\em Complexes of trees and nested set complexes},\n",
      "%   Pacific Journal of Mathematics {\\bf 227} (2006) 271--286.\n",
      "   \n",
      "\\bibitem{FR} A.~Fink and F.~Rinc\\'on:\n",
      "{\\em Stiefel tropical linear spaces},\n",
      "J.~Combin.~Theory~A {\\bf 135} (2015) 291--331.\n",
      "\n",
      "% \\bibitem{FMPV}   P.T.~Fletcher, J.~Moeller, J.M.~Phillips and S.~Venkatasubramanian:\n",
      "% {\\em Computing hulls, centerpoints and VC dimension in positive definite spaces},\n",
      "% presented at Algorithms and Data Structures Symposium, New York, 2011;\n",
      "% original version at {\\tt arXiv:0912.1580}.\n",
      "\n",
      "\n",
      "\\bibitem{polymake} E.~Gawrilow and M.~Joswig: {\\em polymake: a\n",
      "  framework for analyzing convex polytopes}, in Polytopes: â¬âcombinatorics\n",
      "  and computation, 43--73, DMV Seminar 29,\n",
      "  Birkh\\\"auser, Basel, 2000. \n",
      "\n",
      "% \\bibitem{Hedges2009} S.~B.~ Hedges: {\\em Vertebrates (Vertebrata)}\n",
      "%   from the book {\\em The Timeline of Life} edited by S. B. Hedges and\n",
      "%   S. Kumar.  (2009) 309--314. Oxford University Press.  \n",
      "\n",
      "% \\bibitem{openbook} T.~Hotz, S.~Huckemann, H.~Le,\n",
      "% J.~Marron, J.~Mattingly, E.~Miller, J.~Nolen, M.~Owen,\n",
      "% V.~Patrangenaru and S.~Skwerer:\n",
      "% {\\em Sticky central limit theorems on open books},\n",
      "% Annals~of Applied Probability {\\bf 6} (2013) 2238--2258.\n",
      "\n",
      "\\bibitem{bigM} Griva Igor, Nash Stephan G., Sofer Ariela: Linear and Nonlinear Optimization (2nd ed.). Society for Industrial Mathematics.\n",
      "\n",
      "\\bibitem{DS} M.~Develin and B.~Sturmfels: {\\em Tropical convexity}, Doc. Math. {\\bf 9} (2004), 1--27.\n",
      "\n",
      "\\bibitem{Mesq} Maddison, W. P. and D.R. Maddison. 2017. Mesquite: a modular system for\n",
      "     evolutionary analysis.  Version 3.31  http://mesquiteproject.org.\n",
      "\n",
      "\\bibitem{KWK} C.~Kuo,  J.~P.~Wares, and J.~C.~Kissinger:\n",
      "{\\em  The Apicomplexan whole-genome phylogeny: An analysis of\n",
      "  incongruence among gene trees}, Mol. Biol. Evol. {\\bf 25} (2008)\n",
      "2689--2698.\n",
      "\n",
      "% \\bibitem{LSZ} D.~Liang, X.~X.~Shen, and P.~Zhang: \n",
      "% {\\em One thousand two hundred ninety nuclear genes from a genome-wide\n",
      "%   survey support lungfishes as the sister group of tetrapods},\n",
      "% Mol. Biol. Evol. {\\bf 30} (2013) 1803--1807.\n",
      "\n",
      "\\bibitem{J}\n",
      "M.~Joswig: {\\em Essentials of Tropical Combinatorics}, in preparation, {\\url http://page.math.tu-berlin.de/~joswig/etc/index.html}, accessed in 2017.\n",
      "\n",
      "\\bibitem{JSY}\n",
      "M.~Joswig, B.~Sturmfels and J.~Yu:\n",
      "{\\em Affine buildings and tropical convexity},\n",
      " Albanian J.~Math. {\\bf 1} (2007) 187--211.\n",
      " \n",
      "\\bibitem{Lenstra} H.~W.~Lenstra:\n",
      "{\\em Integer Programming with a fixed number of\n",
      "  Variables}. Mathematics of Operations Research. {\\bf 8} (1983)\n",
      "538--548.\n",
      "\n",
      "\\bibitem{LSTY} B.~Lin, B.~Sturmfels, X.~Tang, and R.~Yoshida:\n",
      "{\\em Convexity in Tree Spaces}. SIAM Discrete Math. {\\bf 3} (2017) 2015--2038.\n",
      "\n",
      "\\bibitem{LY} B.~Lin and R.~Yoshida: {\\em Tropical Fermat-Weber\n",
      "    points}.  (2016) {\\url https://arxiv.org/abs/1604.04674}.\n",
      "\\bibitem{MS}  D.~Maclagan and B.~Sturmfels:\n",
      "{\\em  Introduction to Tropical Geometry},\n",
      "Graduate Studies in Mathematics, 161, American Mathematical Society, \n",
      "Providence, RI, 2015. \n",
      "\n",
      "% \\bibitem{Mil} E.~Miller: {\\em Fruit flies and moduli: Interactions between biology\n",
      "% and mathematics}, Notices of the American Mathematical Society\n",
      "% {\\bf 62}(10) (2015) 1178--1183.\n",
      "\n",
      "% \\bibitem{MOP} E.~Miller, M.~Owen and S.~Provan:\n",
      "% {\\em Polyhedral computational geometry for averaging\n",
      "% metric phylogenetic trees}, \n",
      "% Adva\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old Headless mode will be removed from the Chrome binary soon. Please use the new Headless mode (https://developer.chrome.com/docs/chromium/new-headless) or the chrome-headless-shell which is a standalone implementation of the old Headless mode (https://developer.chrome.com/blog/chrome-headless-shell).\n",
      "\n",
      "[1226/182112.787788:WARNING:viz_main_impl.cc(85)] VizNullHypothesis is disabled (not a warning)\n",
      "[1226/182112.817910:ERROR:cv_display_link_mac.mm(165)] CVDisplayLinkCreateWithCGDisplay failed. CVReturn: -6670\n",
      "[1226/182112.871600:ERROR:cv_display_link_mac.mm(165)] CVDisplayLinkCreateWithCGDisplay failed. CVReturn: -6670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高亮文本已保存为 PNG 图片:data/test_result/chunkSize50_chunkOverlap10_LatexTextSplitter_20241226_182112.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "283242 bytes written to file /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/test_result/chunkSize50_chunkOverlap10_LatexTextSplitter_20241226_182112.png\n"
     ]
    }
   ],
   "source": [
    "max_len = 20000\n",
    "highlighter = TextHighlighter(\n",
    "    long_text=sample_text,\n",
    "    chunking_api=splitter.split_text,\n",
    "    max_length=max_len\n",
    ")\n",
    "\n",
    "# 显示高亮文本\n",
    "highlighter.display_highlighted_text()\n",
    "\n",
    "output_dir = \"data/test_result\"\n",
    "highlighter.save_highlighted_text(output_dir, \"LatexTextSplitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/28.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/79.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/124.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/175.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/186.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/217.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/357.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/246.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/306.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/141.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/110.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/281.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/332.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/272.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/363.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/223.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/233.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/373.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/262.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/322.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/291.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/100.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/151.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/380.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/316.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/256.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/347.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/207.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/196.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/165.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/134.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/5.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/69.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/38.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/18.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/49.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/285.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/114.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/145.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/367.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/227.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/336.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/276.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/171.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/120.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/242.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/302.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/213.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/182.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/353.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/343.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/192.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/203.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/312.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/252.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/130.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/161.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/266.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/326.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/237.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/377.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/155.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/384.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/104.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/295.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/59.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/1.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/19.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/48.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/115.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/284.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/144.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/366.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/226.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/337.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/277.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/170.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/121.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/243.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/303.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/183.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/212.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/352.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/342.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/202.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/193.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/313.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/253.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/131.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/160.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/267.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/327.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/236.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/376.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/154.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/385.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/294.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/105.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/58.md\n",
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/markdown/markdown-documentation-transformers/29.md\n",
      "100个文档平均运行时间:0.0001秒\n"
     ]
    }
   ],
   "source": [
    "type=\".md\"\n",
    "paths,splitter = choose_type(type)\n",
    "\n",
    "file_paths = [os.path.join(paths, f) for f in os.listdir(paths) if f.endswith(type)]\n",
    "file_paths = file_paths[:100]\n",
    "total_time = 0\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sample_text = f.read()\n",
    "\n",
    "    print(f\"\\n测试文件: {file_path}\")\n",
    "    start_time = time.time()\n",
    "    splitter.split_text(sample_text)\n",
    "    end_time = time.time()\n",
    "    total_time += end_time - start_time\n",
    "    \n",
    "average_time = total_time / len(file_paths)\n",
    "print(f\"100个文档平均运行时间:{average_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高亮展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机选取的文本片段起始索引: 2755, 长度: 20000\n",
      "选取的文本片段:\n",
      "lore the following options:\n",
      "\n",
      "-   [Look into building your own custom Docker container with efficient softare prebuilds](#efficient-software-prebuilds)\n",
      "-   [Consider a model that uses Mixture of Experts (MoE)](#mixture-of-experts)\n",
      "-   [Convert your model to BetterTransformer to leverage PyTorch native attention](#using-pytorch-native-attention)\n",
      "\n",
      "Finally, if all of the above is still not enough, even after switching to a server-grade GPU like A100, consider moving to a multi-GPU setup. All these approaches are still valid in a multi-GPU setup, plus you can leverage additional parallelism techniques outlined in the [multi-GPU section](perf_train_gpu_many).\n",
      "\n",
      "## Batch size choice\n",
      "\n",
      "To achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and input/output neuron counts that are of size 2^N. Often it’s a multiple of 8, but it can be higher depending on the hardware being used and the model’s dtype.\n",
      "\n",
      "For reference, check out NVIDIA’s recommendation for [input/output neuron counts](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) and [batch size](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size) for fully connected layers (which are involved in GEMMs (General Matrix Multiplications)).\n",
      "\n",
      "[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) define the multiplier based on the dtype and the hardware. For instance, for fp16 data type a multiple of 8 is recommended, unless it’s an A100 GPU, in which case use multiples of 64.\n",
      "\n",
      "For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup.\n",
      "\n",
      "## Gradient Accumulation\n",
      "\n",
      "The **gradient accumulation** method aims to calculate gradients in smaller increments instead of computing them for the entire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward and backward passes through the model and accumulating the gradients during the process. Once a sufficient number of gradients have been accumulated, the model’s optimization step is executed. By employing gradient accumulation, it becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU’s memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can slow down the training process.\n",
      "\n",
      "You can enable gradient accumulation by adding the `gradient_accumulation_steps` argument to [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n",
      "```\n",
      "\n",
      "In the above example, your effective batch size becomes 4.\n",
      "\n",
      "Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example [further down in this guide](#using-accelerate).\n",
      "\n",
      "While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can result in a more pronounced training slowdown. Consider the following example. Let’s say, the `per_device_train_batch_size=4` without gradient accumulation hits the GPU’s limit. If you would like to train with batches of size 64, do not set the `per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` and set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of the available GPU resources.\n",
      "\n",
      "For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537) and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\n",
      "\n",
      "## Gradient Checkpointing\n",
      "\n",
      "Some large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. This is because there are other components that also require memory storage.\n",
      "\n",
      "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
      "\n",
      "**Gradient checkpointing** offers a compromise between these two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For an in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).\n",
      "\n",
      "To enable gradient checkpointing in the [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer), pass the corresponding a flag to [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(\n",
      "    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\n",
      ")\n",
      "```\n",
      "\n",
      "Alternatively, use 🤗 Accelerate - find the 🤗 Accelerate example [further in this guide](#using-accelerate).\n",
      "\n",
      "While gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.\n",
      "\n",
      "## Mixed precision training\n",
      "\n",
      "**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by utilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point precision (fp32 or float32) to represent and process variables. However, not all variables require this high precision level to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit floating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed in half-precision, while some are still in full precision, the approach is called mixed precision training.\n",
      "\n",
      "Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures (such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check out the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about the differences between these data types.\n",
      "\n",
      "### fp16\n",
      "\n",
      "The main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\n",
      "\n",
      "To enable mixed precision training, set the `fp16` flag to `True`:\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n",
      "```\n",
      "\n",
      "If you prefer to use 🤗 Accelerate, find the 🤗 Accelerate example [further in this guide](#using-accelerate).\n",
      "\n",
      "### BF16\n",
      "\n",
      "If you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While bf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have is `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which is about the same as fp32 - because both have 8-bits used for the numerical range.\n",
      "\n",
      "You can enable BF16 in the 🤗 Trainer with:\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(bf16=True, **default_args)\n",
      "```\n",
      "\n",
      "### TF32\n",
      "\n",
      "The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total. It’s “magical” in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add the following to your code:\n",
      "\n",
      "```\n",
      "import torch\n",
      "torch.backends.cuda.matmul.allow_tf32 = True\n",
      "torch.backends.cudnn.allow_tf32 = True\n",
      "```\n",
      "\n",
      "CUDA will automatically switch to using tf32 instead of fp32 where possible, assuming that the used GPU is from the Ampere series.\n",
      "\n",
      "According to [NVIDIA research](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/), the majority of machine learning training workloads show the same perplexity and convergence with tf32 training as with fp32. If you’re already using fp16 or bf16 mixed precision it may help with the throughput as well.\n",
      "\n",
      "You can enable this mode in the 🤗 Trainer:\n",
      "\n",
      "```\n",
      "TrainingArguments(tf32=True, **default_args)\n",
      "```\n",
      "\n",
      "tf32 can’t be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an internal CUDA data type. You need `torch>=1.7` to use tf32 data types.\n",
      "\n",
      "For additional information on tf32 vs other precisions, please refer to the following benchmarks: [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803) and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189).\n",
      "\n",
      "## Flash Attention 2\n",
      "\n",
      "You can speedup the training throughput by using Flash Attention 2 integration in transformers. Check out the appropriate section in the [single GPU section](./perf_infer_gpu_one#Flash-Attention-2) to learn more about how to load a model with Flash Attention 2 modules.\n",
      "\n",
      "## Optimizer choice\n",
      "\n",
      "The most common optimizer used to train transformer models is Adam or AdamW (Adam with weight decay). Adam achieves good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory footprint of the order of the number of model parameters. To remedy this, you can use an alternative optimizer. For example if you have [NVIDIA/apex](https://github.com/NVIDIA/apex) installed, `adamw_apex_fused` will give you the fastest training experience among all supported AdamW optimizers.\n",
      "\n",
      "[Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer) integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, `adamw_apex_fused`, `adamw_anyprecision`, `adafactor`, or `adamw_bnb_8bit`. More optimizers can be plugged in via a third-party implementation.\n",
      "\n",
      "Let’s take a closer look at two alternatives to AdamW optimizer:\n",
      "\n",
      "1.  `adafactor` which is available in [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer)\n",
      "2.  `adamw_bnb_8bit` is also available in Trainer, but a third-party integration is provided below for demonstration.\n",
      "\n",
      "For comparison, for a 3B-parameter model, like “t5-3b”:\n",
      "\n",
      "-   A standard AdamW optimizer will need 24GB of GPU memory because it uses 8 bytes for each parameter (8\\*3 => 24GB)\n",
      "-   Adafactor optimizer will need more than 12GB. It uses slightly more than 4 bytes for each parameter, so 4\\*3 and then some extra.\n",
      "-   8bit BNB quantized optimizer will use only (2\\*3) 6GB if all optimizer states are quantized.\n",
      "\n",
      "### Adafactor\n",
      "\n",
      "Adafactor doesn’t store rolling averages for each element in weight matrices. Instead, it keeps aggregated information (sums of rolling averages row- and column-wise), significantly reducing its footprint. However, compared to Adam, Adafactor may have slower convergence in certain cases.\n",
      "\n",
      "You can switch to Adafactor by setting `optim=\"adafactor\"` in [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adafactor\", **default_args)\n",
      "```\n",
      "\n",
      "Combined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training) you can notice up to 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of Adafactor can be worse than Adam.\n",
      "\n",
      "### 8-bit Adam\n",
      "\n",
      "Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind mixed precision training.\n",
      "\n",
      "To use `adamw_bnb_8bit`, you simply need to set `optim=\"adamw_bnb_8bit\"` in [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adamw_bnb_8bit\", **default_args)\n",
      "```\n",
      "\n",
      "However, we can also use a third-party implementation of the 8-bit optimizer for demonstration purposes to see how that can be integrated.\n",
      "\n",
      "First, follow the installation guide in the GitHub [repo](https://github.com/TimDettmers/bitsandbytes) to install the `bitsandbytes` library that implements the 8-bit Adam optimizer.\n",
      "\n",
      "Next you need to initialize the optimizer. This involves two steps:\n",
      "\n",
      "-   First, group the model’s parameters into two groups - one where weight decay should be applied, and the other one where it should not. Usually, biases and layer norm parameters are not weight decayed.\n",
      "-   Then do some argument housekeeping to use the same parameters as the previously used AdamW optimizer.\n",
      "\n",
      "```\n",
      "import bitsandbytes as bnb\n",
      "from torch import nn\n",
      "from transformers.trainer_pt_utils import get_parameter_names\n",
      "\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
      "\n",
      "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
      "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
      "optimizer_grouped_parameters = [\n",
      "    {\n",
      "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
      "        \"weight_decay\": training_args.weight_decay,\n",
      "    },\n",
      "    {\n",
      "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
      "        \"weight_decay\": 0.0,\n",
      "    },\n",
      "]\n",
      "\n",
      "optimizer_kwargs = {\n",
      "    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
      "    \"eps\": training_args.adam_epsilon,\n",
      "}\n",
      "optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
      "adam_bnb_optim = bnb.optim.Adam8bit(\n",
      "    optimizer_grouped_parameters,\n",
      "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
      "    eps=training_args.adam_epsilon,\n",
      "    lr=training_args.learning_rate,\n",
      ")\n",
      "```\n",
      "\n",
      "Finally, pass the custom optimizer as an argument to the `Trainer`:\n",
      "\n",
      "```\n",
      "trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))\n",
      "```\n",
      "\n",
      "Combined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training), you can expect to get about a 3x memory improvement and even slightly higher throughput as using Adafactor.\n",
      "\n",
      "### multi\\_tensor\n",
      "\n",
      "pytorch-nightly introduced `torch.optim._multi_tensor` which should significantly speed up the optimizers for situations with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner, take a look at this GitHub [issue](https://github.com/huggingface/transformers/issues/9965).\n",
      "\n",
      "## Data preloading\n",
      "\n",
      "One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default, everything happens in the main process, and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization. Configure the following arguments to reduce the bottleneck:\n",
      "\n",
      "-   `DataLoader(pin_memory=True, ...)` - ensures the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory.\n",
      "-   `DataLoader(num_workers=4, ...)` - spawn several workers to preload data faster. During training, watch the GPU utilization stats; if it’s far from 100%, experiment with increasing the number of workers. Of course, the problem could be elsewhere, so many workers won’t necessarily lead to better performance.\n",
      "\n",
      "When using [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer), the corresponding [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments) are: `dataloader_pin_memory` (`True` by default), and `dataloader_num_workers` (defaults to `0`).\n",
      "\n",
      "## DeepSpeed ZeRO\n",
      "\n",
      "DeepSpeed is an open-source deep learning optimization library that is integrated with 🤗 Transformers and 🤗 Accelerate. It provides a wide range of features and optimizations designed to improve the efficiency and scalability of large-scale deep learning training.\n",
      "\n",
      "If your model fits onto a single GPU and you have enough space to fit a small batch size, you don’t need to use DeepSpeed as it’ll only slow things down. However, if the model doesn’t fit onto a single GPU or you can’t fit a small batch, you can leverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models. In this case, you need to separately [install the library](main_classes/deepspeed#installation), then follow one of the guides to create a configuration file and launch DeepSpeed:\n",
      "\n",
      "-   For an in-depth guide on DeepSpeed integration with [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer), review [the corresponding documentation](main_classes/deepspeed), specifically the [section for a single GPU](main_classes/deepspeed#deployment-with-one-gpu). Some adjustments are required to use DeepSpeed in a notebook; please take a look at the [corresponding guide](main_classes/deepspeed#deployment-in-notebooks).\n",
      "-   If you prefer to use 🤗 Accelerate, refer to [🤗 Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed).\n",
      "\n",
      "## Using torch.compile\n",
      "\n",
      "PyTorch 2.0 introduced a new compile function that doesn’t require any modification to existing PyTorch code but can optimize your code by adding a single line of code: `model = torch.compile(model)`.\n",
      "\n",
      "If using [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer), you only need `to` pass the `torch_compile` option in the [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(torch_compile=True, **default_args)\n",
      "```\n",
      "\n",
      "`torch.compile` uses Python’s frame evaluation API to automatically create a graph from existing PyTorch programs. After capturing the graph, different backends can be deployed to lower the graph to an optimized engine. You can find more details and benchmarks in [PyTorch documentation](https://pytorch.org/get-started/pytorch-2.0/).\n",
      "\n",
      "`torch.compile` has a growing list of backends, which can be found in by calling `torchdynamo.list_backends()`, each of which with its optional dependencies.\n",
      "\n",
      "Choose which backend to use by specifying it via `torch_compile_backend` in the [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments). Some of the most commonly used backends are:\n",
      "\n",
      "**Debugging backends**:\n",
      "\n",
      "-   `dynamo.optimize(\"eager\")` - Uses PyTorch to run the extracted GraphModule. This is quite useful in debugging TorchDynamo issues.\n",
      "-   `dynamo.optimize(\"aot_eager\")` - Uses AotAutograd with no compiler, i.e, just using PyTorch eager for the AotAutograd’s extracted forward and backward graphs. This is useful for debugging, and unlikely to give speedups.\n",
      "\n",
      "**Training & inference backends**:\n",
      "\n",
      "\n",
      "文本已分成 16 块。\n",
      "分块结果:\n",
      "Chunk 1: lore the following options:\n",
      "\n",
      "-   [Look into building your own custom Docker container with efficient softare prebuilds](#efficient-software-prebuilds)\n",
      "-   [Consider a model that uses Mixture of Experts (MoE)](#mixture-of-experts)\n",
      "-   [Convert your model to BetterTransformer to leverage PyTorch native attention](#using-pytorch-native-attention)\n",
      "\n",
      "Finally, if all of the above is still not enough, even after switching to a server-grade GPU like A100, consider moving to a multi-GPU setup. All these approaches are still valid in a multi-GPU setup, plus you can leverage additional parallelism techniques outlined in the [multi-GPU section](perf_train_gpu_many).\n",
      "\n",
      "Chunk 2: Batch size choice\n",
      "\n",
      "To achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and input/output neuron counts that are of size 2^N. Often it’s a multiple of 8, but it can be higher depending on the hardware being used and the model’s dtype.\n",
      "\n",
      "For reference, check out NVIDIA’s recommendation for [input/output neuron counts](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) and [batch size](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size) for fully connected layers (which are involved in GEMMs (General Matrix Multiplications)).\n",
      "\n",
      "[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) define the multiplier based on the dtype and the hardware. For instance, for fp16 data type a multiple of 8 is recommended, unless it’s an A100 GPU, in which case use multiples of 64.\n",
      "\n",
      "For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup.\n",
      "\n",
      "Chunk 3: Gradient Accumulation\n",
      "\n",
      "The **gradient accumulation** method aims to calculate gradients in smaller increments instead of computing them for the entire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward and backward passes through the model and accumulating the gradients during the process. Once a sufficient number of gradients have been accumulated, the model’s optimization step is executed. By employing gradient accumulation, it becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU’s memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can slow down the training process.\n",
      "\n",
      "You can enable gradient accumulation by adding the `gradient_accumulation_steps` argument to [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n",
      "```\n",
      "\n",
      "In the above example, your effective batch size becomes 4.\n",
      "\n",
      "Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example [further down in this guide](#using-accelerate).\n",
      "\n",
      "While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can result in a more pronounced training slowdown. Consider the following example. Let’s say, the `per_device_train_batch_size=4` without gradient accumulation hits the GPU’s limit. If you would like to train with batches of size 64, do not set the `per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` and set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of the available GPU resources.\n",
      "\n",
      "For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537) and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\n",
      "\n",
      "Chunk 4: Gradient Checkpointing\n",
      "\n",
      "Some large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. This is because there are other components that also require memory storage.\n",
      "\n",
      "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
      "\n",
      "**Gradient checkpointing** offers a compromise between these two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For an in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).\n",
      "\n",
      "To enable gradient checkpointing in the [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer), pass the corresponding a flag to [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(\n",
      "    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\n",
      ")\n",
      "```\n",
      "\n",
      "Alternatively, use 🤗 Accelerate - find the 🤗 Accelerate example [further in this guide](#using-accelerate).\n",
      "\n",
      "While gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.\n",
      "\n",
      "Chunk 5: Mixed precision training\n",
      "\n",
      "**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by utilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point precision (fp32 or float32) to represent and process variables. However, not all variables require this high precision level to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit floating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed in half-precision, while some are still in full precision, the approach is called mixed precision training.\n",
      "\n",
      "Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures (such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check out the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about the differences between these data types.\n",
      "\n",
      "Chunk 6: fp16\n",
      "\n",
      "The main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\n",
      "\n",
      "To enable mixed precision training, set the `fp16` flag to `True`:\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n",
      "```\n",
      "\n",
      "If you prefer to use 🤗 Accelerate, find the 🤗 Accelerate example [further in this guide](#using-accelerate).\n",
      "\n",
      "Chunk 7: BF16\n",
      "\n",
      "If you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While bf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have is `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which is about the same as fp32 - because both have 8-bits used for the numerical range.\n",
      "\n",
      "You can enable BF16 in the 🤗 Trainer with:\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(bf16=True, **default_args)\n",
      "```\n",
      "\n",
      "Chunk 8: TF32\n",
      "\n",
      "The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total. It’s “magical” in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add the following to your code:\n",
      "\n",
      "```\n",
      "import torch\n",
      "torch.backends.cuda.matmul.allow_tf32 = True\n",
      "torch.backends.cudnn.allow_tf32 = True\n",
      "```\n",
      "\n",
      "CUDA will automatically switch to using tf32 instead of fp32 where possible, assuming that the used GPU is from the Ampere series.\n",
      "\n",
      "According to [NVIDIA research](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/), the majority of machine learning training workloads show the same perplexity and convergence with tf32 training as with fp32. If you’re already using fp16 or bf16 mixed precision it may help with the throughput as well.\n",
      "\n",
      "You can enable this mode in the 🤗 Trainer:\n",
      "\n",
      "```\n",
      "TrainingArguments(tf32=True, **default_args)\n",
      "```\n",
      "\n",
      "tf32 can’t be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an internal CUDA data type. You need `torch>=1.7` to use tf32 data types.\n",
      "\n",
      "For additional information on tf32 vs other precisions, please refer to the following benchmarks: [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803) and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189).\n",
      "\n",
      "Chunk 9: Flash Attention 2\n",
      "\n",
      "You can speedup the training throughput by using Flash Attention 2 integration in transformers. Check out the appropriate section in the [single GPU section](./perf_infer_gpu_one#Flash-Attention-2) to learn more about how to load a model with Flash Attention 2 modules.\n",
      "\n",
      "Chunk 10: Optimizer choice\n",
      "\n",
      "The most common optimizer used to train transformer models is Adam or AdamW (Adam with weight decay). Adam achieves good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory footprint of the order of the number of model parameters. To remedy this, you can use an alternative optimizer. For example if you have [NVIDIA/apex](https://github.com/NVIDIA/apex) installed, `adamw_apex_fused` will give you the fastest training experience among all supported AdamW optimizers.\n",
      "\n",
      "[Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer) integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, `adamw_apex_fused`, `adamw_anyprecision`, `adafactor`, or `adamw_bnb_8bit`. More optimizers can be plugged in via a third-party implementation.\n",
      "\n",
      "Let’s take a closer look at two alternatives to AdamW optimizer:\n",
      "\n",
      "1.  `adafactor` which is available in [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer)\n",
      "2.  `adamw_bnb_8bit` is also available in Trainer, but a third-party integration is provided below for demonstration.\n",
      "\n",
      "For comparison, for a 3B-parameter model, like “t5-3b”:\n",
      "\n",
      "-   A standard AdamW optimizer will need 24GB of GPU memory because it uses 8 bytes for each parameter (8\\*3 => 24GB)\n",
      "-   Adafactor optimizer will need more than 12GB. It uses slightly more than 4 bytes for each parameter, so 4\\*3 and then some extra.\n",
      "-   8bit BNB quantized optimizer will use only (2\\*3) 6GB if all optimizer states are quantized.\n",
      "\n",
      "Chunk 11: Adafactor\n",
      "\n",
      "Adafactor doesn’t store rolling averages for each element in weight matrices. Instead, it keeps aggregated information (sums of rolling averages row- and column-wise), significantly reducing its footprint. However, compared to Adam, Adafactor may have slower convergence in certain cases.\n",
      "\n",
      "You can switch to Adafactor by setting `optim=\"adafactor\"` in [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adafactor\", **default_args)\n",
      "```\n",
      "\n",
      "Combined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training) you can notice up to 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of Adafactor can be worse than Adam.\n",
      "\n",
      "Chunk 12: 8-bit Adam\n",
      "\n",
      "Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind mixed precision training.\n",
      "\n",
      "To use `adamw_bnb_8bit`, you simply need to set `optim=\"adamw_bnb_8bit\"` in [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adamw_bnb_8bit\", **default_args)\n",
      "```\n",
      "\n",
      "However, we can also use a third-party implementation of the 8-bit optimizer for demonstration purposes to see how that can be integrated.\n",
      "\n",
      "First, follow the installation guide in the GitHub [repo](https://github.com/TimDettmers/bitsandbytes) to install the `bitsandbytes` library that implements the 8-bit Adam optimizer.\n",
      "\n",
      "Next you need to initialize the optimizer. This involves two steps:\n",
      "\n",
      "-   First, group the model’s parameters into two groups - one where weight decay should be applied, and the other one where it should not. Usually, biases and layer norm parameters are not weight decayed.\n",
      "-   Then do some argument housekeeping to use the same parameters as the previously used AdamW optimizer.\n",
      "\n",
      "```\n",
      "import bitsandbytes as bnb\n",
      "from torch import nn\n",
      "from transformers.trainer_pt_utils import get_parameter_names\n",
      "\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
      "\n",
      "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
      "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
      "optimizer_grouped_parameters = [\n",
      "    {\n",
      "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
      "        \"weight_decay\": training_args.weight_decay,\n",
      "    },\n",
      "    {\n",
      "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
      "        \"weight_decay\": 0.0,\n",
      "    },\n",
      "]\n",
      "\n",
      "optimizer_kwargs = {\n",
      "    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
      "    \"eps\": training_args.adam_epsilon,\n",
      "}\n",
      "optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
      "adam_bnb_optim = bnb.optim.Adam8bit(\n",
      "    optimizer_grouped_parameters,\n",
      "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
      "    eps=training_args.adam_epsilon,\n",
      "    lr=training_args.learning_rate,\n",
      ")\n",
      "```\n",
      "\n",
      "Finally, pass the custom optimizer as an argument to the `Trainer`:\n",
      "\n",
      "```\n",
      "trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))\n",
      "```\n",
      "\n",
      "Combined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training), you can expect to get about a 3x memory improvement and even slightly higher throughput as using Adafactor.\n",
      "\n",
      "Chunk 13: multi\\_tensor\n",
      "\n",
      "pytorch-nightly introduced `torch.optim._multi_tensor` which should significantly speed up the optimizers for situations with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner, take a look at this GitHub [issue](https://github.com/huggingface/transformers/issues/9965).\n",
      "\n",
      "Chunk 14: Data preloading\n",
      "\n",
      "One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default, everything happens in the main process, and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization. Configure the following arguments to reduce the bottleneck:\n",
      "\n",
      "-   `DataLoader(pin_memory=True, ...)` - ensures the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory.\n",
      "-   `DataLoader(num_workers=4, ...)` - spawn several workers to preload data faster. During training, watch the GPU utilization stats; if it’s far from 100%, experiment with increasing the number of workers. Of course, the problem could be elsewhere, so many workers won’t necessarily lead to better performance.\n",
      "\n",
      "When using [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer), the corresponding [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments) are: `dataloader_pin_memory` (`True` by default), and `dataloader_num_workers` (defaults to `0`).\n",
      "\n",
      "Chunk 15: DeepSpeed ZeRO\n",
      "\n",
      "DeepSpeed is an open-source deep learning optimization library that is integrated with 🤗 Transformers and 🤗 Accelerate. It provides a wide range of features and optimizations designed to improve the efficiency and scalability of large-scale deep learning training.\n",
      "\n",
      "If your model fits onto a single GPU and you have enough space to fit a small batch size, you don’t need to use DeepSpeed as it’ll only slow things down. However, if the model doesn’t fit onto a single GPU or you can’t fit a small batch, you can leverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models. In this case, you need to separately [install the library](main_classes/deepspeed#installation), then follow one of the guides to create a configuration file and launch DeepSpeed:\n",
      "\n",
      "-   For an in-depth guide on DeepSpeed integration with [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer), review [the corresponding documentation](main_classes/deepspeed), specifically the [section for a single GPU](main_classes/deepspeed#deployment-with-one-gpu). Some adjustments are required to use DeepSpeed in a notebook; please take a look at the [corresponding guide](main_classes/deepspeed#deployment-in-notebooks).\n",
      "-   If you prefer to use 🤗 Accelerate, refer to [🤗 Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed).\n",
      "\n",
      "Chunk 16: Using torch.compile\n",
      "\n",
      "PyTorch 2.0 introduced a new compile function that doesn’t require any modification to existing PyTorch code but can optimize your code by adding a single line of code: `model = torch.compile(model)`.\n",
      "\n",
      "If using [Trainer](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer), you only need `to` pass the `torch_compile` option in the [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments):\n",
      "\n",
      "```\n",
      "training_args = TrainingArguments(torch_compile=True, **default_args)\n",
      "```\n",
      "\n",
      "`torch.compile` uses Python’s frame evaluation API to automatically create a graph from existing PyTorch programs. After capturing the graph, different backends can be deployed to lower the graph to an optimized engine. You can find more details and benchmarks in [PyTorch documentation](https://pytorch.org/get-started/pytorch-2.0/).\n",
      "\n",
      "`torch.compile` has a growing list of backends, which can be found in by calling `torchdynamo.list_backends()`, each of which with its optional dependencies.\n",
      "\n",
      "Choose which backend to use by specifying it via `torch_compile_backend` in the [TrainingArguments](/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments). Some of the most commonly used backends are:\n",
      "\n",
      "**Debugging backends**:\n",
      "\n",
      "-   `dynamo.optimize(\"eager\")` - Uses PyTorch to run the extracted GraphModule. This is quite useful in debugging TorchDynamo issues.\n",
      "-   `dynamo.optimize(\"aot_eager\")` - Uses AotAutograd with no compiler, i.e, just using PyTorch eager for the AotAutograd’s extracted forward and backward graphs. This is useful for debugging, and unlikely to give speedups.\n",
      "\n",
      "**Training & inference backends**:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old Headless mode will be removed from the Chrome binary soon. Please use the new Headless mode (https://developer.chrome.com/docs/chromium/new-headless) or the chrome-headless-shell which is a standalone implementation of the old Headless mode (https://developer.chrome.com/blog/chrome-headless-shell).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高亮文本已保存为 PNG 图片:data/test_result/chunkSize50_chunkOverlap10_MarkdownTextSplitter_20241226_182317.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1226/182317.631835:WARNING:viz_main_impl.cc(85)] VizNullHypothesis is disabled (not a warning)\n",
      "[1226/182317.652912:ERROR:cv_display_link_mac.mm(165)] CVDisplayLinkCreateWithCGDisplay failed. CVReturn: -6670\n",
      "[1226/182317.653141:ERROR:cv_display_link_mac.mm(165)] CVDisplayLinkCreateWithCGDisplay failed. CVReturn: -6670\n",
      "363954 bytes written to file /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/test_result/chunkSize50_chunkOverlap10_MarkdownTextSplitter_20241226_182317.png\n"
     ]
    }
   ],
   "source": [
    "max_len = 20000\n",
    "highlighter = TextHighlighter(\n",
    "    long_text=sample_text,\n",
    "    chunking_api=splitter.split_text,\n",
    "    max_length=max_len\n",
    ")\n",
    "\n",
    "output_dir = \"data/test_result\"\n",
    "highlighter.save_highlighted_text(output_dir, \"MarkdownTextSplitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试文件: /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/python/rag.py\n",
      "100个文档平均运行时间:0.0000秒\n"
     ]
    }
   ],
   "source": [
    "type=\".py\"\n",
    "paths,splitter = choose_type(type)\n",
    "\n",
    "file_paths = [os.path.join(paths, f) for f in os.listdir(paths) if f.endswith(type)]\n",
    "file_paths = file_paths[:100]\n",
    "total_time = 0\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sample_text = f.read()\n",
    "\n",
    "    print(f\"\\n测试文件: {file_path}\")\n",
    "    start_time = time.time()\n",
    "    splitter.split_text(sample_text)\n",
    "    end_time = time.time()\n",
    "    \n",
    "average_time = total_time / len(file_paths)\n",
    "print(f\"100个文档平均运行时间:{average_time:.4f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高亮展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机选取的文本片段起始索引: 10130, 长度: 20000\n",
      "选取的文本片段:\n",
      " j in space_positions[i]:\n",
      "                        full_newline_probs.insert(j, np.zeros_like(newline_probs[0]))\n",
      "                        full_sentence_probs.insert(j, np.zeros_like(sentence_probs[0]))\n",
      "\n",
      "                    newline_probs = np.array(full_newline_probs)\n",
      "                    sentence_probs = np.array(full_sentence_probs)\n",
      "\n",
      "                if return_paragraph_probabilities:\n",
      "                    yield sentence_probs, newline_probs\n",
      "                else:\n",
      "                    yield sentence_probs\n",
      "\n",
      "    def split(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        lang_code: str = None,\n",
      "        style: str = None,\n",
      "        threshold: float = None,\n",
      "        stride=64,\n",
      "        block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "        remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "        paragraph_threshold: float = 0.5,\n",
      "        strip_whitespace: bool = False,\n",
      "        do_paragraph_segmentation=False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "        if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._split(\n",
      "                    [text_or_texts],\n",
      "                    lang_code=lang_code,\n",
      "                    style=style,\n",
      "                    threshold=threshold,\n",
      "                    stride=stride,\n",
      "                    block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                    outer_batch_size=outer_batch_size,\n",
      "                    paragraph_threshold=paragraph_threshold,\n",
      "                    strip_whitespace=strip_whitespace,\n",
      "                    do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                    verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            return self._split(\n",
      "                text_or_texts,\n",
      "                lang_code=lang_code,\n",
      "                style=style,\n",
      "                threshold=threshold,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                paragraph_threshold=paragraph_threshold,\n",
      "                strip_whitespace=strip_whitespace,\n",
      "                do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                verbose=verbose,\n",
      "            )\n",
      "\n",
      "    def get_threshold(self, lang_code: str, style: str, return_punctuation_threshold: bool = False):\n",
      "        try:\n",
      "            _, _, punctuation_threshold, threshold = self.mixtures[lang_code][style]\n",
      "        except KeyError:\n",
      "            raise ValueError(f\"Could not find a mixture for the style '{style}' and language '{lang_code}'.\")\n",
      "\n",
      "        if return_punctuation_threshold:\n",
      "            return punctuation_threshold\n",
      "\n",
      "        return threshold\n",
      "\n",
      "    def _split(\n",
      "        self,\n",
      "        texts,\n",
      "        lang_code: str,\n",
      "        style: str,\n",
      "        threshold: float,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        paragraph_threshold: float,\n",
      "        do_paragraph_segmentation: bool,\n",
      "        strip_whitespace: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        if style is not None:\n",
      "            if lang_code is None:\n",
      "                raise ValueError(\"Please specify a `lang_code` when passing a `style` to adapt to.\")\n",
      "\n",
      "            if self.mixtures is None:\n",
      "                raise ValueError(\n",
      "                    \"This model does not have any associated mixtures. Maybe they are missing from the model directory?\"\n",
      "                )\n",
      "\n",
      "            try:\n",
      "                _, _, default_threshold, _ = self.mixtures[lang_code][style]\n",
      "            except KeyError:\n",
      "                raise ValueError(f\"Could not find a mixture for the style '{style}'.\")\n",
      "        else:\n",
      "            # the established default for newline prob threshold is 0.01\n",
      "            default_threshold = 0.01\n",
      "\n",
      "        sentence_threshold = threshold if threshold is not None else default_threshold\n",
      "\n",
      "        for text, probs in zip(\n",
      "            texts,\n",
      "            self.predict_proba(\n",
      "                texts,\n",
      "                lang_code=lang_code,\n",
      "                style=style,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                return_paragraph_probabilities=do_paragraph_segmentation,\n",
      "                verbose=verbose,\n",
      "            ),\n",
      "        ):\n",
      "            if do_paragraph_segmentation:\n",
      "                sentence_probs, newline_probs = probs\n",
      "\n",
      "                offset = 0\n",
      "\n",
      "                paragraphs = []\n",
      "\n",
      "                for paragraph in indices_to_sentences(text, np.where(newline_probs > paragraph_threshold)[0]):\n",
      "                    sentences = []\n",
      "\n",
      "                    for sentence in indices_to_sentences(\n",
      "                        paragraph,\n",
      "                        np.where(\n",
      "                            sentence_probs[offset : offset + len(paragraph)] > sentence_threshold,\n",
      "                        )[0],\n",
      "                        strip_whitespace=strip_whitespace,\n",
      "                    ):\n",
      "                        sentences.append(sentence)\n",
      "\n",
      "                    paragraphs.append(sentences)\n",
      "                    offset += len(paragraph)\n",
      "\n",
      "                yield paragraphs\n",
      "            else:\n",
      "                sentences = indices_to_sentences(\n",
      "                    text, np.where(probs > sentence_threshold)[0], strip_whitespace=strip_whitespace\n",
      "                )\n",
      "                yield sentences\n",
      "\n",
      "\n",
      "class SaT:\n",
      "    def __init__(\n",
      "        self,\n",
      "        model_name_or_model,\n",
      "        from_pretrained_kwargs=None,\n",
      "        ort_providers=None,\n",
      "        ort_kwargs=None,\n",
      "        style_or_domain: str = None,\n",
      "        language: str = None,\n",
      "        lora_path: str = None,  # local\n",
      "        hub_prefix=\"segment-any-text\",\n",
      "    ):\n",
      "        self.model_name_or_model = model_name_or_model\n",
      "        self.ort_providers = ort_providers\n",
      "        self.ort_kwargs = ort_kwargs\n",
      "\n",
      "        self.use_lora = False\n",
      "\n",
      "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebookAI/xlm-roberta-base\")\n",
      "\n",
      "        if isinstance(model_name_or_model, (str, Path)):\n",
      "            model_name = str(model_name_or_model)\n",
      "            is_local = os.path.isdir(model_name)\n",
      "\n",
      "            if not is_local and hub_prefix is not None:\n",
      "                model_name_to_fetch = f\"{hub_prefix}/{model_name}\"\n",
      "            else:\n",
      "                model_name_to_fetch = model_name\n",
      "\n",
      "            if is_local:\n",
      "                model_path = Path(model_name)\n",
      "                onnx_path = model_path / \"model_optimized.onnx\"\n",
      "                if not onnx_path.exists():\n",
      "                    onnx_path = None\n",
      "            else:\n",
      "                # no need to load if no ort_providers set\n",
      "                if ort_providers is not None:\n",
      "                    onnx_path = cached_file(\n",
      "                        model_name_to_fetch, \"model_optimized.onnx\", **(from_pretrained_kwargs or {})\n",
      "                    )\n",
      "                else:\n",
      "                    onnx_path = None\n",
      "\n",
      "            if ort_providers is not None:\n",
      "                if onnx_path is None:\n",
      "                    raise ValueError(\n",
      "                        \"Could not find an ONNX model in the model directory. Try `use_ort=False` to run with PyTorch.\"\n",
      "                    )\n",
      "\n",
      "                try:\n",
      "                    import onnxruntime as ort  # noqa\n",
      "                except ModuleNotFoundError:\n",
      "                    raise ValueError(\"Please install `onnxruntime` to use SaT with an ONNX model.\")\n",
      "\n",
      "                # to register models for AutoConfig\n",
      "                import wtpsplit.configs  # noqa\n",
      "\n",
      "                self.model = SaTORTWrapper(\n",
      "                    AutoConfig.from_pretrained(model_name_to_fetch, **(from_pretrained_kwargs or {})),\n",
      "                    ort.InferenceSession(str(onnx_path), providers=ort_providers, **(ort_kwargs or {})),\n",
      "                )\n",
      "                if lora_path:\n",
      "                    raise ValueError(\n",
      "                        \"If using ONNX with LoRA, execute `scripts/export_to_onnx_sat.py` with `use_lora=True`.\"\n",
      "                        \"Reference the chosen `output_dir` here for `model_name_or_model`. and set `lora_path=None`.\"\n",
      "                    )\n",
      "            else:\n",
      "                # to register models for AutoConfig\n",
      "                try:\n",
      "                    import torch  # noqa\n",
      "                except ModuleNotFoundError:\n",
      "                    raise ValueError(\"Please install `torch` to use WtP with a PyTorch model.\")\n",
      "\n",
      "                import wtpsplit.models  # noqa\n",
      "\n",
      "                self.model = PyTorchWrapper(\n",
      "                    AutoModelForTokenClassification.from_pretrained(\n",
      "                        model_name_to_fetch, **(from_pretrained_kwargs or {})\n",
      "                    )\n",
      "                )\n",
      "            # LoRA LOADING\n",
      "            if not lora_path:\n",
      "                if (style_or_domain and not language) or (language and not style_or_domain):\n",
      "                    raise ValueError(\"Please specify both language and style_or_domain!\")\n",
      "            if (style_or_domain and language) or lora_path:\n",
      "                import adapters  # noqa\n",
      "                from adapters.models import MODEL_MIXIN_MAPPING  # noqa\n",
      "                from adapters.models.bert.mixin_bert import BertModelAdaptersMixin  # noqa\n",
      "\n",
      "                # monkey patch mixin to avoid forking whole adapters library\n",
      "                MODEL_MIXIN_MAPPING[\"SubwordXLMRobertaModel\"] = BertModelAdaptersMixin\n",
      "                model_type = self.model.model.config.model_type\n",
      "                # adapters need xlm-roberta as model type.\n",
      "                self.model.model.config.model_type = \"xlm-roberta\"\n",
      "                adapters.init(self.model.model)\n",
      "                # reset model type (used later)\n",
      "                self.model.model.config.model_type = model_type\n",
      "                try:\n",
      "                    if not lora_path:\n",
      "                        for file in [\n",
      "                            \"adapter_config.json\",\n",
      "                            \"head_config.json\",\n",
      "                            \"pytorch_adapter.bin\",\n",
      "                            \"pytorch_model_head.bin\",\n",
      "                        ]:\n",
      "                            hf_hub_download(\n",
      "                                repo_id=model_name_to_fetch,\n",
      "                                subfolder=f\"loras/{style_or_domain}/{language}\",\n",
      "                                filename=file,\n",
      "                                local_dir=Constants.CACHE_DIR,\n",
      "                            )\n",
      "                        lora_load_path = str(Constants.CACHE_DIR / \"loras\" / style_or_domain / language)\n",
      "                    else:\n",
      "                        lora_load_path = lora_path\n",
      "\n",
      "                    self.model.model.load_adapter(\n",
      "                        lora_load_path,\n",
      "                        set_active=True,\n",
      "                        with_head=True,\n",
      "                        load_as=\"sat-lora\",\n",
      "                    )\n",
      "                    # merge lora weights into transformer for 0 efficiency overhead\n",
      "                    self.model.model.merge_adapter(\"sat-lora\")\n",
      "                    self.use_lora = True\n",
      "                except:  # noqa\n",
      "                    if lora_path:\n",
      "                        print(f\"LoRA at {lora_path} not found, using base model...\")\n",
      "                    else:\n",
      "                        print(f\"LoRA {style_or_domain}/{language} not found, using base model...\")\n",
      "        else:\n",
      "            if ort_providers is not None:\n",
      "                raise ValueError(\"You can only use onnxruntime with a model directory, not a model object.\")\n",
      "\n",
      "            self.model = model_name_or_model\n",
      "\n",
      "    def __getattr__(self, name):\n",
      "        assert hasattr(self, \"model\")\n",
      "        return getattr(self.model, name)\n",
      "\n",
      "    def predict_proba(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        stride=256,\n",
      "        block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "        remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "        return_paragraph_probabilities=False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "        if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._predict_proba(\n",
      "                    [text_or_texts],\n",
      "                    stride=stride,\n",
      "                    block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                    outer_batch_size=outer_batch_size,\n",
      "                    return_paragraph_probabilities=return_paragraph_probabilities,\n",
      "                    verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            return self._predict_proba(\n",
      "                text_or_texts,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                return_paragraph_probabilities=return_paragraph_probabilities,\n",
      "                verbose=verbose,\n",
      "            )\n",
      "\n",
      "    def _predict_proba(\n",
      "        self,\n",
      "        texts,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        return_paragraph_probabilities: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        def newline_probability_fn(logits):\n",
      "            return sigmoid(logits[:, Constants.NEWLINE_INDEX])\n",
      "\n",
      "        n_outer_batches = math.ceil(len(texts) / outer_batch_size)\n",
      "\n",
      "        for outer_batch_idx in range(n_outer_batches):\n",
      "            start, end = outer_batch_idx * outer_batch_size, min((outer_batch_idx + 1) * outer_batch_size, len(texts))\n",
      "\n",
      "            outer_batch_texts = texts[start:end]\n",
      "            input_texts = []\n",
      "            space_positions = []\n",
      "\n",
      "            for text in outer_batch_texts:\n",
      "                if remove_whitespace_before_inference:\n",
      "                    text_space_positions = []\n",
      "                    input_text = \"\"\n",
      "\n",
      "                    for c in text:\n",
      "                        if c == \" \":\n",
      "                            text_space_positions.append(len(input_text) + len(text_space_positions))\n",
      "                        else:\n",
      "                            input_text += c\n",
      "\n",
      "                    space_positions.append(text_space_positions)\n",
      "                else:\n",
      "                    input_text = text\n",
      "\n",
      "                input_texts.append(input_text)\n",
      "\n",
      "            empty_string_indices = [i for i, text in enumerate(input_texts) if not text.strip()]\n",
      "            # remove empty strings from input_texts\n",
      "            input_texts = [text for text in input_texts if text.strip()]\n",
      "            if input_texts:\n",
      "                outer_batch_logits, _, tokenizer, tokenizer_output = extract(\n",
      "                    input_texts,\n",
      "                    self.model,\n",
      "                    stride=stride,\n",
      "                    max_block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    verbose=verbose,\n",
      "                    tokenizer=self.tokenizer,\n",
      "                )\n",
      "\n",
      "                # convert token probabilities to character probabilities for the entire array\n",
      "                outer_batch_logits = [\n",
      "                    token_to_char_probs(\n",
      "                        input_texts[i],\n",
      "                        tokenizer_output[\"input_ids\"][i],\n",
      "                        outer_batch_logits[i],\n",
      "                        tokenizer,\n",
      "                        tokenizer_output[\"offset_mapping\"][i],\n",
      "                    )\n",
      "                    for i in range(len(input_texts))\n",
      "                ]\n",
      "            else:\n",
      "                outer_batch_logits = []\n",
      "\n",
      "            # add back empty strings\n",
      "            for i in empty_string_indices:\n",
      "                outer_batch_logits.insert(i, np.ones([1, 1]) * -np.inf)\n",
      "\n",
      "            for i, (text, logits) in enumerate(zip(outer_batch_texts, outer_batch_logits)):\n",
      "                sentence_probs = newline_probs = newline_probability_fn(logits)\n",
      "\n",
      "                if remove_whitespace_before_inference:\n",
      "                    full_newline_probs, full_sentence_probs = list(newline_probs), list(sentence_probs)\n",
      "\n",
      "                    for j in space_positions[i]:\n",
      "                        full_newline_probs.insert(j, np.zeros_like(newline_probs[0]))\n",
      "                        full_sentence_probs.insert(j, np.zeros_like(sentence_probs[0]))\n",
      "\n",
      "                    newline_probs = np.array(full_newline_probs)\n",
      "                    sentence_probs = np.array(full_sentence_probs)\n",
      "\n",
      "                if return_paragraph_probabilities:\n",
      "                    yield sentence_probs, newline_probs\n",
      "                else:\n",
      "                    yield sentence_probs\n",
      "\n",
      "    def split(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        threshold: float = None,\n",
      "        stride=64,\n",
      "        block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "        remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "        paragraph_threshold: float = 0.5,\n",
      "        strip_whitespace: bool = False,\n",
      "        do_paragraph_segmentation: bool = False,\n",
      "        treat_newline_as_space: bool = False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "        if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._split(\n",
      "                    [text_or_texts],\n",
      "                    threshold=threshold,\n",
      "                    stride=stride,\n",
      "                    block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                    outer_batch_size=outer_batch_size,\n",
      "                    paragraph_threshold=paragraph_threshold,\n",
      "                    strip_whitespace=strip_whitespace,\n",
      "                    do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                    treat_newline_as_space=treat_newline_as_space,\n",
      "                    verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            return self._split(\n",
      "                text_or_texts,\n",
      "                threshold=threshold,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                paragraph_threshold=paragraph_threshold,\n",
      "                strip_whitespace=strip_whitespace,\n",
      "                do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                treat_newline_as_space=treat_newline_as_space,\n",
      "                verbose=verbose,\n",
      "            )\n",
      "\n",
      "    def _split(\n",
      "        self,\n",
      "        texts,\n",
      "        threshold: float,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        paragraph_threshold: float,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        do_paragraph_segmentation: bool,\n",
      "        treat_newline_as_space: bool,\n",
      "        strip_whitespace: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        def get_default_threshold(model_str: str):\n",
      "            if \"sm\" in model_str:\n",
      "                return 0.25\n",
      "            if self.use_lora:\n",
      "                return 0.5\n",
      "            if \"no-limited-lookahead\" in model_str and \"sm\" not in model_str:\n",
      "                return 0.01\n",
      "            return 0.025\n",
      "\n",
      "        default_threshold = get_default_threshold(self.model_name_or_model)\n",
      "        sentence_threshold = threshold if threshold is not None else default_threshold\n",
      "\n",
      "        for text, probs in zip(\n",
      "            tex\n",
      "\n",
      "文本已分成 2 块。\n",
      "分块结果:\n",
      "Chunk 1:  j in space_positions[i]:\n",
      "                        full_newline_probs.insert(j, np.zeros_like(newline_probs[0]))\n",
      "                        full_sentence_probs.insert(j, np.zeros_like(sentence_probs[0]))\n",
      "\n",
      "                    newline_probs = np.array(full_newline_probs)\n",
      "                    sentence_probs = np.array(full_sentence_probs)\n",
      "\n",
      "                if return_paragraph_probabilities:\n",
      "                    yield sentence_probs, newline_probs\n",
      "                else:\n",
      "                    yield sentence_probs\n",
      "\n",
      "    def split(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        lang_code: str = None,\n",
      "        style: str = None,\n",
      "        threshold: float = None,\n",
      "        stride=64,\n",
      "        block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "        remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "        paragraph_threshold: float = 0.5,\n",
      "        strip_whitespace: bool = False,\n",
      "        do_paragraph_segmentation=False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "        if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._split(\n",
      "                    [text_or_texts],\n",
      "                    lang_code=lang_code,\n",
      "                    style=style,\n",
      "                    threshold=threshold,\n",
      "                    stride=stride,\n",
      "                    block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                    outer_batch_size=outer_batch_size,\n",
      "                    paragraph_threshold=paragraph_threshold,\n",
      "                    strip_whitespace=strip_whitespace,\n",
      "                    do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                    verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            return self._split(\n",
      "                text_or_texts,\n",
      "                lang_code=lang_code,\n",
      "                style=style,\n",
      "                threshold=threshold,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                paragraph_threshold=paragraph_threshold,\n",
      "                strip_whitespace=strip_whitespace,\n",
      "                do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                verbose=verbose,\n",
      "            )\n",
      "\n",
      "    def get_threshold(self, lang_code: str, style: str, return_punctuation_threshold: bool = False):\n",
      "        try:\n",
      "            _, _, punctuation_threshold, threshold = self.mixtures[lang_code][style]\n",
      "        except KeyError:\n",
      "            raise ValueError(f\"Could not find a mixture for the style '{style}' and language '{lang_code}'.\")\n",
      "\n",
      "        if return_punctuation_threshold:\n",
      "            return punctuation_threshold\n",
      "\n",
      "        return threshold\n",
      "\n",
      "    def _split(\n",
      "        self,\n",
      "        texts,\n",
      "        lang_code: str,\n",
      "        style: str,\n",
      "        threshold: float,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        paragraph_threshold: float,\n",
      "        do_paragraph_segmentation: bool,\n",
      "        strip_whitespace: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        if style is not None:\n",
      "            if lang_code is None:\n",
      "                raise ValueError(\"Please specify a `lang_code` when passing a `style` to adapt to.\")\n",
      "\n",
      "            if self.mixtures is None:\n",
      "                raise ValueError(\n",
      "                    \"This model does not have any associated mixtures. Maybe they are missing from the model directory?\"\n",
      "                )\n",
      "\n",
      "            try:\n",
      "                _, _, default_threshold, _ = self.mixtures[lang_code][style]\n",
      "            except KeyError:\n",
      "                raise ValueError(f\"Could not find a mixture for the style '{style}'.\")\n",
      "        else:\n",
      "            # the established default for newline prob threshold is 0.01\n",
      "            default_threshold = 0.01\n",
      "\n",
      "        sentence_threshold = threshold if threshold is not None else default_threshold\n",
      "\n",
      "        for text, probs in zip(\n",
      "            texts,\n",
      "            self.predict_proba(\n",
      "                texts,\n",
      "                lang_code=lang_code,\n",
      "                style=style,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                return_paragraph_probabilities=do_paragraph_segmentation,\n",
      "                verbose=verbose,\n",
      "            ),\n",
      "        ):\n",
      "            if do_paragraph_segmentation:\n",
      "                sentence_probs, newline_probs = probs\n",
      "\n",
      "                offset = 0\n",
      "\n",
      "                paragraphs = []\n",
      "\n",
      "                for paragraph in indices_to_sentences(text, np.where(newline_probs > paragraph_threshold)[0]):\n",
      "                    sentences = []\n",
      "\n",
      "                    for sentence in indices_to_sentences(\n",
      "                        paragraph,\n",
      "                        np.where(\n",
      "                            sentence_probs[offset : offset + len(paragraph)] > sentence_threshold,\n",
      "                        )[0],\n",
      "                        strip_whitespace=strip_whitespace,\n",
      "                    ):\n",
      "                        sentences.append(sentence)\n",
      "\n",
      "                    paragraphs.append(sentences)\n",
      "                    offset += len(paragraph)\n",
      "\n",
      "                yield paragraphs\n",
      "            else:\n",
      "                sentences = indices_to_sentences(\n",
      "                    text, np.where(probs > sentence_threshold)[0], strip_whitespace=strip_whitespace\n",
      "                )\n",
      "                yield sentences\n",
      "\n",
      "\n",
      "Chunk 2: SaT:\n",
      "    def __init__(\n",
      "        self,\n",
      "        model_name_or_model,\n",
      "        from_pretrained_kwargs=None,\n",
      "        ort_providers=None,\n",
      "        ort_kwargs=None,\n",
      "        style_or_domain: str = None,\n",
      "        language: str = None,\n",
      "        lora_path: str = None,  # local\n",
      "        hub_prefix=\"segment-any-text\",\n",
      "    ):\n",
      "        self.model_name_or_model = model_name_or_model\n",
      "        self.ort_providers = ort_providers\n",
      "        self.ort_kwargs = ort_kwargs\n",
      "\n",
      "        self.use_lora = False\n",
      "\n",
      "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebookAI/xlm-roberta-base\")\n",
      "\n",
      "        if isinstance(model_name_or_model, (str, Path)):\n",
      "            model_name = str(model_name_or_model)\n",
      "            is_local = os.path.isdir(model_name)\n",
      "\n",
      "            if not is_local and hub_prefix is not None:\n",
      "                model_name_to_fetch = f\"{hub_prefix}/{model_name}\"\n",
      "            else:\n",
      "                model_name_to_fetch = model_name\n",
      "\n",
      "            if is_local:\n",
      "                model_path = Path(model_name)\n",
      "                onnx_path = model_path / \"model_optimized.onnx\"\n",
      "                if not onnx_path.exists():\n",
      "                    onnx_path = None\n",
      "            else:\n",
      "                # no need to load if no ort_providers set\n",
      "                if ort_providers is not None:\n",
      "                    onnx_path = cached_file(\n",
      "                        model_name_to_fetch, \"model_optimized.onnx\", **(from_pretrained_kwargs or {})\n",
      "                    )\n",
      "                else:\n",
      "                    onnx_path = None\n",
      "\n",
      "            if ort_providers is not None:\n",
      "                if onnx_path is None:\n",
      "                    raise ValueError(\n",
      "                        \"Could not find an ONNX model in the model directory. Try `use_ort=False` to run with PyTorch.\"\n",
      "                    )\n",
      "\n",
      "                try:\n",
      "                    import onnxruntime as ort  # noqa\n",
      "                except ModuleNotFoundError:\n",
      "                    raise ValueError(\"Please install `onnxruntime` to use SaT with an ONNX model.\")\n",
      "\n",
      "                # to register models for AutoConfig\n",
      "                import wtpsplit.configs  # noqa\n",
      "\n",
      "                self.model = SaTORTWrapper(\n",
      "                    AutoConfig.from_pretrained(model_name_to_fetch, **(from_pretrained_kwargs or {})),\n",
      "                    ort.InferenceSession(str(onnx_path), providers=ort_providers, **(ort_kwargs or {})),\n",
      "                )\n",
      "                if lora_path:\n",
      "                    raise ValueError(\n",
      "                        \"If using ONNX with LoRA, execute `scripts/export_to_onnx_sat.py` with `use_lora=True`.\"\n",
      "                        \"Reference the chosen `output_dir` here for `model_name_or_model`. and set `lora_path=None`.\"\n",
      "                    )\n",
      "            else:\n",
      "                # to register models for AutoConfig\n",
      "                try:\n",
      "                    import torch  # noqa\n",
      "                except ModuleNotFoundError:\n",
      "                    raise ValueError(\"Please install `torch` to use WtP with a PyTorch model.\")\n",
      "\n",
      "                import wtpsplit.models  # noqa\n",
      "\n",
      "                self.model = PyTorchWrapper(\n",
      "                    AutoModelForTokenClassification.from_pretrained(\n",
      "                        model_name_to_fetch, **(from_pretrained_kwargs or {})\n",
      "                    )\n",
      "                )\n",
      "            # LoRA LOADING\n",
      "            if not lora_path:\n",
      "                if (style_or_domain and not language) or (language and not style_or_domain):\n",
      "                    raise ValueError(\"Please specify both language and style_or_domain!\")\n",
      "            if (style_or_domain and language) or lora_path:\n",
      "                import adapters  # noqa\n",
      "                from adapters.models import MODEL_MIXIN_MAPPING  # noqa\n",
      "                from adapters.models.bert.mixin_bert import BertModelAdaptersMixin  # noqa\n",
      "\n",
      "                # monkey patch mixin to avoid forking whole adapters library\n",
      "                MODEL_MIXIN_MAPPING[\"SubwordXLMRobertaModel\"] = BertModelAdaptersMixin\n",
      "                model_type = self.model.model.config.model_type\n",
      "                # adapters need xlm-roberta as model type.\n",
      "                self.model.model.config.model_type = \"xlm-roberta\"\n",
      "                adapters.init(self.model.model)\n",
      "                # reset model type (used later)\n",
      "                self.model.model.config.model_type = model_type\n",
      "                try:\n",
      "                    if not lora_path:\n",
      "                        for file in [\n",
      "                            \"adapter_config.json\",\n",
      "                            \"head_config.json\",\n",
      "                            \"pytorch_adapter.bin\",\n",
      "                            \"pytorch_model_head.bin\",\n",
      "                        ]:\n",
      "                            hf_hub_download(\n",
      "                                repo_id=model_name_to_fetch,\n",
      "                                subfolder=f\"loras/{style_or_domain}/{language}\",\n",
      "                                filename=file,\n",
      "                                local_dir=Constants.CACHE_DIR,\n",
      "                            )\n",
      "                        lora_load_path = str(Constants.CACHE_DIR / \"loras\" / style_or_domain / language)\n",
      "                    else:\n",
      "                        lora_load_path = lora_path\n",
      "\n",
      "                    self.model.model.load_adapter(\n",
      "                        lora_load_path,\n",
      "                        set_active=True,\n",
      "                        with_head=True,\n",
      "                        load_as=\"sat-lora\",\n",
      "                    )\n",
      "                    # merge lora weights into transformer for 0 efficiency overhead\n",
      "                    self.model.model.merge_adapter(\"sat-lora\")\n",
      "                    self.use_lora = True\n",
      "                except:  # noqa\n",
      "                    if lora_path:\n",
      "                        print(f\"LoRA at {lora_path} not found, using base model...\")\n",
      "                    else:\n",
      "                        print(f\"LoRA {style_or_domain}/{language} not found, using base model...\")\n",
      "        else:\n",
      "            if ort_providers is not None:\n",
      "                raise ValueError(\"You can only use onnxruntime with a model directory, not a model object.\")\n",
      "\n",
      "            self.model = model_name_or_model\n",
      "\n",
      "    def __getattr__(self, name):\n",
      "        assert hasattr(self, \"model\")\n",
      "        return getattr(self.model, name)\n",
      "\n",
      "    def predict_proba(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        stride=256,\n",
      "        block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "        remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "        return_paragraph_probabilities=False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "        if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._predict_proba(\n",
      "                    [text_or_texts],\n",
      "                    stride=stride,\n",
      "                    block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                    outer_batch_size=outer_batch_size,\n",
      "                    return_paragraph_probabilities=return_paragraph_probabilities,\n",
      "                    verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            return self._predict_proba(\n",
      "                text_or_texts,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                return_paragraph_probabilities=return_paragraph_probabilities,\n",
      "                verbose=verbose,\n",
      "            )\n",
      "\n",
      "    def _predict_proba(\n",
      "        self,\n",
      "        texts,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        return_paragraph_probabilities: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        def newline_probability_fn(logits):\n",
      "            return sigmoid(logits[:, Constants.NEWLINE_INDEX])\n",
      "\n",
      "        n_outer_batches = math.ceil(len(texts) / outer_batch_size)\n",
      "\n",
      "        for outer_batch_idx in range(n_outer_batches):\n",
      "            start, end = outer_batch_idx * outer_batch_size, min((outer_batch_idx + 1) * outer_batch_size, len(texts))\n",
      "\n",
      "            outer_batch_texts = texts[start:end]\n",
      "            input_texts = []\n",
      "            space_positions = []\n",
      "\n",
      "            for text in outer_batch_texts:\n",
      "                if remove_whitespace_before_inference:\n",
      "                    text_space_positions = []\n",
      "                    input_text = \"\"\n",
      "\n",
      "                    for c in text:\n",
      "                        if c == \" \":\n",
      "                            text_space_positions.append(len(input_text) + len(text_space_positions))\n",
      "                        else:\n",
      "                            input_text += c\n",
      "\n",
      "                    space_positions.append(text_space_positions)\n",
      "                else:\n",
      "                    input_text = text\n",
      "\n",
      "                input_texts.append(input_text)\n",
      "\n",
      "            empty_string_indices = [i for i, text in enumerate(input_texts) if not text.strip()]\n",
      "            # remove empty strings from input_texts\n",
      "            input_texts = [text for text in input_texts if text.strip()]\n",
      "            if input_texts:\n",
      "                outer_batch_logits, _, tokenizer, tokenizer_output = extract(\n",
      "                    input_texts,\n",
      "                    self.model,\n",
      "                    stride=stride,\n",
      "                    max_block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    verbose=verbose,\n",
      "                    tokenizer=self.tokenizer,\n",
      "                )\n",
      "\n",
      "                # convert token probabilities to character probabilities for the entire array\n",
      "                outer_batch_logits = [\n",
      "                    token_to_char_probs(\n",
      "                        input_texts[i],\n",
      "                        tokenizer_output[\"input_ids\"][i],\n",
      "                        outer_batch_logits[i],\n",
      "                        tokenizer,\n",
      "                        tokenizer_output[\"offset_mapping\"][i],\n",
      "                    )\n",
      "                    for i in range(len(input_texts))\n",
      "                ]\n",
      "            else:\n",
      "                outer_batch_logits = []\n",
      "\n",
      "            # add back empty strings\n",
      "            for i in empty_string_indices:\n",
      "                outer_batch_logits.insert(i, np.ones([1, 1]) * -np.inf)\n",
      "\n",
      "            for i, (text, logits) in enumerate(zip(outer_batch_texts, outer_batch_logits)):\n",
      "                sentence_probs = newline_probs = newline_probability_fn(logits)\n",
      "\n",
      "                if remove_whitespace_before_inference:\n",
      "                    full_newline_probs, full_sentence_probs = list(newline_probs), list(sentence_probs)\n",
      "\n",
      "                    for j in space_positions[i]:\n",
      "                        full_newline_probs.insert(j, np.zeros_like(newline_probs[0]))\n",
      "                        full_sentence_probs.insert(j, np.zeros_like(sentence_probs[0]))\n",
      "\n",
      "                    newline_probs = np.array(full_newline_probs)\n",
      "                    sentence_probs = np.array(full_sentence_probs)\n",
      "\n",
      "                if return_paragraph_probabilities:\n",
      "                    yield sentence_probs, newline_probs\n",
      "                else:\n",
      "                    yield sentence_probs\n",
      "\n",
      "    def split(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        threshold: float = None,\n",
      "        stride=64,\n",
      "        block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "        remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "        paragraph_threshold: float = 0.5,\n",
      "        strip_whitespace: bool = False,\n",
      "        do_paragraph_segmentation: bool = False,\n",
      "        treat_newline_as_space: bool = False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "        if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._split(\n",
      "                    [text_or_texts],\n",
      "                    threshold=threshold,\n",
      "                    stride=stride,\n",
      "                    block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                    outer_batch_size=outer_batch_size,\n",
      "                    paragraph_threshold=paragraph_threshold,\n",
      "                    strip_whitespace=strip_whitespace,\n",
      "                    do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                    treat_newline_as_space=treat_newline_as_space,\n",
      "                    verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            return self._split(\n",
      "                text_or_texts,\n",
      "                threshold=threshold,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                paragraph_threshold=paragraph_threshold,\n",
      "                strip_whitespace=strip_whitespace,\n",
      "                do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                treat_newline_as_space=treat_newline_as_space,\n",
      "                verbose=verbose,\n",
      "            )\n",
      "\n",
      "    def _split(\n",
      "        self,\n",
      "        texts,\n",
      "        threshold: float,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        paragraph_threshold: float,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        do_paragraph_segmentation: bool,\n",
      "        treat_newline_as_space: bool,\n",
      "        strip_whitespace: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        def get_default_threshold(model_str: str):\n",
      "            if \"sm\" in model_str:\n",
      "                return 0.25\n",
      "            if self.use_lora:\n",
      "                return 0.5\n",
      "            if \"no-limited-lookahead\" in model_str and \"sm\" not in model_str:\n",
      "                return 0.01\n",
      "            return 0.025\n",
      "\n",
      "        default_threshold = get_default_threshold(self.model_name_or_model)\n",
      "        sentence_threshold = threshold if threshold is not None else default_threshold\n",
      "\n",
      "        for text, probs in zip(\n",
      "            tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old Headless mode will be removed from the Chrome binary soon. Please use the new Headless mode (https://developer.chrome.com/docs/chromium/new-headless) or the chrome-headless-shell which is a standalone implementation of the old Headless mode (https://developer.chrome.com/blog/chrome-headless-shell).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高亮文本已保存为 PNG 图片:data/test_result/chunkSize100_chunkOverlap0_PythonCodeTextSplitter_20241226_182353.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1226/182354.052649:WARNING:viz_main_impl.cc(85)] VizNullHypothesis is disabled (not a warning)\n",
      "[1226/182354.072763:ERROR:cv_display_link_mac.mm(165)] CVDisplayLinkCreateWithCGDisplay failed. CVReturn: -6670\n",
      "[1226/182354.073003:ERROR:cv_display_link_mac.mm(165)] CVDisplayLinkCreateWithCGDisplay failed. CVReturn: -6670\n",
      "168300 bytes written to file /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/data/test_result/chunkSize100_chunkOverlap0_PythonCodeTextSplitter_20241226_182353.png\n"
     ]
    }
   ],
   "source": [
    "max_len = 20000\n",
    "highlighter = TextHighlighter(\n",
    "    long_text=sample_text,\n",
    "    chunking_api=splitter.split_text,\n",
    "    max_length=max_len\n",
    ")\n",
    "\n",
    "output_dir = \"data/test_result\"\n",
    "highlighter.save_highlighted_text(output_dir, \"PythonCodeTextSplitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机选取的文本片段起始索引: 6658, 长度: 20000\n",
      "选取的文本片段:\n",
      "e=verbose,\n",
      "            )\n",
      "\n",
      "    def _predict_proba(\n",
      "        self,\n",
      "        texts,\n",
      "        lang_code: str,\n",
      "        style: str,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        return_paragraph_probabilities: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        if style is not None:\n",
      "            if lang_code is None:\n",
      "                raise ValueError(\"Please specify a `lang_code` when passing a `style` to adapt to.\")\n",
      "\n",
      "            if self.mixtures is None:\n",
      "                raise ValueError(\n",
      "                    \"This model does not have any associated mixtures. Maybe they are missing from the model directory?\"\n",
      "                )\n",
      "\n",
      "            try:\n",
      "                clf, _, _, _ = self.mixtures[lang_code][style]\n",
      "            except KeyError:\n",
      "                raise ValueError(f\"Could not find a mixture for the style '{style}'.\")\n",
      "        else:\n",
      "            clf = None\n",
      "\n",
      "        n_outer_batches = math.ceil(len(texts) / outer_batch_size)\n",
      "\n",
      "        for outer_batch_idx in range(n_outer_batches):\n",
      "            start, end = outer_batch_idx * outer_batch_size, min((outer_batch_idx + 1) * outer_batch_size, len(texts))\n",
      "\n",
      "            outer_batch_texts = texts[start:end]\n",
      "            input_texts = []\n",
      "            space_positions = []\n",
      "\n",
      "            for text in outer_batch_texts:\n",
      "                if remove_whitespace_before_inference:\n",
      "                    text_space_positions = []\n",
      "                    input_text = \"\"\n",
      "\n",
      "                    for c in text:\n",
      "                        if c == \" \":\n",
      "                            text_space_positions.append(len(input_text) + len(text_space_positions))\n",
      "                        else:\n",
      "                            input_text += c\n",
      "\n",
      "                    space_positions.append(text_space_positions)\n",
      "                else:\n",
      "                    input_text = text\n",
      "\n",
      "                input_texts.append(input_text)\n",
      "\n",
      "            empty_string_indices = [i for i, text in enumerate(input_texts) if not text.strip()]\n",
      "            # remove empty strings from input_texts\n",
      "            input_texts = [text for text in input_texts if text.strip()]\n",
      "\n",
      "            if input_texts:\n",
      "                outer_batch_logits = extract(\n",
      "                    input_texts,\n",
      "                    self.model,\n",
      "                    lang_code=lang_code,\n",
      "                    stride=stride,\n",
      "                    max_block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    verbose=verbose,\n",
      "                )[0]\n",
      "            else:\n",
      "                outer_batch_logits = []\n",
      "\n",
      "            def newline_probability_fn(logits):\n",
      "                return sigmoid(logits[:, Constants.NEWLINE_INDEX])\n",
      "\n",
      "            # add back empty strings\n",
      "            for i in empty_string_indices:\n",
      "                outer_batch_logits.insert(i, np.ones([1, 1]) * -np.inf)\n",
      "\n",
      "            for i, (text, logits) in enumerate(zip(outer_batch_texts, outer_batch_logits)):\n",
      "                if style is not None:\n",
      "                    sentence_probs = clf.predict_proba(logits)[:, 1]\n",
      "                    newline_probs = newline_probability_fn(logits)\n",
      "                else:\n",
      "                    sentence_probs = newline_probs = newline_probability_fn(logits)\n",
      "\n",
      "                if remove_whitespace_before_inference:\n",
      "                    full_newline_probs, full_sentence_probs = list(newline_probs), list(sentence_probs)\n",
      "\n",
      "                    for j in space_positions[i]:\n",
      "                        full_newline_probs.insert(j, np.zeros_like(newline_probs[0]))\n",
      "                        full_sentence_probs.insert(j, np.zeros_like(sentence_probs[0]))\n",
      "\n",
      "                    newline_probs = np.array(full_newline_probs)\n",
      "                    sentence_probs = np.array(full_sentence_probs)\n",
      "\n",
      "                if return_paragraph_probabilities:\n",
      "                    yield sentence_probs, newline_probs\n",
      "                else:\n",
      "                    yield sentence_probs\n",
      "\n",
      "    def split(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        lang_code: str = None,\n",
      "        style: str = None,\n",
      "        threshold: float = None,\n",
      "        stride=64,\n",
      "        block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "        remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "        paragraph_threshold: float = 0.5,\n",
      "        strip_whitespace: bool = False,\n",
      "        do_paragraph_segmentation=False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "        if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._split(\n",
      "                    [text_or_texts],\n",
      "                    lang_code=lang_code,\n",
      "                    style=style,\n",
      "                    threshold=threshold,\n",
      "                    stride=stride,\n",
      "                    block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                    outer_batch_size=outer_batch_size,\n",
      "                    paragraph_threshold=paragraph_threshold,\n",
      "                    strip_whitespace=strip_whitespace,\n",
      "                    do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                    verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            return self._split(\n",
      "                text_or_texts,\n",
      "                lang_code=lang_code,\n",
      "                style=style,\n",
      "                threshold=threshold,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                paragraph_threshold=paragraph_threshold,\n",
      "                strip_whitespace=strip_whitespace,\n",
      "                do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "                verbose=verbose,\n",
      "            )\n",
      "\n",
      "    def get_threshold(self, lang_code: str, style: str, return_punctuation_threshold: bool = False):\n",
      "        try:\n",
      "            _, _, punctuation_threshold, threshold = self.mixtures[lang_code][style]\n",
      "        except KeyError:\n",
      "            raise ValueError(f\"Could not find a mixture for the style '{style}' and language '{lang_code}'.\")\n",
      "\n",
      "        if return_punctuation_threshold:\n",
      "            return punctuation_threshold\n",
      "\n",
      "        return threshold\n",
      "\n",
      "    def _split(\n",
      "        self,\n",
      "        texts,\n",
      "        lang_code: str,\n",
      "        style: str,\n",
      "        threshold: float,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        paragraph_threshold: float,\n",
      "        do_paragraph_segmentation: bool,\n",
      "        strip_whitespace: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        if style is not None:\n",
      "            if lang_code is None:\n",
      "                raise ValueError(\"Please specify a `lang_code` when passing a `style` to adapt to.\")\n",
      "\n",
      "            if self.mixtures is None:\n",
      "                raise ValueError(\n",
      "                    \"This model does not have any associated mixtures. Maybe they are missing from the model directory?\"\n",
      "                )\n",
      "\n",
      "            try:\n",
      "                _, _, default_threshold, _ = self.mixtures[lang_code][style]\n",
      "            except KeyError:\n",
      "                raise ValueError(f\"Could not find a mixture for the style '{style}'.\")\n",
      "        else:\n",
      "            # the established default for newline prob threshold is 0.01\n",
      "            default_threshold = 0.01\n",
      "\n",
      "        sentence_threshold = threshold if threshold is not None else default_threshold\n",
      "\n",
      "        for text, probs in zip(\n",
      "            texts,\n",
      "            self.predict_proba(\n",
      "                texts,\n",
      "                lang_code=lang_code,\n",
      "                style=style,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                return_paragraph_probabilities=do_paragraph_segmentation,\n",
      "                verbose=verbose,\n",
      "            ),\n",
      "        ):\n",
      "            if do_paragraph_segmentation:\n",
      "                sentence_probs, newline_probs = probs\n",
      "\n",
      "                offset = 0\n",
      "\n",
      "                paragraphs = []\n",
      "\n",
      "                for paragraph in indices_to_sentences(text, np.where(newline_probs > paragraph_threshold)[0]):\n",
      "                    sentences = []\n",
      "\n",
      "                    for sentence in indices_to_sentences(\n",
      "                        paragraph,\n",
      "                        np.where(\n",
      "                            sentence_probs[offset : offset + len(paragraph)] > sentence_threshold,\n",
      "                        )[0],\n",
      "                        strip_whitespace=strip_whitespace,\n",
      "                    ):\n",
      "                        sentences.append(sentence)\n",
      "\n",
      "                    paragraphs.append(sentences)\n",
      "                    offset += len(paragraph)\n",
      "\n",
      "                yield paragraphs\n",
      "            else:\n",
      "                sentences = indices_to_sentences(\n",
      "                    text, np.where(probs > sentence_threshold)[0], strip_whitespace=strip_whitespace\n",
      "                )\n",
      "                yield sentences\n",
      "\n",
      "\n",
      "class SaT:\n",
      "    def __init__(\n",
      "        self,\n",
      "        model_name_or_model,\n",
      "        from_pretrained_kwargs=None,\n",
      "        ort_providers=None,\n",
      "        ort_kwargs=None,\n",
      "        style_or_domain: str = None,\n",
      "        language: str = None,\n",
      "        lora_path: str = None,  # local\n",
      "        hub_prefix=\"segment-any-text\",\n",
      "    ):\n",
      "        self.model_name_or_model = model_name_or_model\n",
      "        self.ort_providers = ort_providers\n",
      "        self.ort_kwargs = ort_kwargs\n",
      "\n",
      "        self.use_lora = False\n",
      "\n",
      "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebookAI/xlm-roberta-base\")\n",
      "\n",
      "        if isinstance(model_name_or_model, (str, Path)):\n",
      "            model_name = str(model_name_or_model)\n",
      "            is_local = os.path.isdir(model_name)\n",
      "\n",
      "            if not is_local and hub_prefix is not None:\n",
      "                model_name_to_fetch = f\"{hub_prefix}/{model_name}\"\n",
      "            else:\n",
      "                model_name_to_fetch = model_name\n",
      "\n",
      "            if is_local:\n",
      "                model_path = Path(model_name)\n",
      "                onnx_path = model_path / \"model_optimized.onnx\"\n",
      "                if not onnx_path.exists():\n",
      "                    onnx_path = None\n",
      "            else:\n",
      "                # no need to load if no ort_providers set\n",
      "                if ort_providers is not None:\n",
      "                    onnx_path = cached_file(\n",
      "                        model_name_to_fetch, \"model_optimized.onnx\", **(from_pretrained_kwargs or {})\n",
      "                    )\n",
      "                else:\n",
      "                    onnx_path = None\n",
      "\n",
      "            if ort_providers is not None:\n",
      "                if onnx_path is None:\n",
      "                    raise ValueError(\n",
      "                        \"Could not find an ONNX model in the model directory. Try `use_ort=False` to run with PyTorch.\"\n",
      "                    )\n",
      "\n",
      "                try:\n",
      "                    import onnxruntime as ort  # noqa\n",
      "                except ModuleNotFoundError:\n",
      "                    raise ValueError(\"Please install `onnxruntime` to use SaT with an ONNX model.\")\n",
      "\n",
      "                # to register models for AutoConfig\n",
      "                import wtpsplit.configs  # noqa\n",
      "\n",
      "                self.model = SaTORTWrapper(\n",
      "                    AutoConfig.from_pretrained(model_name_to_fetch, **(from_pretrained_kwargs or {})),\n",
      "                    ort.InferenceSession(str(onnx_path), providers=ort_providers, **(ort_kwargs or {})),\n",
      "                )\n",
      "                if lora_path:\n",
      "                    raise ValueError(\n",
      "                        \"If using ONNX with LoRA, execute `scripts/export_to_onnx_sat.py` with `use_lora=True`.\"\n",
      "                        \"Reference the chosen `output_dir` here for `model_name_or_model`. and set `lora_path=None`.\"\n",
      "                    )\n",
      "            else:\n",
      "                # to register models for AutoConfig\n",
      "                try:\n",
      "                    import torch  # noqa\n",
      "                except ModuleNotFoundError:\n",
      "                    raise ValueError(\"Please install `torch` to use WtP with a PyTorch model.\")\n",
      "\n",
      "                import wtpsplit.models  # noqa\n",
      "\n",
      "                self.model = PyTorchWrapper(\n",
      "                    AutoModelForTokenClassification.from_pretrained(\n",
      "                        model_name_to_fetch, **(from_pretrained_kwargs or {})\n",
      "                    )\n",
      "                )\n",
      "            # LoRA LOADING\n",
      "            if not lora_path:\n",
      "                if (style_or_domain and not language) or (language and not style_or_domain):\n",
      "                    raise ValueError(\"Please specify both language and style_or_domain!\")\n",
      "            if (style_or_domain and language) or lora_path:\n",
      "                import adapters  # noqa\n",
      "                from adapters.models import MODEL_MIXIN_MAPPING  # noqa\n",
      "                from adapters.models.bert.mixin_bert import BertModelAdaptersMixin  # noqa\n",
      "\n",
      "                # monkey patch mixin to avoid forking whole adapters library\n",
      "                MODEL_MIXIN_MAPPING[\"SubwordXLMRobertaModel\"] = BertModelAdaptersMixin\n",
      "                model_type = self.model.model.config.model_type\n",
      "                # adapters need xlm-roberta as model type.\n",
      "                self.model.model.config.model_type = \"xlm-roberta\"\n",
      "                adapters.init(self.model.model)\n",
      "                # reset model type (used later)\n",
      "                self.model.model.config.model_type = model_type\n",
      "                try:\n",
      "                    if not lora_path:\n",
      "                        for file in [\n",
      "                            \"adapter_config.json\",\n",
      "                            \"head_config.json\",\n",
      "                            \"pytorch_adapter.bin\",\n",
      "                            \"pytorch_model_head.bin\",\n",
      "                        ]:\n",
      "                            hf_hub_download(\n",
      "                                repo_id=model_name_to_fetch,\n",
      "                                subfolder=f\"loras/{style_or_domain}/{language}\",\n",
      "                                filename=file,\n",
      "                                local_dir=Constants.CACHE_DIR,\n",
      "                            )\n",
      "                        lora_load_path = str(Constants.CACHE_DIR / \"loras\" / style_or_domain / language)\n",
      "                    else:\n",
      "                        lora_load_path = lora_path\n",
      "\n",
      "                    self.model.model.load_adapter(\n",
      "                        lora_load_path,\n",
      "                        set_active=True,\n",
      "                        with_head=True,\n",
      "                        load_as=\"sat-lora\",\n",
      "                    )\n",
      "                    # merge lora weights into transformer for 0 efficiency overhead\n",
      "                    self.model.model.merge_adapter(\"sat-lora\")\n",
      "                    self.use_lora = True\n",
      "                except:  # noqa\n",
      "                    if lora_path:\n",
      "                        print(f\"LoRA at {lora_path} not found, using base model...\")\n",
      "                    else:\n",
      "                        print(f\"LoRA {style_or_domain}/{language} not found, using base model...\")\n",
      "        else:\n",
      "            if ort_providers is not None:\n",
      "                raise ValueError(\"You can only use onnxruntime with a model directory, not a model object.\")\n",
      "\n",
      "            self.model = model_name_or_model\n",
      "\n",
      "    def __getattr__(self, name):\n",
      "        assert hasattr(self, \"model\")\n",
      "        return getattr(self.model, name)\n",
      "\n",
      "    def predict_proba(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        stride=256,\n",
      "        block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "        remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "        return_paragraph_probabilities=False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "        if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._predict_proba(\n",
      "                    [text_or_texts],\n",
      "                    stride=stride,\n",
      "                    block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                    outer_batch_size=outer_batch_size,\n",
      "                    return_paragraph_probabilities=return_paragraph_probabilities,\n",
      "                    verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            return self._predict_proba(\n",
      "                text_or_texts,\n",
      "                stride=stride,\n",
      "                block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "                remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "                outer_batch_size=outer_batch_size,\n",
      "                return_paragraph_probabilities=return_paragraph_probabilities,\n",
      "                verbose=verbose,\n",
      "            )\n",
      "\n",
      "    def _predict_proba(\n",
      "        self,\n",
      "        texts,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "        return_paragraph_probabilities: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "        def newline_probability_fn(logits):\n",
      "            return sigmoid(logits[:, Constants.NEWLINE_INDEX])\n",
      "\n",
      "        n_outer_batches = math.ceil(len(texts) / outer_batch_size)\n",
      "\n",
      "        for outer_batch_idx in range(n_outer_batches):\n",
      "            start, end = outer_batch_idx * outer_batch_size, min((outer_batch_idx + 1) * outer_batch_size, len(texts))\n",
      "\n",
      "            outer_batch_texts = texts[start:end]\n",
      "            input_texts = []\n",
      "            space_positions = []\n",
      "\n",
      "            for text in outer_batch_texts:\n",
      "                if remove_whitespace_before_inference:\n",
      "                    text_space_positions = []\n",
      "                    input_text = \"\"\n",
      "\n",
      "                    for c in text:\n",
      "                        if c == \" \":\n",
      "                            text_space_positions.append(len(input_text) + len(text_space_positions))\n",
      "                        else:\n",
      "                            input_text += c\n",
      "\n",
      "                    space_positions.append(text_space_positions)\n",
      "                else:\n",
      "                    input_text = text\n",
      "\n",
      "                input_texts.append(input_text)\n",
      "\n",
      "            empty_string_indices = [i for i, text in enumerate(input_texts) if not text.strip()]\n",
      "            # remove empty strings from input_texts\n",
      "            input_texts = [text for text in input_texts if text.strip()]\n",
      "            if input_texts:\n",
      "                outer_batch_logits, _, tokenizer, tokenizer_output = extract(\n",
      "                    input_texts,\n",
      "                    self.model,\n",
      "                    stride=stride,\n",
      "                    max_block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "                    verbose=verbose,\n",
      "                    tokenizer=self.tokenizer,\n",
      "                )\n",
      "\n",
      "                # convert token probabilities to character probabilities for the entire array\n",
      "                outer_batch_logits = [\n",
      "                    token_to_char_probs(\n",
      "                        input_texts[i],\n",
      "                        tokenizer_output[\"input_ids\"][i],\n",
      "                        outer_batch_logits[i],\n",
      "                        tokenizer,\n",
      "                        tokenizer_output[\"offset_mapping\"][i],\n",
      "                    )\n",
      "                    for i in range(len(input_texts))\n",
      "                ]\n",
      "            else:\n",
      "                outer_batch_logits = []\n",
      "\n",
      "            # add back empty strings\n",
      "            for i in empty_string_indices:\n",
      "                outer_batch_logits.insert(i, np.ones([1, 1]) * -np.inf)\n",
      "\n",
      "            for i, (text, logits) in enumerate(zip(outer_batch_texts, outer_batch_logits)):\n",
      "                sentence_probs = newline_probs = newline_probability_fn(logits)\n",
      "\n",
      "                if remove_whitespace_before_inference:\n",
      "                    full_newline_probs, full_sentence_probs = list(newline_probs), list(sentence_probs)\n",
      "\n",
      "             \n",
      "\n",
      "文本已分成 290 块。\n",
      "分块结果:\n",
      "Chunk 1: e=verbose,\n",
      "            )\n",
      "Chunk 2: def _predict_proba(\n",
      "        self,\n",
      "        texts,\n",
      "        lang_code: str,\n",
      "        style: str,\n",
      "Chunk 3: stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "Chunk 4: pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "Chunk 5: outer_batch_size: int,\n",
      "        return_paragraph_probabilities: bool,\n",
      "        verbose: bool,\n",
      "Chunk 6: ):\n",
      "        if style is not None:\n",
      "            if lang_code is None:\n",
      "Chunk 7: raise ValueError(\"Please specify a `lang_code` when passing a `style` to adapt\n",
      "Chunk 8: to.\")\n",
      "Chunk 9: if self.mixtures is None:\n",
      "                raise ValueError(\n",
      "Chunk 10: \"This model does not have any associated mixtures. Maybe they are missing from\n",
      "Chunk 11: the model directory?\"\n",
      "Chunk 12: )\n",
      "Chunk 13: try:\n",
      "                clf, _, _, _ = self.mixtures[lang_code][style]\n",
      "Chunk 14: except KeyError:\n",
      "Chunk 15: raise ValueError(f\"Could not find a mixture for the style '{style}'.\")\n",
      "Chunk 16: else:\n",
      "            clf = None\n",
      "Chunk 17: n_outer_batches = math.ceil(len(texts) / outer_batch_size)\n",
      "Chunk 18: for outer_batch_idx in range(n_outer_batches):\n",
      "Chunk 19: start, end = outer_batch_idx * outer_batch_size, min((outer_batch_idx + 1) *\n",
      "Chunk 20: outer_batch_size, len(texts))\n",
      "Chunk 21: outer_batch_texts = texts[start:end]\n",
      "            input_texts = []\n",
      "Chunk 22: space_positions = []\n",
      "Chunk 23: for text in outer_batch_texts:\n",
      "                if remove_whitespace_before_inference:\n",
      "Chunk 24: text_space_positions = []\n",
      "                    input_text = \"\"\n",
      "Chunk 25: for c in text:\n",
      "                        if c == \" \":\n",
      "Chunk 26: text_space_positions.append(len(input_text) +\n",
      "Chunk 27: len(text_space_positions))\n",
      "Chunk 28: else:\n",
      "                            input_text += c\n",
      "Chunk 29: space_positions.append(text_space_positions)\n",
      "                else:\n",
      "Chunk 30: input_text = text\n",
      "Chunk 31: input_texts.append(input_text)\n",
      "Chunk 32: empty_string_indices = [i for i, text in enumerate(input_texts) if not text.strip()]\n",
      "Chunk 33: # remove empty strings from input_texts\n",
      "Chunk 34: input_texts = [text for text in input_texts if text.strip()]\n",
      "Chunk 35: if input_texts:\n",
      "                outer_batch_logits = extract(\n",
      "Chunk 36: input_texts,\n",
      "                    self.model,\n",
      "Chunk 37: lang_code=lang_code,\n",
      "                    stride=stride,\n",
      "Chunk 38: max_block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "Chunk 39: pad_last_batch=pad_last_batch,\n",
      "                    verbose=verbose,\n",
      "Chunk 40: )[0]\n",
      "            else:\n",
      "                outer_batch_logits = []\n",
      "Chunk 41: def newline_probability_fn(logits):\n",
      "Chunk 42: return sigmoid(logits[:, Constants.NEWLINE_INDEX])\n",
      "Chunk 43: # add back empty strings\n",
      "            for i in empty_string_indices:\n",
      "Chunk 44: outer_batch_logits.insert(i, np.ones([1, 1]) * -np.inf)\n",
      "Chunk 45: for i, (text, logits) in enumerate(zip(outer_batch_texts, outer_batch_logits)):\n",
      "Chunk 46: if style is not None:\n",
      "Chunk 47: sentence_probs = clf.predict_proba(logits)[:, 1]\n",
      "Chunk 48: newline_probs = newline_probability_fn(logits)\n",
      "                else:\n",
      "Chunk 49: sentence_probs = newline_probs = newline_probability_fn(logits)\n",
      "Chunk 50: if remove_whitespace_before_inference:\n",
      "Chunk 51: full_newline_probs, full_sentence_probs = list(newline_probs),\n",
      "Chunk 52: list(sentence_probs)\n",
      "Chunk 53: for j in space_positions[i]:\n",
      "Chunk 54: full_newline_probs.insert(j, np.zeros_like(newline_probs[0]))\n",
      "Chunk 55: full_sentence_probs.insert(j, np.zeros_like(sentence_probs[0]))\n",
      "Chunk 56: newline_probs = np.array(full_newline_probs)\n",
      "Chunk 57: sentence_probs = np.array(full_sentence_probs)\n",
      "Chunk 58: if return_paragraph_probabilities:\n",
      "Chunk 59: yield sentence_probs, newline_probs\n",
      "                else:\n",
      "Chunk 60: yield sentence_probs\n",
      "Chunk 61: def split(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        lang_code: str = None,\n",
      "Chunk 62: style: str = None,\n",
      "        threshold: float = None,\n",
      "        stride=64,\n",
      "Chunk 63: block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "Chunk 64: remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "Chunk 65: paragraph_threshold: float = 0.5,\n",
      "        strip_whitespace: bool = False,\n",
      "Chunk 66: do_paragraph_segmentation=False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "Chunk 67: if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "                self._split(\n",
      "Chunk 68: [text_or_texts],\n",
      "                    lang_code=lang_code,\n",
      "Chunk 69: style=style,\n",
      "                    threshold=threshold,\n",
      "Chunk 70: stride=stride,\n",
      "                    block_size=block_size,\n",
      "Chunk 71: batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "Chunk 72: remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "Chunk 73: outer_batch_size=outer_batch_size,\n",
      "Chunk 74: paragraph_threshold=paragraph_threshold,\n",
      "Chunk 75: strip_whitespace=strip_whitespace,\n",
      "Chunk 76: do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "Chunk 77: verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "Chunk 78: return self._split(\n",
      "                text_or_texts,\n",
      "                lang_code=lang_code,\n",
      "Chunk 79: style=style,\n",
      "                threshold=threshold,\n",
      "                stride=stride,\n",
      "Chunk 80: block_size=block_size,\n",
      "                batch_size=batch_size,\n",
      "Chunk 81: pad_last_batch=pad_last_batch,\n",
      "Chunk 82: remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "Chunk 83: outer_batch_size=outer_batch_size,\n",
      "Chunk 84: paragraph_threshold=paragraph_threshold,\n",
      "Chunk 85: strip_whitespace=strip_whitespace,\n",
      "Chunk 86: do_paragraph_segmentation=do_paragraph_segmentation,\n",
      "Chunk 87: verbose=verbose,\n",
      "            )\n",
      "Chunk 88: def get_threshold(self, lang_code: str, style: str, return_punctuation_threshold: bool =\n",
      "Chunk 89: False):\n",
      "Chunk 90: try:\n",
      "            _, _, punctuation_threshold, threshold = self.mixtures[lang_code][style]\n",
      "Chunk 91: except KeyError:\n",
      "Chunk 92: raise ValueError(f\"Could not find a mixture for the style '{style}' and language\n",
      "Chunk 93: '{lang_code}'.\")\n",
      "Chunk 94: if return_punctuation_threshold:\n",
      "            return punctuation_threshold\n",
      "Chunk 95: return threshold\n",
      "Chunk 96: def _split(\n",
      "        self,\n",
      "        texts,\n",
      "        lang_code: str,\n",
      "        style: str,\n",
      "Chunk 97: threshold: float,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "        batch_size: int,\n",
      "Chunk 98: pad_last_batch: bool,\n",
      "        remove_whitespace_before_inference: bool,\n",
      "Chunk 99: outer_batch_size: int,\n",
      "        paragraph_threshold: float,\n",
      "Chunk 100: do_paragraph_segmentation: bool,\n",
      "        strip_whitespace: bool,\n",
      "        verbose: bool,\n",
      "Chunk 101: ):\n",
      "        if style is not None:\n",
      "            if lang_code is None:\n",
      "Chunk 102: raise ValueError(\"Please specify a `lang_code` when passing a `style` to adapt\n",
      "Chunk 103: to.\")\n",
      "Chunk 104: if self.mixtures is None:\n",
      "                raise ValueError(\n",
      "Chunk 105: \"This model does not have any associated mixtures. Maybe they are missing from\n",
      "Chunk 106: the model directory?\"\n",
      "Chunk 107: )\n",
      "Chunk 108: try:\n",
      "                _, _, default_threshold, _ = self.mixtures[lang_code][style]\n",
      "Chunk 109: except KeyError:\n",
      "Chunk 110: raise ValueError(f\"Could not find a mixture for the style '{style}'.\")\n",
      "Chunk 111: else:\n",
      "            # the established default for newline prob threshold is 0.01\n",
      "Chunk 112: default_threshold = 0.01\n",
      "Chunk 113: sentence_threshold = threshold if threshold is not None else default_threshold\n",
      "Chunk 114: for text, probs in zip(\n",
      "            texts,\n",
      "            self.predict_proba(\n",
      "Chunk 115: texts,\n",
      "                lang_code=lang_code,\n",
      "                style=style,\n",
      "Chunk 116: stride=stride,\n",
      "                block_size=block_size,\n",
      "Chunk 117: batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "Chunk 118: remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "Chunk 119: outer_batch_size=outer_batch_size,\n",
      "Chunk 120: return_paragraph_probabilities=do_paragraph_segmentation,\n",
      "Chunk 121: verbose=verbose,\n",
      "            ),\n",
      "        ):\n",
      "Chunk 122: if do_paragraph_segmentation:\n",
      "                sentence_probs, newline_probs = probs\n",
      "Chunk 123: offset = 0\n",
      "\n",
      "                paragraphs = []\n",
      "Chunk 124: for paragraph in indices_to_sentences(text, np.where(newline_probs >\n",
      "Chunk 125: paragraph_threshold)[0]):\n",
      "Chunk 126: sentences = []\n",
      "Chunk 127: for sentence in indices_to_sentences(\n",
      "                        paragraph,\n",
      "Chunk 128: np.where(\n",
      "Chunk 129: sentence_probs[offset : offset + len(paragraph)] > sentence_threshold,\n",
      "Chunk 130: )[0],\n",
      "                        strip_whitespace=strip_whitespace,\n",
      "Chunk 131: ):\n",
      "                        sentences.append(sentence)\n",
      "Chunk 132: paragraphs.append(sentences)\n",
      "                    offset += len(paragraph)\n",
      "Chunk 133: yield paragraphs\n",
      "            else:\n",
      "Chunk 134: sentences = indices_to_sentences(\n",
      "Chunk 135: text, np.where(probs > sentence_threshold)[0],\n",
      "Chunk 136: strip_whitespace=strip_whitespace\n",
      "Chunk 137: )\n",
      "                yield sentences\n",
      "Chunk 138: class SaT:\n",
      "    def __init__(\n",
      "        self,\n",
      "        model_name_or_model,\n",
      "Chunk 139: from_pretrained_kwargs=None,\n",
      "        ort_providers=None,\n",
      "        ort_kwargs=None,\n",
      "Chunk 140: style_or_domain: str = None,\n",
      "        language: str = None,\n",
      "Chunk 141: lora_path: str = None,  # local\n",
      "        hub_prefix=\"segment-any-text\",\n",
      "    ):\n",
      "Chunk 142: self.model_name_or_model = model_name_or_model\n",
      "        self.ort_providers = ort_providers\n",
      "Chunk 143: self.ort_kwargs = ort_kwargs\n",
      "Chunk 144: self.use_lora = False\n",
      "Chunk 145: self.tokenizer = AutoTokenizer.from_pretrained(\"facebookAI/xlm-roberta-base\")\n",
      "Chunk 146: if isinstance(model_name_or_model, (str, Path)):\n",
      "Chunk 147: model_name = str(model_name_or_model)\n",
      "            is_local = os.path.isdir(model_name)\n",
      "Chunk 148: if not is_local and hub_prefix is not None:\n",
      "Chunk 149: model_name_to_fetch = f\"{hub_prefix}/{model_name}\"\n",
      "            else:\n",
      "Chunk 150: model_name_to_fetch = model_name\n",
      "Chunk 151: if is_local:\n",
      "                model_path = Path(model_name)\n",
      "Chunk 152: onnx_path = model_path / \"model_optimized.onnx\"\n",
      "Chunk 153: if not onnx_path.exists():\n",
      "                    onnx_path = None\n",
      "            else:\n",
      "Chunk 154: # no need to load if no ort_providers set\n",
      "Chunk 155: if ort_providers is not None:\n",
      "                    onnx_path = cached_file(\n",
      "Chunk 156: model_name_to_fetch, \"model_optimized.onnx\", **(from_pretrained_kwargs or\n",
      "Chunk 157: {})\n",
      "Chunk 158: )\n",
      "                else:\n",
      "                    onnx_path = None\n",
      "Chunk 159: if ort_providers is not None:\n",
      "                if onnx_path is None:\n",
      "Chunk 160: raise ValueError(\n",
      "Chunk 161: \"Could not find an ONNX model in the model directory. Try `use_ort=False`\n",
      "Chunk 162: to run with PyTorch.\"\n",
      "Chunk 163: )\n",
      "Chunk 164: try:\n",
      "                    import onnxruntime as ort  # noqa\n",
      "Chunk 165: except ModuleNotFoundError:\n",
      "Chunk 166: raise ValueError(\"Please install `onnxruntime` to use SaT with an ONNX model.\")\n",
      "Chunk 167: # to register models for AutoConfig\n",
      "Chunk 168: import wtpsplit.configs  # noqa\n",
      "Chunk 169: self.model = SaTORTWrapper(\n",
      "Chunk 170: AutoConfig.from_pretrained(model_name_to_fetch, **(from_pretrained_kwargs or\n",
      "Chunk 171: {})),\n",
      "Chunk 172: ort.InferenceSession(str(onnx_path), providers=ort_providers, **(ort_kwargs or\n",
      "Chunk 173: {})),\n",
      "Chunk 174: )\n",
      "                if lora_path:\n",
      "                    raise ValueError(\n",
      "Chunk 175: \"If using ONNX with LoRA, execute `scripts/export_to_onnx_sat.py` with\n",
      "Chunk 176: `use_lora=True`.\"\n",
      "Chunk 177: \"Reference the chosen `output_dir` here for `model_name_or_model`. and set\n",
      "Chunk 178: `lora_path=None`.\"\n",
      "Chunk 179: )\n",
      "            else:\n",
      "                # to register models for AutoConfig\n",
      "Chunk 180: try:\n",
      "                    import torch  # noqa\n",
      "Chunk 181: except ModuleNotFoundError:\n",
      "Chunk 182: raise ValueError(\"Please install `torch` to use WtP with a PyTorch model.\")\n",
      "Chunk 183: import wtpsplit.models  # noqa\n",
      "Chunk 184: self.model = PyTorchWrapper(\n",
      "Chunk 185: AutoModelForTokenClassification.from_pretrained(\n",
      "Chunk 186: model_name_to_fetch, **(from_pretrained_kwargs or {})\n",
      "                    )\n",
      "Chunk 187: )\n",
      "            # LoRA LOADING\n",
      "            if not lora_path:\n",
      "Chunk 188: if (style_or_domain and not language) or (language and not style_or_domain):\n",
      "Chunk 189: raise ValueError(\"Please specify both language and style_or_domain!\")\n",
      "Chunk 190: if (style_or_domain and language) or lora_path:\n",
      "                import adapters  # noqa\n",
      "Chunk 191: from adapters.models import MODEL_MIXIN_MAPPING  # noqa\n",
      "Chunk 192: from adapters.models.bert.mixin_bert import BertModelAdaptersMixin  # noqa\n",
      "Chunk 193: # monkey patch mixin to avoid forking whole adapters library\n",
      "Chunk 194: MODEL_MIXIN_MAPPING[\"SubwordXLMRobertaModel\"] = BertModelAdaptersMixin\n",
      "Chunk 195: model_type = self.model.model.config.model_type\n",
      "Chunk 196: # adapters need xlm-roberta as model type.\n",
      "Chunk 197: self.model.model.config.model_type = \"xlm-roberta\"\n",
      "Chunk 198: adapters.init(self.model.model)\n",
      "                # reset model type (used later)\n",
      "Chunk 199: self.model.model.config.model_type = model_type\n",
      "                try:\n",
      "Chunk 200: if not lora_path:\n",
      "                        for file in [\n",
      "Chunk 201: \"adapter_config.json\",\n",
      "                            \"head_config.json\",\n",
      "Chunk 202: \"pytorch_adapter.bin\",\n",
      "Chunk 203: \"pytorch_model_head.bin\",\n",
      "                        ]:\n",
      "Chunk 204: hf_hub_download(\n",
      "Chunk 205: repo_id=model_name_to_fetch,\n",
      "Chunk 206: subfolder=f\"loras/{style_or_domain}/{language}\",\n",
      "Chunk 207: filename=file,\n",
      "Chunk 208: local_dir=Constants.CACHE_DIR,\n",
      "                            )\n",
      "Chunk 209: lora_load_path = str(Constants.CACHE_DIR / \"loras\" / style_or_domain /\n",
      "Chunk 210: language)\n",
      "Chunk 211: else:\n",
      "                        lora_load_path = lora_path\n",
      "Chunk 212: self.model.model.load_adapter(\n",
      "                        lora_load_path,\n",
      "Chunk 213: set_active=True,\n",
      "                        with_head=True,\n",
      "Chunk 214: load_as=\"sat-lora\",\n",
      "                    )\n",
      "Chunk 215: # merge lora weights into transformer for 0 efficiency overhead\n",
      "Chunk 216: self.model.model.merge_adapter(\"sat-lora\")\n",
      "Chunk 217: self.use_lora = True\n",
      "                except:  # noqa\n",
      "Chunk 218: if lora_path:\n",
      "Chunk 219: print(f\"LoRA at {lora_path} not found, using base model...\")\n",
      "Chunk 220: else:\n",
      "Chunk 221: print(f\"LoRA {style_or_domain}/{language} not found, using base model...\")\n",
      "Chunk 222: else:\n",
      "            if ort_providers is not None:\n",
      "Chunk 223: raise ValueError(\"You can only use onnxruntime with a model directory, not a model\n",
      "Chunk 224: object.\")\n",
      "Chunk 225: self.model = model_name_or_model\n",
      "Chunk 226: def __getattr__(self, name):\n",
      "        assert hasattr(self, \"model\")\n",
      "Chunk 227: return getattr(self.model, name)\n",
      "Chunk 228: def predict_proba(\n",
      "        self,\n",
      "        text_or_texts,\n",
      "        stride=256,\n",
      "Chunk 229: block_size: int = 512,\n",
      "        batch_size=32,\n",
      "        pad_last_batch: bool = False,\n",
      "Chunk 230: remove_whitespace_before_inference: bool = False,\n",
      "        outer_batch_size=1000,\n",
      "Chunk 231: return_paragraph_probabilities=False,\n",
      "        verbose: bool = False,\n",
      "    ):\n",
      "Chunk 232: if isinstance(text_or_texts, str):\n",
      "            return next(\n",
      "Chunk 233: self._predict_proba(\n",
      "                    [text_or_texts],\n",
      "Chunk 234: stride=stride,\n",
      "                    block_size=block_size,\n",
      "Chunk 235: batch_size=batch_size,\n",
      "                    pad_last_batch=pad_last_batch,\n",
      "Chunk 236: remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "Chunk 237: outer_batch_size=outer_batch_size,\n",
      "Chunk 238: return_paragraph_probabilities=return_paragraph_probabilities,\n",
      "Chunk 239: verbose=verbose,\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "Chunk 240: return self._predict_proba(\n",
      "                text_or_texts,\n",
      "Chunk 241: stride=stride,\n",
      "                block_size=block_size,\n",
      "Chunk 242: batch_size=batch_size,\n",
      "                pad_last_batch=pad_last_batch,\n",
      "Chunk 243: remove_whitespace_before_inference=remove_whitespace_before_inference,\n",
      "Chunk 244: outer_batch_size=outer_batch_size,\n",
      "Chunk 245: return_paragraph_probabilities=return_paragraph_probabilities,\n",
      "Chunk 246: verbose=verbose,\n",
      "            )\n",
      "Chunk 247: def _predict_proba(\n",
      "        self,\n",
      "        texts,\n",
      "        stride: int,\n",
      "        block_size: int,\n",
      "Chunk 248: batch_size: int,\n",
      "        pad_last_batch: bool,\n",
      "Chunk 249: remove_whitespace_before_inference: bool,\n",
      "        outer_batch_size: int,\n",
      "Chunk 250: return_paragraph_probabilities: bool,\n",
      "        verbose: bool,\n",
      "    ):\n",
      "Chunk 251: def newline_probability_fn(logits):\n",
      "Chunk 252: return sigmoid(logits[:, Constants.NEWLINE_INDEX])\n",
      "Chunk 253: n_outer_batches = math.ceil(len(texts) / outer_batch_size)\n",
      "Chunk 254: for outer_batch_idx in range(n_outer_batches):\n",
      "Chunk 255: start, end = outer_batch_idx * outer_batch_size, min((outer_batch_idx + 1) *\n",
      "Chunk 256: outer_batch_size, len(texts))\n",
      "Chunk 257: outer_batch_texts = texts[start:end]\n",
      "            input_texts = []\n",
      "Chunk 258: space_positions = []\n",
      "Chunk 259: for text in outer_batch_texts:\n",
      "                if remove_whitespace_before_inference:\n",
      "Chunk 260: text_space_positions = []\n",
      "                    input_text = \"\"\n",
      "Chunk 261: for c in text:\n",
      "                        if c == \" \":\n",
      "Chunk 262: text_space_positions.append(len(input_text) +\n",
      "Chunk 263: len(text_space_positions))\n",
      "Chunk 264: else:\n",
      "                            input_text += c\n",
      "Chunk 265: space_positions.append(text_space_positions)\n",
      "                else:\n",
      "Chunk 266: input_text = text\n",
      "Chunk 267: input_texts.append(input_text)\n",
      "Chunk 268: empty_string_indices = [i for i, text in enumerate(input_texts) if not text.strip()]\n",
      "Chunk 269: # remove empty strings from input_texts\n",
      "Chunk 270: input_texts = [text for text in input_texts if text.strip()]\n",
      "Chunk 271: if input_texts:\n",
      "Chunk 272: outer_batch_logits, _, tokenizer, tokenizer_output = extract(\n",
      "Chunk 273: input_texts,\n",
      "                    self.model,\n",
      "                    stride=stride,\n",
      "Chunk 274: max_block_size=block_size,\n",
      "                    batch_size=batch_size,\n",
      "Chunk 275: pad_last_batch=pad_last_batch,\n",
      "                    verbose=verbose,\n",
      "Chunk 276: tokenizer=self.tokenizer,\n",
      "                )\n",
      "Chunk 277: # convert token probabilities to character probabilities for the entire array\n",
      "Chunk 278: outer_batch_logits = [\n",
      "                    token_to_char_probs(\n",
      "Chunk 279: input_texts[i],\n",
      "                        tokenizer_output[\"input_ids\"][i],\n",
      "Chunk 280: outer_batch_logits[i],\n",
      "                        tokenizer,\n",
      "Chunk 281: tokenizer_output[\"offset_mapping\"][i],\n",
      "                    )\n",
      "Chunk 282: for i in range(len(input_texts))\n",
      "                ]\n",
      "            else:\n",
      "Chunk 283: outer_batch_logits = []\n",
      "Chunk 284: # add back empty strings\n",
      "            for i in empty_string_indices:\n",
      "Chunk 285: outer_batch_logits.insert(i, np.ones([1, 1]) * -np.inf)\n",
      "Chunk 286: for i, (text, logits) in enumerate(zip(outer_batch_texts, outer_batch_logits)):\n",
      "Chunk 287: sentence_probs = newline_probs = newline_probability_fn(logits)\n",
      "Chunk 288: if remove_whitespace_before_inference:\n",
      "Chunk 289: full_newline_probs, full_sentence_probs = list(newline_probs),\n",
      "Chunk 290: list(sentence_probs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old Headless mode will be removed from the Chrome binary soon. Please use the new Headless mode (https://developer.chrome.com/docs/chromium/new-headless) or the chrome-headless-shell which is a standalone implementation of the old Headless mode (https://developer.chrome.com/blog/chrome-headless-shell).\n",
      "\n",
      "[1226/153142.385660:WARNING:viz_main_impl.cc(85)] VizNullHypothesis is disabled (not a warning)\n",
      "[1226/153142.418003:ERROR:cv_display_link_mac.mm(165)] CVDisplayLinkCreateWithCGDisplay failed. CVReturn: -6670\n",
      "[1226/153142.450965:ERROR:cv_display_link_mac.mm(165)] CVDisplayLinkCreateWithCGDisplay failed. CVReturn: -6670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高亮文本已保存为 PNG 图片:data/test_result/python/chunkSizeunknown_chunkOverlapunknown_pythoncodesplitter_20241226_153141.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "354767 bytes written to file /Users/yuuki/Documents/Code/zhihuishu/chunk_experiment/util/test/data/test_result/python/chunkSizeunknown_chunkOverlapunknown_pythoncodesplitter_20241226_153141.png\n"
     ]
    }
   ],
   "source": [
    "highlighter.save_highlighted_text(\"data/test_result/python\",\"pythoncodesplitter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
